{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "21d66b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import svd\n",
    "from dtuimldmtools import *\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import torch\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from dtuimldmtools import rlr_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "1df2ad1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>HR_Mean</th>\n",
       "      <th>HR_Median</th>\n",
       "      <th>HR_std</th>\n",
       "      <th>HR_Min</th>\n",
       "      <th>HR_Max</th>\n",
       "      <th>HR_AUC</th>\n",
       "      <th>Round</th>\n",
       "      <th>Phase</th>\n",
       "      <th>Individual</th>\n",
       "      <th>Puzzler</th>\n",
       "      <th>Frustrated</th>\n",
       "      <th>Cohort</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>77.965186</td>\n",
       "      <td>78.000</td>\n",
       "      <td>3.345290</td>\n",
       "      <td>73.23</td>\n",
       "      <td>83.37</td>\n",
       "      <td>22924.945</td>\n",
       "      <td>round_3</td>\n",
       "      <td>phase3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>D1_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>70.981097</td>\n",
       "      <td>70.570</td>\n",
       "      <td>2.517879</td>\n",
       "      <td>67.12</td>\n",
       "      <td>78.22</td>\n",
       "      <td>21930.400</td>\n",
       "      <td>round_3</td>\n",
       "      <td>phase2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>D1_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>73.371959</td>\n",
       "      <td>73.360</td>\n",
       "      <td>3.259569</td>\n",
       "      <td>67.88</td>\n",
       "      <td>80.22</td>\n",
       "      <td>21647.085</td>\n",
       "      <td>round_3</td>\n",
       "      <td>phase1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>D1_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>78.916822</td>\n",
       "      <td>77.880</td>\n",
       "      <td>4.054595</td>\n",
       "      <td>72.32</td>\n",
       "      <td>84.92</td>\n",
       "      <td>25258.905</td>\n",
       "      <td>round_2</td>\n",
       "      <td>phase3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>D1_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>77.322226</td>\n",
       "      <td>74.550</td>\n",
       "      <td>6.047603</td>\n",
       "      <td>70.52</td>\n",
       "      <td>90.15</td>\n",
       "      <td>23890.565</td>\n",
       "      <td>round_2</td>\n",
       "      <td>phase2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>D1_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>163</td>\n",
       "      <td>73.594539</td>\n",
       "      <td>72.380</td>\n",
       "      <td>9.474556</td>\n",
       "      <td>57.43</td>\n",
       "      <td>93.53</td>\n",
       "      <td>21482.985</td>\n",
       "      <td>round_4</td>\n",
       "      <td>phase2</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>D1_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>164</td>\n",
       "      <td>57.839897</td>\n",
       "      <td>54.130</td>\n",
       "      <td>6.796647</td>\n",
       "      <td>52.97</td>\n",
       "      <td>74.14</td>\n",
       "      <td>16825.740</td>\n",
       "      <td>round_4</td>\n",
       "      <td>phase1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>D1_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>165</td>\n",
       "      <td>64.237295</td>\n",
       "      <td>65.195</td>\n",
       "      <td>3.589241</td>\n",
       "      <td>58.97</td>\n",
       "      <td>72.63</td>\n",
       "      <td>18691.065</td>\n",
       "      <td>round_1</td>\n",
       "      <td>phase3</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>D1_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>166</td>\n",
       "      <td>70.834320</td>\n",
       "      <td>70.440</td>\n",
       "      <td>2.391160</td>\n",
       "      <td>66.65</td>\n",
       "      <td>76.07</td>\n",
       "      <td>20753.005</td>\n",
       "      <td>round_1</td>\n",
       "      <td>phase2</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>D1_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>167</td>\n",
       "      <td>71.133878</td>\n",
       "      <td>69.225</td>\n",
       "      <td>14.069337</td>\n",
       "      <td>57.17</td>\n",
       "      <td>114.33</td>\n",
       "      <td>20820.320</td>\n",
       "      <td>round_1</td>\n",
       "      <td>phase1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>D1_2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>168 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0    HR_Mean  HR_Median     HR_std  HR_Min  HR_Max     HR_AUC  \\\n",
       "0             0  77.965186     78.000   3.345290   73.23   83.37  22924.945   \n",
       "1             1  70.981097     70.570   2.517879   67.12   78.22  21930.400   \n",
       "2             2  73.371959     73.360   3.259569   67.88   80.22  21647.085   \n",
       "3             3  78.916822     77.880   4.054595   72.32   84.92  25258.905   \n",
       "4             4  77.322226     74.550   6.047603   70.52   90.15  23890.565   \n",
       "..          ...        ...        ...        ...     ...     ...        ...   \n",
       "163         163  73.594539     72.380   9.474556   57.43   93.53  21482.985   \n",
       "164         164  57.839897     54.130   6.796647   52.97   74.14  16825.740   \n",
       "165         165  64.237295     65.195   3.589241   58.97   72.63  18691.065   \n",
       "166         166  70.834320     70.440   2.391160   66.65   76.07  20753.005   \n",
       "167         167  71.133878     69.225  14.069337   57.17  114.33  20820.320   \n",
       "\n",
       "       Round   Phase  Individual  Puzzler  Frustrated Cohort  \n",
       "0    round_3  phase3           1        1           1   D1_1  \n",
       "1    round_3  phase2           1        1           5   D1_1  \n",
       "2    round_3  phase1           1        1           0   D1_1  \n",
       "3    round_2  phase3           1        1           1   D1_1  \n",
       "4    round_2  phase2           1        1           5   D1_1  \n",
       "..       ...     ...         ...      ...         ...    ...  \n",
       "163  round_4  phase2          14        0           8   D1_2  \n",
       "164  round_4  phase1          14        0           0   D1_2  \n",
       "165  round_1  phase3          14        0           1   D1_2  \n",
       "166  round_1  phase2          14        0           4   D1_2  \n",
       "167  round_1  phase1          14        0           0   D1_2  \n",
       "\n",
       "[168 rows x 13 columns]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\malth\\OneDrive - Danmarks Tekniske Universitet\\Porgrammering\\02445---Individuel-assignment-\\HR_data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "27eed65c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HR_Mean</th>\n",
       "      <th>HR_Median</th>\n",
       "      <th>HR_std</th>\n",
       "      <th>HR_Min</th>\n",
       "      <th>HR_Max</th>\n",
       "      <th>HR_AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.260749</td>\n",
       "      <td>0.392342</td>\n",
       "      <td>-0.631771</td>\n",
       "      <td>0.824668</td>\n",
       "      <td>-0.445924</td>\n",
       "      <td>-0.131939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.670301</td>\n",
       "      <td>-0.524777</td>\n",
       "      <td>-0.825667</td>\n",
       "      <td>-0.042982</td>\n",
       "      <td>-0.849385</td>\n",
       "      <td>-0.409001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.351575</td>\n",
       "      <td>-0.180394</td>\n",
       "      <td>-0.651859</td>\n",
       "      <td>0.064942</td>\n",
       "      <td>-0.692701</td>\n",
       "      <td>-0.487928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.387612</td>\n",
       "      <td>0.377530</td>\n",
       "      <td>-0.465551</td>\n",
       "      <td>0.695444</td>\n",
       "      <td>-0.324494</td>\n",
       "      <td>0.518262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.175036</td>\n",
       "      <td>-0.033507</td>\n",
       "      <td>0.001493</td>\n",
       "      <td>0.439835</td>\n",
       "      <td>0.085234</td>\n",
       "      <td>0.137066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>-0.321902</td>\n",
       "      <td>-0.301360</td>\n",
       "      <td>0.804569</td>\n",
       "      <td>-1.419009</td>\n",
       "      <td>0.350030</td>\n",
       "      <td>-0.533643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>-2.422157</td>\n",
       "      <td>-2.554041</td>\n",
       "      <td>0.177024</td>\n",
       "      <td>-2.052351</td>\n",
       "      <td>-1.169020</td>\n",
       "      <td>-1.831071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>-1.569319</td>\n",
       "      <td>-1.188238</td>\n",
       "      <td>-0.574603</td>\n",
       "      <td>-1.200322</td>\n",
       "      <td>-1.287316</td>\n",
       "      <td>-1.311424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>-0.689868</td>\n",
       "      <td>-0.540823</td>\n",
       "      <td>-0.855363</td>\n",
       "      <td>-0.109724</td>\n",
       "      <td>-1.017820</td>\n",
       "      <td>-0.737003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>-0.649934</td>\n",
       "      <td>-0.690796</td>\n",
       "      <td>1.881316</td>\n",
       "      <td>-1.455931</td>\n",
       "      <td>1.979542</td>\n",
       "      <td>-0.718250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>168 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      HR_Mean  HR_Median    HR_std    HR_Min    HR_Max    HR_AUC\n",
       "0    0.260749   0.392342 -0.631771  0.824668 -0.445924 -0.131939\n",
       "1   -0.670301  -0.524777 -0.825667 -0.042982 -0.849385 -0.409001\n",
       "2   -0.351575  -0.180394 -0.651859  0.064942 -0.692701 -0.487928\n",
       "3    0.387612   0.377530 -0.465551  0.695444 -0.324494  0.518262\n",
       "4    0.175036  -0.033507  0.001493  0.439835  0.085234  0.137066\n",
       "..        ...        ...       ...       ...       ...       ...\n",
       "163 -0.321902  -0.301360  0.804569 -1.419009  0.350030 -0.533643\n",
       "164 -2.422157  -2.554041  0.177024 -2.052351 -1.169020 -1.831071\n",
       "165 -1.569319  -1.188238 -0.574603 -1.200322 -1.287316 -1.311424\n",
       "166 -0.689868  -0.540823 -0.855363 -0.109724 -1.017820 -0.737003\n",
       "167 -0.649934  -0.690796  1.881316 -1.455931  1.979542 -0.718250\n",
       "\n",
       "[168 rows x 6 columns]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = df[['HR_Mean','HR_Median','HR_std','HR_Min','HR_Max','HR_AUC']]\n",
    "x_standardized = (features - features.mean()) / features.std()\n",
    "x = torch.tensor(x_standardized.values, dtype=torch.float32)\n",
    "y = torch.tensor(df['Frustrated'].values, dtype=torch.long)\n",
    "groups = df['Individual']\n",
    "x_standardized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48391cda",
   "metadata": {},
   "source": [
    "### Model 1 ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bf1493",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[145]\u001b[39m\u001b[32m, line 50\u001b[39m\n\u001b[32m     48\u001b[39m     loss = criterion(outputs, y_train)\n\u001b[32m     49\u001b[39m     loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m#Evaluation of the model:\u001b[39;00m\n\u001b[32m     54\u001b[39m model.eval()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\malth\\Miniconda3\\envs\\dtu02450\\Lib\\site-packages\\torch\\optim\\optimizer.py:493\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    488\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    489\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    490\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    491\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    496\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\malth\\Miniconda3\\envs\\dtu02450\\Lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     89\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     90\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     93\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\malth\\Miniconda3\\envs\\dtu02450\\Lib\\site-packages\\torch\\optim\\adam.py:244\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    232\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    234\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    235\u001b[39m         group,\n\u001b[32m    236\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    241\u001b[39m         state_steps,\n\u001b[32m    242\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\malth\\Miniconda3\\envs\\dtu02450\\Lib\\site-packages\\torch\\optim\\optimizer.py:154\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\malth\\Miniconda3\\envs\\dtu02450\\Lib\\site-packages\\torch\\optim\\adam.py:876\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    873\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    874\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m876\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    887\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    891\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    892\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    893\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    894\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    895\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\malth\\Miniconda3\\envs\\dtu02450\\Lib\\site-packages\\torch\\optim\\adam.py:398\u001b[39m, in \u001b[36m_single_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[39m\n\u001b[32m    395\u001b[39m step_t += \u001b[32m1\u001b[39m\n\u001b[32m    397\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m weight_decay != \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m     grad = \u001b[43mgrad\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.is_complex(param):\n\u001b[32m    401\u001b[39m     grad = torch.view_as_real(grad)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "class ANNClassifier(torch.nn.Module):\n",
    "    def __init__(self,input_dim, hidden_units, output_dim):\n",
    "        super(ANNClassifier,self).__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim,hidden_units),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_units, hidden_units),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_units, hidden_units),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_units, hidden_units),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.3),\n",
    "            torch.nn.Linear(hidden_units,output_dim)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "    \n",
    "#Hyperparameters:\n",
    "input_dim = x.shape[1]\n",
    "hidden_units = 132\n",
    "output_dim = 10\n",
    "\n",
    "epochs = 5000\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Group K-Fold:\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "ann_accuracy = []\n",
    "\n",
    "for train_idx, test_idx in gkf.split(x,y, groups = groups):\n",
    "    model = ANNClassifier(input_dim,hidden_units,output_dim)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    X_train, X_test = x[train_idx], x[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}: loss = {loss.item():.4f}\")\n",
    "\n",
    "    #Evaluation of the model:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test)\n",
    "        y_pred_labels = torch.argmax(y_pred,dim = 1)\n",
    "        acc = accuracy_score(y_test.numpy(), y_pred_labels.numpy())\n",
    "        ann_accuracy.append(acc)\n",
    "\n",
    "print(f\"ANN Average Accuracy: {np.mean(ann_accuracy):3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "189f70c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fold 1 ---\n",
      "Fold 1 | Iteration 500 | Loss: 2.3868 | Train Acc: 0.1136 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 2.3708 | Train Acc: 0.1212 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 2.3476 | Train Acc: 0.1439 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 2.3298 | Train Acc: 0.3030 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 2.3064 | Train Acc: 0.3030 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 2.2814 | Train Acc: 0.3030 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 2.2393 | Train Acc: 0.3030 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 2.2000 | Train Acc: 0.3030 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 2.1564 | Train Acc: 0.3030 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 2.1046 | Train Acc: 0.3030 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 2.0570 | Train Acc: 0.3030 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 2.0089 | Train Acc: 0.3030 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.9329 | Train Acc: 0.3030 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.9089 | Train Acc: 0.3030 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.9198 | Train Acc: 0.3030 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.8902 | Train Acc: 0.3030 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.8955 | Train Acc: 0.3030 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.8773 | Train Acc: 0.3030 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.8586 | Train Acc: 0.3030 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.8122 | Train Acc: 0.3030 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.8492 | Train Acc: 0.3030 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.8115 | Train Acc: 0.3106 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.8018 | Train Acc: 0.3106 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 1.8178 | Train Acc: 0.3182 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 1.8004 | Train Acc: 0.2879 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 1.8268 | Train Acc: 0.2955 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 1.7968 | Train Acc: 0.3258 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.7806 | Train Acc: 0.3258 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.7959 | Train Acc: 0.3030 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.7585 | Train Acc: 0.3030 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.7755 | Train Acc: 0.3030 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.7582 | Train Acc: 0.3030 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.7663 | Train Acc: 0.3030 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.7276 | Train Acc: 0.3030 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.7566 | Train Acc: 0.3485 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.7326 | Train Acc: 0.3561 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.7156 | Train Acc: 0.3561 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.7146 | Train Acc: 0.3333 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.6916 | Train Acc: 0.3409 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.7067 | Train Acc: 0.3409 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.7194 | Train Acc: 0.3485 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.6890 | Train Acc: 0.3485 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.7085 | Train Acc: 0.3561 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.6983 | Train Acc: 0.3485 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.6939 | Train Acc: 0.3409 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.6442 | Train Acc: 0.3636 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.6544 | Train Acc: 0.3712 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.6335 | Train Acc: 0.3712 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.6215 | Train Acc: 0.3712 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.6241 | Train Acc: 0.3636 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.6374 | Train Acc: 0.3636 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.6275 | Train Acc: 0.3636 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.6200 | Train Acc: 0.3636 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.6287 | Train Acc: 0.3636 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.6298 | Train Acc: 0.3788 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.5963 | Train Acc: 0.3712 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.5808 | Train Acc: 0.3712 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.6083 | Train Acc: 0.3636 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.5708 | Train Acc: 0.3712 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.5917 | Train Acc: 0.3712 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.5521 | Train Acc: 0.3712 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.5275 | Train Acc: 0.3788 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.5636 | Train Acc: 0.3788 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.5670 | Train Acc: 0.3864 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.5421 | Train Acc: 0.3788 | Test Acc: 0.0833\n",
      "Fold 1 | Iteration 500 | Loss: 1.5320 | Train Acc: 0.3939 | Test Acc: 0.0833\n",
      "Fold 1 | Iteration 500 | Loss: 1.5377 | Train Acc: 0.4015 | Test Acc: 0.0833\n",
      "Fold 1 | Iteration 500 | Loss: 1.5076 | Train Acc: 0.3939 | Test Acc: 0.0833\n",
      "Fold 1 | Iteration 500 | Loss: 1.4782 | Train Acc: 0.3788 | Test Acc: 0.0833\n",
      "Fold 1 | Iteration 500 | Loss: 1.4996 | Train Acc: 0.3939 | Test Acc: 0.0833\n",
      "Fold 1 | Iteration 500 | Loss: 1.5032 | Train Acc: 0.3939 | Test Acc: 0.0833\n",
      "Fold 1 | Iteration 500 | Loss: 1.4881 | Train Acc: 0.3939 | Test Acc: 0.0833\n",
      "Fold 1 | Iteration 500 | Loss: 1.5172 | Train Acc: 0.4015 | Test Acc: 0.0833\n",
      "Fold 1 | Iteration 500 | Loss: 1.4923 | Train Acc: 0.4242 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.5000 | Train Acc: 0.4167 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.4606 | Train Acc: 0.4091 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.4570 | Train Acc: 0.4091 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.4358 | Train Acc: 0.4242 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.4540 | Train Acc: 0.4167 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.4660 | Train Acc: 0.4167 | Test Acc: 0.0833\n",
      "Fold 1 | Iteration 500 | Loss: 1.4579 | Train Acc: 0.4394 | Test Acc: 0.0833\n",
      "Fold 1 | Iteration 500 | Loss: 1.4428 | Train Acc: 0.4167 | Test Acc: 0.0833\n",
      "Fold 1 | Iteration 500 | Loss: 1.4127 | Train Acc: 0.4167 | Test Acc: 0.0833\n",
      "Fold 1 | Iteration 500 | Loss: 1.4263 | Train Acc: 0.4167 | Test Acc: 0.0833\n",
      "Fold 1 | Iteration 500 | Loss: 1.4159 | Train Acc: 0.4394 | Test Acc: 0.0833\n",
      "Fold 1 | Iteration 500 | Loss: 1.3931 | Train Acc: 0.4394 | Test Acc: 0.0833\n",
      "Fold 1 | Iteration 500 | Loss: 1.3663 | Train Acc: 0.4394 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.4065 | Train Acc: 0.4394 | Test Acc: 0.0833\n",
      "Fold 1 | Iteration 500 | Loss: 1.3646 | Train Acc: 0.4394 | Test Acc: 0.0833\n",
      "Fold 1 | Iteration 500 | Loss: 1.3658 | Train Acc: 0.4394 | Test Acc: 0.0833\n",
      "Fold 1 | Iteration 500 | Loss: 1.4108 | Train Acc: 0.4394 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.3386 | Train Acc: 0.4545 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.3495 | Train Acc: 0.4470 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.3513 | Train Acc: 0.4545 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.3326 | Train Acc: 0.4545 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.3284 | Train Acc: 0.4545 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.3268 | Train Acc: 0.4621 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.3472 | Train Acc: 0.4773 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.3051 | Train Acc: 0.4621 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.3150 | Train Acc: 0.4697 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.2649 | Train Acc: 0.4773 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.2693 | Train Acc: 0.5152 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.2830 | Train Acc: 0.5076 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.2553 | Train Acc: 0.5152 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.2951 | Train Acc: 0.5000 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.2307 | Train Acc: 0.5076 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.2267 | Train Acc: 0.5000 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.2310 | Train Acc: 0.5152 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.2578 | Train Acc: 0.5530 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.2267 | Train Acc: 0.5530 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.2085 | Train Acc: 0.5455 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.2039 | Train Acc: 0.5530 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.2048 | Train Acc: 0.5530 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.2040 | Train Acc: 0.5606 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.1845 | Train Acc: 0.5606 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.1344 | Train Acc: 0.5455 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.1853 | Train Acc: 0.5303 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.1400 | Train Acc: 0.5303 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.1801 | Train Acc: 0.5379 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.1393 | Train Acc: 0.5682 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.1286 | Train Acc: 0.5833 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.1480 | Train Acc: 0.5909 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.1468 | Train Acc: 0.5833 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.1002 | Train Acc: 0.5606 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.1135 | Train Acc: 0.5606 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 1.1201 | Train Acc: 0.5833 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.0872 | Train Acc: 0.5909 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 1.0845 | Train Acc: 0.6061 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 1.0816 | Train Acc: 0.6061 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.0645 | Train Acc: 0.6212 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.0669 | Train Acc: 0.6288 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.0637 | Train Acc: 0.6288 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.0689 | Train Acc: 0.6288 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 1.0250 | Train Acc: 0.6439 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.0583 | Train Acc: 0.6667 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 0.9988 | Train Acc: 0.6591 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.0233 | Train Acc: 0.6364 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.0393 | Train Acc: 0.6288 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.0244 | Train Acc: 0.6439 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 0.9516 | Train Acc: 0.6515 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 0.9925 | Train Acc: 0.6742 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.0267 | Train Acc: 0.6818 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 0.9726 | Train Acc: 0.6742 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.0019 | Train Acc: 0.6667 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 0.9937 | Train Acc: 0.6591 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 0.9705 | Train Acc: 0.6515 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 0.9837 | Train Acc: 0.6742 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 0.9909 | Train Acc: 0.6818 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 0.9721 | Train Acc: 0.6818 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 0.9318 | Train Acc: 0.7045 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.9760 | Train Acc: 0.6818 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.9262 | Train Acc: 0.6742 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 0.9479 | Train Acc: 0.6894 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 0.8845 | Train Acc: 0.6742 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 0.9482 | Train Acc: 0.6818 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 0.9234 | Train Acc: 0.7045 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 0.8994 | Train Acc: 0.7197 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 0.9313 | Train Acc: 0.7197 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 0.8989 | Train Acc: 0.7273 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.8526 | Train Acc: 0.7045 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.8882 | Train Acc: 0.7045 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.8482 | Train Acc: 0.7197 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 0.8691 | Train Acc: 0.7197 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 0.8468 | Train Acc: 0.7273 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 0.8627 | Train Acc: 0.7424 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.8380 | Train Acc: 0.7273 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.8929 | Train Acc: 0.7121 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.8544 | Train Acc: 0.7121 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 0.8529 | Train Acc: 0.7197 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 0.7975 | Train Acc: 0.7197 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 0.7890 | Train Acc: 0.7273 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 0.8029 | Train Acc: 0.7424 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.7909 | Train Acc: 0.7576 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 0.7922 | Train Acc: 0.7576 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 0.8128 | Train Acc: 0.7652 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 0.7880 | Train Acc: 0.7500 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 0.8400 | Train Acc: 0.7197 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 0.7886 | Train Acc: 0.7348 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.8163 | Train Acc: 0.7500 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.7634 | Train Acc: 0.7500 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.7711 | Train Acc: 0.7803 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.7871 | Train Acc: 0.7879 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.7685 | Train Acc: 0.7652 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 0.7576 | Train Acc: 0.7500 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 0.7047 | Train Acc: 0.7500 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 0.7861 | Train Acc: 0.7424 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.7043 | Train Acc: 0.7727 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 0.7299 | Train Acc: 0.8030 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.7405 | Train Acc: 0.7955 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.6760 | Train Acc: 0.7727 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.7445 | Train Acc: 0.7803 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.6907 | Train Acc: 0.7879 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.6824 | Train Acc: 0.7955 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.6781 | Train Acc: 0.7879 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.6813 | Train Acc: 0.7955 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.6672 | Train Acc: 0.7955 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.6999 | Train Acc: 0.7955 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.6517 | Train Acc: 0.7879 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 0.7025 | Train Acc: 0.8182 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 0.6678 | Train Acc: 0.8106 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.6368 | Train Acc: 0.8106 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.6953 | Train Acc: 0.8182 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.6606 | Train Acc: 0.8182 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.6349 | Train Acc: 0.8182 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.5931 | Train Acc: 0.8258 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.5822 | Train Acc: 0.8409 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.6468 | Train Acc: 0.8333 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.6450 | Train Acc: 0.8106 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.6068 | Train Acc: 0.8258 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.6377 | Train Acc: 0.8333 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.5912 | Train Acc: 0.8333 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 0.6014 | Train Acc: 0.8409 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.6155 | Train Acc: 0.8485 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.5714 | Train Acc: 0.8258 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.5347 | Train Acc: 0.8333 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.5940 | Train Acc: 0.8409 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.5653 | Train Acc: 0.8485 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.5275 | Train Acc: 0.8561 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.5726 | Train Acc: 0.8409 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.5419 | Train Acc: 0.8485 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.5667 | Train Acc: 0.8561 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.5661 | Train Acc: 0.8485 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.5365 | Train Acc: 0.8712 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.5476 | Train Acc: 0.8561 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.5326 | Train Acc: 0.8485 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.5427 | Train Acc: 0.8712 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.5253 | Train Acc: 0.8561 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.5578 | Train Acc: 0.8561 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.4910 | Train Acc: 0.8561 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.5073 | Train Acc: 0.8712 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.5395 | Train Acc: 0.8788 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.4979 | Train Acc: 0.8636 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.5067 | Train Acc: 0.8636 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.5393 | Train Acc: 0.8712 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.4997 | Train Acc: 0.8636 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.4831 | Train Acc: 0.8864 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.4512 | Train Acc: 0.8939 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.4593 | Train Acc: 0.8939 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.4550 | Train Acc: 0.8864 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.4791 | Train Acc: 0.8712 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.4551 | Train Acc: 0.8788 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.4525 | Train Acc: 0.8636 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.4253 | Train Acc: 0.8788 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.4563 | Train Acc: 0.8864 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.4443 | Train Acc: 0.8864 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.4517 | Train Acc: 0.8788 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.4177 | Train Acc: 0.8788 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.4673 | Train Acc: 0.8939 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.4344 | Train Acc: 0.8788 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.4355 | Train Acc: 0.8788 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.4016 | Train Acc: 0.9015 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.3912 | Train Acc: 0.8939 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.4169 | Train Acc: 0.9015 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.3673 | Train Acc: 0.9015 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.4326 | Train Acc: 0.9091 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.3711 | Train Acc: 0.8939 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.3934 | Train Acc: 0.8939 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.3868 | Train Acc: 0.8939 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.4005 | Train Acc: 0.8788 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.4082 | Train Acc: 0.8939 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.3530 | Train Acc: 0.8864 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.3402 | Train Acc: 0.9091 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.3839 | Train Acc: 0.9015 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.3333 | Train Acc: 0.9015 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.3128 | Train Acc: 0.8864 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.3334 | Train Acc: 0.9015 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.3811 | Train Acc: 0.9015 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.3448 | Train Acc: 0.9091 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.3782 | Train Acc: 0.8864 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.3609 | Train Acc: 0.8864 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.3391 | Train Acc: 0.8939 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.3614 | Train Acc: 0.9167 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.3384 | Train Acc: 0.9091 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.2913 | Train Acc: 0.9091 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.3128 | Train Acc: 0.9167 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.3768 | Train Acc: 0.9242 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.3350 | Train Acc: 0.9167 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.3226 | Train Acc: 0.9242 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.3422 | Train Acc: 0.9167 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.3582 | Train Acc: 0.9242 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.3093 | Train Acc: 0.9394 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.3291 | Train Acc: 0.9242 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.3852 | Train Acc: 0.9167 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.3245 | Train Acc: 0.9394 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.3149 | Train Acc: 0.9242 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.3054 | Train Acc: 0.9394 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.2823 | Train Acc: 0.9470 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.3092 | Train Acc: 0.9242 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.2990 | Train Acc: 0.9394 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.3143 | Train Acc: 0.9394 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.3508 | Train Acc: 0.9470 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.2637 | Train Acc: 0.9167 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.2738 | Train Acc: 0.9318 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.2664 | Train Acc: 0.9470 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.2760 | Train Acc: 0.9470 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.2898 | Train Acc: 0.9470 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.2889 | Train Acc: 0.9242 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.3169 | Train Acc: 0.9470 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.2760 | Train Acc: 0.9545 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.2612 | Train Acc: 0.9470 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.2988 | Train Acc: 0.9318 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.2967 | Train Acc: 0.9318 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.2868 | Train Acc: 0.9697 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.2782 | Train Acc: 0.9621 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.2639 | Train Acc: 0.9621 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.2449 | Train Acc: 0.9545 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.2679 | Train Acc: 0.9394 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.2722 | Train Acc: 0.9394 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.2637 | Train Acc: 0.9621 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.2621 | Train Acc: 0.9697 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.2673 | Train Acc: 0.9318 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.2169 | Train Acc: 0.9167 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.2277 | Train Acc: 0.9621 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.2463 | Train Acc: 0.9773 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.2196 | Train Acc: 0.9470 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.2604 | Train Acc: 0.9621 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.2331 | Train Acc: 0.9697 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.2314 | Train Acc: 0.9470 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.2630 | Train Acc: 0.9924 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.2356 | Train Acc: 0.9848 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.2262 | Train Acc: 0.9697 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.2326 | Train Acc: 0.9773 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.2101 | Train Acc: 0.9697 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.2068 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.2466 | Train Acc: 0.9545 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.2230 | Train Acc: 0.9773 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1747 | Train Acc: 0.9848 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.2656 | Train Acc: 0.9697 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.2151 | Train Acc: 0.9621 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.2452 | Train Acc: 0.9621 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.2149 | Train Acc: 0.9621 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.2269 | Train Acc: 0.9697 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.2148 | Train Acc: 0.9621 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.2269 | Train Acc: 0.9621 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.2349 | Train Acc: 0.9621 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.1803 | Train Acc: 0.9848 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.1605 | Train Acc: 0.9697 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.2070 | Train Acc: 0.9470 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.2233 | Train Acc: 0.9697 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.1841 | Train Acc: 0.9773 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.2426 | Train Acc: 0.9848 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.2204 | Train Acc: 0.9848 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1694 | Train Acc: 0.9545 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.2237 | Train Acc: 0.9848 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.2213 | Train Acc: 0.9773 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.2160 | Train Acc: 0.9924 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.1994 | Train Acc: 0.9848 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.1952 | Train Acc: 0.9848 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.1779 | Train Acc: 0.9848 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1763 | Train Acc: 0.9773 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1779 | Train Acc: 0.9773 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1956 | Train Acc: 0.9773 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1653 | Train Acc: 0.9848 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.1880 | Train Acc: 0.9848 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1886 | Train Acc: 0.9848 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.1614 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.1865 | Train Acc: 0.9697 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.1732 | Train Acc: 0.9697 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.1791 | Train Acc: 0.9848 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.2270 | Train Acc: 0.9773 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.1784 | Train Acc: 0.9697 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.2169 | Train Acc: 0.9848 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.1962 | Train Acc: 0.9773 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1578 | Train Acc: 0.9773 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1757 | Train Acc: 0.9848 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.1569 | Train Acc: 0.9697 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.1672 | Train Acc: 0.9924 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.1313 | Train Acc: 0.9924 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.1520 | Train Acc: 0.9773 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.1590 | Train Acc: 1.0000 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.1664 | Train Acc: 0.9697 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.1657 | Train Acc: 0.9848 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.1707 | Train Acc: 0.9848 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.1581 | Train Acc: 0.9621 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.1468 | Train Acc: 0.9848 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1634 | Train Acc: 0.9848 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.1307 | Train Acc: 0.9773 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1945 | Train Acc: 0.9848 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1577 | Train Acc: 0.9848 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1666 | Train Acc: 0.9848 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.1587 | Train Acc: 0.9848 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.1359 | Train Acc: 0.9697 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.1637 | Train Acc: 0.9848 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.1317 | Train Acc: 0.9773 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1343 | Train Acc: 0.9545 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.1651 | Train Acc: 0.9621 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1498 | Train Acc: 0.9848 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1678 | Train Acc: 0.9773 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1801 | Train Acc: 0.9924 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1115 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.1475 | Train Acc: 0.9848 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.1726 | Train Acc: 0.9924 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.1577 | Train Acc: 0.9848 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1780 | Train Acc: 0.9773 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1417 | Train Acc: 0.9848 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1309 | Train Acc: 0.9697 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1235 | Train Acc: 0.9621 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.1466 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1211 | Train Acc: 0.9924 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1413 | Train Acc: 0.9697 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.1765 | Train Acc: 1.0000 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.1211 | Train Acc: 0.9924 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.1378 | Train Acc: 0.9924 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.1253 | Train Acc: 0.9924 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1179 | Train Acc: 0.9924 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1452 | Train Acc: 0.9924 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.1207 | Train Acc: 0.9924 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.1331 | Train Acc: 0.9848 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1283 | Train Acc: 0.9924 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.1033 | Train Acc: 1.0000 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.1542 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1121 | Train Acc: 0.9924 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1388 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1114 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1085 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1482 | Train Acc: 1.0000 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.1050 | Train Acc: 1.0000 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.1145 | Train Acc: 1.0000 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.1196 | Train Acc: 1.0000 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.1488 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1155 | Train Acc: 0.9848 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1057 | Train Acc: 0.9924 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1485 | Train Acc: 0.9924 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.1158 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1044 | Train Acc: 0.9924 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1077 | Train Acc: 0.9924 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1078 | Train Acc: 1.0000 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.1186 | Train Acc: 1.0000 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.1084 | Train Acc: 0.9924 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.1286 | Train Acc: 1.0000 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.0983 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1492 | Train Acc: 0.9924 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1280 | Train Acc: 0.9924 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1241 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1138 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1149 | Train Acc: 0.9924 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.1146 | Train Acc: 0.9924 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0949 | Train Acc: 0.9924 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1316 | Train Acc: 0.9924 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.1258 | Train Acc: 0.9924 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.1423 | Train Acc: 1.0000 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.1187 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1304 | Train Acc: 0.9924 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1144 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0862 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0851 | Train Acc: 0.9924 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0858 | Train Acc: 0.9924 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1080 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1106 | Train Acc: 0.9924 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1151 | Train Acc: 0.9924 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1025 | Train Acc: 0.9924 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1158 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0928 | Train Acc: 0.9924 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0795 | Train Acc: 0.9924 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0840 | Train Acc: 0.9924 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.0784 | Train Acc: 1.0000 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.0791 | Train Acc: 1.0000 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.0965 | Train Acc: 1.0000 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.0936 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0676 | Train Acc: 1.0000 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.0720 | Train Acc: 1.0000 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.0777 | Train Acc: 1.0000 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.1259 | Train Acc: 1.0000 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.1027 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1179 | Train Acc: 0.9924 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0790 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1047 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0886 | Train Acc: 0.9924 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0862 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0862 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0963 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0685 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0876 | Train Acc: 1.0000 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.0895 | Train Acc: 1.0000 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.0793 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1147 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0799 | Train Acc: 0.9924 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1105 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0712 | Train Acc: 0.9924 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0763 | Train Acc: 0.9924 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0667 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0794 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0699 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0668 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0698 | Train Acc: 0.9924 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0634 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1034 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0923 | Train Acc: 0.9924 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0706 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0760 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0615 | Train Acc: 1.0000 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.0741 | Train Acc: 1.0000 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.0739 | Train Acc: 1.0000 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.0590 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0650 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0854 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0787 | Train Acc: 0.9848 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0878 | Train Acc: 0.9848 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.0828 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0529 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "\n",
      "--- Fold 2 ---\n",
      "Fold 2 | Iteration 500 | Loss: 2.3979 | Train Acc: 0.1667 | Test Acc: 0.0278\n",
      "Fold 2 | Iteration 500 | Loss: 2.3821 | Train Acc: 0.2652 | Test Acc: 0.0278\n",
      "Fold 2 | Iteration 500 | Loss: 2.3687 | Train Acc: 0.2424 | Test Acc: 0.0556\n",
      "Fold 2 | Iteration 500 | Loss: 2.3523 | Train Acc: 0.2424 | Test Acc: 0.0556\n",
      "Fold 2 | Iteration 500 | Loss: 2.3383 | Train Acc: 0.2576 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 2.3178 | Train Acc: 0.2652 | Test Acc: 0.0556\n",
      "Fold 2 | Iteration 500 | Loss: 2.2963 | Train Acc: 0.2652 | Test Acc: 0.0278\n",
      "Fold 2 | Iteration 500 | Loss: 2.2802 | Train Acc: 0.2652 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 2.2440 | Train Acc: 0.2576 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 2.2088 | Train Acc: 0.2576 | Test Acc: 0.0556\n",
      "Fold 2 | Iteration 500 | Loss: 2.1710 | Train Acc: 0.2500 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 2.1286 | Train Acc: 0.2576 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 2.0689 | Train Acc: 0.2652 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 2.0329 | Train Acc: 0.2727 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 2.0012 | Train Acc: 0.2727 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 1.9875 | Train Acc: 0.2727 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 1.9873 | Train Acc: 0.2576 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 1.9455 | Train Acc: 0.2576 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 1.9509 | Train Acc: 0.2576 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 1.9008 | Train Acc: 0.2424 | Test Acc: 0.2500\n",
      "Fold 2 | Iteration 500 | Loss: 1.9225 | Train Acc: 0.2500 | Test Acc: 0.2500\n",
      "Fold 2 | Iteration 500 | Loss: 1.9469 | Train Acc: 0.2576 | Test Acc: 0.2500\n",
      "Fold 2 | Iteration 500 | Loss: 1.8999 | Train Acc: 0.2652 | Test Acc: 0.2500\n",
      "Fold 2 | Iteration 500 | Loss: 1.9143 | Train Acc: 0.2803 | Test Acc: 0.2222\n",
      "Fold 2 | Iteration 500 | Loss: 1.8694 | Train Acc: 0.2652 | Test Acc: 0.2222\n",
      "Fold 2 | Iteration 500 | Loss: 1.8514 | Train Acc: 0.2652 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 1.8981 | Train Acc: 0.2652 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 1.8701 | Train Acc: 0.2652 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 1.8693 | Train Acc: 0.2652 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 1.8433 | Train Acc: 0.2576 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 1.8424 | Train Acc: 0.2576 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 1.8119 | Train Acc: 0.2576 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 1.8467 | Train Acc: 0.2576 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 1.8111 | Train Acc: 0.2727 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 1.8297 | Train Acc: 0.2727 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 1.8611 | Train Acc: 0.2803 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 1.8052 | Train Acc: 0.2803 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 1.7944 | Train Acc: 0.2652 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 1.8049 | Train Acc: 0.2803 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 1.7855 | Train Acc: 0.2879 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 1.8070 | Train Acc: 0.2955 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 1.7664 | Train Acc: 0.2955 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 1.7924 | Train Acc: 0.2879 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 1.7725 | Train Acc: 0.2955 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 1.7516 | Train Acc: 0.3030 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 1.7432 | Train Acc: 0.3030 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 1.7561 | Train Acc: 0.3106 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 1.7409 | Train Acc: 0.3106 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 1.7502 | Train Acc: 0.3182 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 1.7153 | Train Acc: 0.3333 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 1.7281 | Train Acc: 0.3333 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 1.6842 | Train Acc: 0.3333 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 1.6746 | Train Acc: 0.3333 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 1.7183 | Train Acc: 0.3409 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 1.6745 | Train Acc: 0.3485 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 1.6601 | Train Acc: 0.3409 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 1.6961 | Train Acc: 0.3258 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 1.6634 | Train Acc: 0.3182 | Test Acc: 0.2222\n",
      "Fold 2 | Iteration 500 | Loss: 1.6312 | Train Acc: 0.3182 | Test Acc: 0.2222\n",
      "Fold 2 | Iteration 500 | Loss: 1.6802 | Train Acc: 0.3182 | Test Acc: 0.2500\n",
      "Fold 2 | Iteration 500 | Loss: 1.6310 | Train Acc: 0.3258 | Test Acc: 0.2500\n",
      "Fold 2 | Iteration 500 | Loss: 1.6512 | Train Acc: 0.3409 | Test Acc: 0.2500\n",
      "Fold 2 | Iteration 500 | Loss: 1.6374 | Train Acc: 0.3333 | Test Acc: 0.2500\n",
      "Fold 2 | Iteration 500 | Loss: 1.5928 | Train Acc: 0.3333 | Test Acc: 0.2222\n",
      "Fold 2 | Iteration 500 | Loss: 1.6089 | Train Acc: 0.3182 | Test Acc: 0.2222\n",
      "Fold 2 | Iteration 500 | Loss: 1.6112 | Train Acc: 0.3258 | Test Acc: 0.2222\n",
      "Fold 2 | Iteration 500 | Loss: 1.5527 | Train Acc: 0.3182 | Test Acc: 0.2222\n",
      "Fold 2 | Iteration 500 | Loss: 1.5711 | Train Acc: 0.3106 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 1.5632 | Train Acc: 0.3333 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 1.5751 | Train Acc: 0.3561 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 1.5725 | Train Acc: 0.3636 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 1.5909 | Train Acc: 0.3788 | Test Acc: 0.2222\n",
      "Fold 2 | Iteration 500 | Loss: 1.5338 | Train Acc: 0.3939 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 1.5385 | Train Acc: 0.3939 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 1.5449 | Train Acc: 0.3939 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 1.5716 | Train Acc: 0.4015 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 1.4906 | Train Acc: 0.3939 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 1.4961 | Train Acc: 0.3864 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 1.4729 | Train Acc: 0.3939 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 1.4692 | Train Acc: 0.3788 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 1.5143 | Train Acc: 0.3939 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 1.4438 | Train Acc: 0.3864 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 1.4256 | Train Acc: 0.3864 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 1.4763 | Train Acc: 0.4167 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 1.4576 | Train Acc: 0.4318 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 1.4152 | Train Acc: 0.4318 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 1.4395 | Train Acc: 0.4470 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 1.4229 | Train Acc: 0.4545 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 1.3877 | Train Acc: 0.4394 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 1.3783 | Train Acc: 0.4545 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 1.3791 | Train Acc: 0.4621 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 1.3978 | Train Acc: 0.4545 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 1.3934 | Train Acc: 0.4470 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 1.4116 | Train Acc: 0.4470 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 1.3552 | Train Acc: 0.4621 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 1.3558 | Train Acc: 0.4773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 1.3835 | Train Acc: 0.4848 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 1.3387 | Train Acc: 0.4924 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 1.3120 | Train Acc: 0.4773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 1.3155 | Train Acc: 0.5000 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 1.3538 | Train Acc: 0.4848 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 1.3064 | Train Acc: 0.4924 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 1.3119 | Train Acc: 0.4924 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 1.2345 | Train Acc: 0.5000 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 1.2698 | Train Acc: 0.5000 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 1.2692 | Train Acc: 0.5227 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 1.3014 | Train Acc: 0.5227 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 1.2445 | Train Acc: 0.5606 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 1.2420 | Train Acc: 0.5379 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 1.2446 | Train Acc: 0.5682 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 1.2516 | Train Acc: 0.5682 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 1.2407 | Train Acc: 0.5455 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 1.1996 | Train Acc: 0.5606 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 1.1855 | Train Acc: 0.5606 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 1.1996 | Train Acc: 0.5530 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 1.2042 | Train Acc: 0.5455 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 1.1919 | Train Acc: 0.5530 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 1.1757 | Train Acc: 0.5682 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 1.2040 | Train Acc: 0.6061 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 1.1729 | Train Acc: 0.6136 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 1.1465 | Train Acc: 0.6136 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 1.1438 | Train Acc: 0.5682 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 1.1477 | Train Acc: 0.5758 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 1.1344 | Train Acc: 0.5758 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 1.1163 | Train Acc: 0.5909 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 1.1454 | Train Acc: 0.5833 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 1.1282 | Train Acc: 0.6136 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 1.0971 | Train Acc: 0.6061 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 1.1196 | Train Acc: 0.6212 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 1.0771 | Train Acc: 0.6212 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 1.0986 | Train Acc: 0.6288 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 1.1126 | Train Acc: 0.6439 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 1.1090 | Train Acc: 0.6136 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 1.0783 | Train Acc: 0.6288 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 1.1135 | Train Acc: 0.6136 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 1.0742 | Train Acc: 0.6212 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 1.0511 | Train Acc: 0.6212 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 1.0275 | Train Acc: 0.6136 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 1.0905 | Train Acc: 0.6364 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 1.0071 | Train Acc: 0.6439 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 1.0234 | Train Acc: 0.6515 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.9950 | Train Acc: 0.6591 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.9874 | Train Acc: 0.6288 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 1.0588 | Train Acc: 0.6818 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.9682 | Train Acc: 0.6591 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 1.0025 | Train Acc: 0.6667 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 1.0237 | Train Acc: 0.6667 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 1.0102 | Train Acc: 0.6364 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.9976 | Train Acc: 0.6591 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.9806 | Train Acc: 0.6742 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.9906 | Train Acc: 0.6742 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.9662 | Train Acc: 0.6970 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.9136 | Train Acc: 0.7045 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.9479 | Train Acc: 0.6970 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.9371 | Train Acc: 0.6894 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.9370 | Train Acc: 0.7045 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.8886 | Train Acc: 0.6970 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.9497 | Train Acc: 0.7045 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.9177 | Train Acc: 0.6970 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.8763 | Train Acc: 0.7197 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.9235 | Train Acc: 0.7273 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.8804 | Train Acc: 0.7424 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.8931 | Train Acc: 0.7273 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.9058 | Train Acc: 0.7197 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.8831 | Train Acc: 0.7121 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.9004 | Train Acc: 0.7121 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.8867 | Train Acc: 0.7121 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.8477 | Train Acc: 0.7121 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.8883 | Train Acc: 0.7197 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.8506 | Train Acc: 0.7197 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.9304 | Train Acc: 0.7424 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.8654 | Train Acc: 0.7348 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.8835 | Train Acc: 0.7348 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.8264 | Train Acc: 0.7348 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.8354 | Train Acc: 0.7424 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.8901 | Train Acc: 0.7500 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.8388 | Train Acc: 0.7348 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.8087 | Train Acc: 0.7045 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.8437 | Train Acc: 0.7045 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.8063 | Train Acc: 0.7424 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.8248 | Train Acc: 0.7500 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.8016 | Train Acc: 0.7652 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.8221 | Train Acc: 0.7424 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.7773 | Train Acc: 0.7273 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.8146 | Train Acc: 0.7424 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.7833 | Train Acc: 0.7197 | Test Acc: 0.0556\n",
      "Fold 2 | Iteration 500 | Loss: 0.8698 | Train Acc: 0.7500 | Test Acc: 0.0556\n",
      "Fold 2 | Iteration 500 | Loss: 0.7701 | Train Acc: 0.7500 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.8101 | Train Acc: 0.7652 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.7592 | Train Acc: 0.7879 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.7600 | Train Acc: 0.7879 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.8121 | Train Acc: 0.7727 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.7645 | Train Acc: 0.7576 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.7762 | Train Acc: 0.7803 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.7251 | Train Acc: 0.7803 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.7120 | Train Acc: 0.7652 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.7311 | Train Acc: 0.7576 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.7650 | Train Acc: 0.7803 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.7112 | Train Acc: 0.7803 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.7203 | Train Acc: 0.7652 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.7035 | Train Acc: 0.7576 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.7314 | Train Acc: 0.7727 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.7019 | Train Acc: 0.7652 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.7458 | Train Acc: 0.7955 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.7472 | Train Acc: 0.7500 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.7030 | Train Acc: 0.7652 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.7141 | Train Acc: 0.7879 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.7098 | Train Acc: 0.7955 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.7653 | Train Acc: 0.8106 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.7007 | Train Acc: 0.7955 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.6813 | Train Acc: 0.7803 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.6895 | Train Acc: 0.7727 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.6960 | Train Acc: 0.7727 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.6787 | Train Acc: 0.7727 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.6719 | Train Acc: 0.7955 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.6826 | Train Acc: 0.7955 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.6508 | Train Acc: 0.8106 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.6635 | Train Acc: 0.8030 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.6579 | Train Acc: 0.7879 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.6537 | Train Acc: 0.7803 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.6579 | Train Acc: 0.7803 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.6432 | Train Acc: 0.7727 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.6887 | Train Acc: 0.7727 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.6483 | Train Acc: 0.7879 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.6009 | Train Acc: 0.7955 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.6125 | Train Acc: 0.8106 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.6565 | Train Acc: 0.8030 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.6114 | Train Acc: 0.8030 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.6454 | Train Acc: 0.8182 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.6450 | Train Acc: 0.8106 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.6014 | Train Acc: 0.8258 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.5835 | Train Acc: 0.8182 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.5987 | Train Acc: 0.7955 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.5770 | Train Acc: 0.8182 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.6152 | Train Acc: 0.8333 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.6010 | Train Acc: 0.8409 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.6221 | Train Acc: 0.8409 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.6142 | Train Acc: 0.8106 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.6041 | Train Acc: 0.8106 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.5539 | Train Acc: 0.8106 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.5651 | Train Acc: 0.7879 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.6075 | Train Acc: 0.8258 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.5583 | Train Acc: 0.8182 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.5975 | Train Acc: 0.8106 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.5472 | Train Acc: 0.8258 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.5452 | Train Acc: 0.8485 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.5625 | Train Acc: 0.8485 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.5850 | Train Acc: 0.8409 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.5910 | Train Acc: 0.8182 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.5346 | Train Acc: 0.8333 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.5512 | Train Acc: 0.8485 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.5731 | Train Acc: 0.8409 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.5572 | Train Acc: 0.8409 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.5801 | Train Acc: 0.8409 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.4943 | Train Acc: 0.8409 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.5204 | Train Acc: 0.8485 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.5158 | Train Acc: 0.8333 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.5244 | Train Acc: 0.8409 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.5285 | Train Acc: 0.8409 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.5167 | Train Acc: 0.8409 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.5192 | Train Acc: 0.8409 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.5325 | Train Acc: 0.8333 | Test Acc: 0.0556\n",
      "Fold 2 | Iteration 500 | Loss: 0.5639 | Train Acc: 0.8409 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.4828 | Train Acc: 0.8636 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.4741 | Train Acc: 0.8561 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.4968 | Train Acc: 0.8485 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.5007 | Train Acc: 0.8182 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.5300 | Train Acc: 0.8409 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.5320 | Train Acc: 0.8409 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.5103 | Train Acc: 0.8485 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.4614 | Train Acc: 0.8485 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.4965 | Train Acc: 0.8333 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.4710 | Train Acc: 0.8409 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.4557 | Train Acc: 0.8409 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.4680 | Train Acc: 0.8485 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.5247 | Train Acc: 0.8712 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.4856 | Train Acc: 0.8485 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.4395 | Train Acc: 0.8333 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.4468 | Train Acc: 0.8409 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.4845 | Train Acc: 0.8636 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.4566 | Train Acc: 0.8485 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.4242 | Train Acc: 0.8636 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.4190 | Train Acc: 0.8561 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.5163 | Train Acc: 0.8788 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.4488 | Train Acc: 0.8636 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.4572 | Train Acc: 0.8636 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.4482 | Train Acc: 0.8409 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.4197 | Train Acc: 0.8485 | Test Acc: 0.0556\n",
      "Fold 2 | Iteration 500 | Loss: 0.5005 | Train Acc: 0.8636 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.4921 | Train Acc: 0.8864 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.4618 | Train Acc: 0.8788 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.4636 | Train Acc: 0.8561 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.4396 | Train Acc: 0.8636 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.4335 | Train Acc: 0.8712 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.4282 | Train Acc: 0.8788 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.4466 | Train Acc: 0.8939 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.4447 | Train Acc: 0.8864 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.3956 | Train Acc: 0.8864 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.3828 | Train Acc: 0.8712 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.4626 | Train Acc: 0.8712 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.4490 | Train Acc: 0.8864 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.4385 | Train Acc: 0.8939 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.4999 | Train Acc: 0.8939 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.4219 | Train Acc: 0.9015 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.4307 | Train Acc: 0.8712 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.3916 | Train Acc: 0.8636 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.4026 | Train Acc: 0.9015 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.3821 | Train Acc: 0.8939 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.3723 | Train Acc: 0.9167 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.4047 | Train Acc: 0.9091 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.4256 | Train Acc: 0.9015 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.4009 | Train Acc: 0.9091 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.3893 | Train Acc: 0.8864 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.4020 | Train Acc: 0.9015 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.3919 | Train Acc: 0.9015 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.3686 | Train Acc: 0.8939 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.3772 | Train Acc: 0.8864 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.3374 | Train Acc: 0.8939 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.3941 | Train Acc: 0.8864 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.3602 | Train Acc: 0.8864 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.3784 | Train Acc: 0.9015 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.3999 | Train Acc: 0.8939 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.3599 | Train Acc: 0.9015 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.3634 | Train Acc: 0.9091 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.3701 | Train Acc: 0.9091 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.3545 | Train Acc: 0.9015 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.3723 | Train Acc: 0.8939 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.3357 | Train Acc: 0.9091 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.4071 | Train Acc: 0.9091 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.4023 | Train Acc: 0.9015 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.3139 | Train Acc: 0.9091 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.3597 | Train Acc: 0.9015 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.3277 | Train Acc: 0.9167 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.3662 | Train Acc: 0.9167 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.3647 | Train Acc: 0.9167 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.3476 | Train Acc: 0.9091 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.3368 | Train Acc: 0.9015 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.3366 | Train Acc: 0.9091 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.3764 | Train Acc: 0.9167 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.3299 | Train Acc: 0.9167 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.3277 | Train Acc: 0.9091 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.3702 | Train Acc: 0.9167 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.3560 | Train Acc: 0.9242 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.3366 | Train Acc: 0.9318 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.3148 | Train Acc: 0.9242 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.2957 | Train Acc: 0.9242 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.3363 | Train Acc: 0.9242 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.3091 | Train Acc: 0.9091 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.3086 | Train Acc: 0.9167 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.3726 | Train Acc: 0.9318 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.3121 | Train Acc: 0.9242 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.3156 | Train Acc: 0.9167 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.2906 | Train Acc: 0.9167 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.3237 | Train Acc: 0.9167 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.3280 | Train Acc: 0.9318 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.3122 | Train Acc: 0.9394 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.3035 | Train Acc: 0.9318 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2885 | Train Acc: 0.9470 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.2974 | Train Acc: 0.9318 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.3198 | Train Acc: 0.9242 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.2936 | Train Acc: 0.9242 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.2814 | Train Acc: 0.9242 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2916 | Train Acc: 0.9318 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2866 | Train Acc: 0.9167 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.3079 | Train Acc: 0.9167 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.2913 | Train Acc: 0.9318 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.2995 | Train Acc: 0.9394 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.3077 | Train Acc: 0.9318 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2927 | Train Acc: 0.9242 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.3091 | Train Acc: 0.9242 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2984 | Train Acc: 0.9318 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.3040 | Train Acc: 0.9318 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2599 | Train Acc: 0.9242 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2827 | Train Acc: 0.9242 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.2804 | Train Acc: 0.9242 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.3104 | Train Acc: 0.9167 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.2414 | Train Acc: 0.9394 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2815 | Train Acc: 0.9318 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2883 | Train Acc: 0.9394 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2824 | Train Acc: 0.9318 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2872 | Train Acc: 0.9242 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2807 | Train Acc: 0.9318 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2537 | Train Acc: 0.9394 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2389 | Train Acc: 0.9470 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2749 | Train Acc: 0.9242 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2824 | Train Acc: 0.9242 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2476 | Train Acc: 0.9394 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2660 | Train Acc: 0.9394 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2706 | Train Acc: 0.9394 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2214 | Train Acc: 0.9318 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2760 | Train Acc: 0.9167 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2646 | Train Acc: 0.9394 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2771 | Train Acc: 0.9394 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2497 | Train Acc: 0.9394 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2814 | Train Acc: 0.9167 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.3083 | Train Acc: 0.9242 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.2943 | Train Acc: 0.9242 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.2764 | Train Acc: 0.9242 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.2858 | Train Acc: 0.9470 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.2289 | Train Acc: 0.9394 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.2294 | Train Acc: 0.9318 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.2764 | Train Acc: 0.9470 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2478 | Train Acc: 0.9545 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2450 | Train Acc: 0.9470 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2825 | Train Acc: 0.9394 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2532 | Train Acc: 0.9318 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2297 | Train Acc: 0.9318 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2727 | Train Acc: 0.9697 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2560 | Train Acc: 0.9470 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2057 | Train Acc: 0.9394 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2418 | Train Acc: 0.9242 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2820 | Train Acc: 0.9318 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2172 | Train Acc: 0.9470 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2413 | Train Acc: 0.9470 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2128 | Train Acc: 0.9318 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2451 | Train Acc: 0.9318 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2010 | Train Acc: 0.9394 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2113 | Train Acc: 0.9470 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2310 | Train Acc: 0.9394 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.1853 | Train Acc: 0.9318 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2111 | Train Acc: 0.9242 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2682 | Train Acc: 0.9394 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.1917 | Train Acc: 0.9621 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2220 | Train Acc: 0.9470 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2176 | Train Acc: 0.9394 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2589 | Train Acc: 0.9318 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2391 | Train Acc: 0.9545 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2229 | Train Acc: 0.9697 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2257 | Train Acc: 0.9470 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2279 | Train Acc: 0.9242 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2261 | Train Acc: 0.9318 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2028 | Train Acc: 0.9545 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2343 | Train Acc: 0.9621 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.2248 | Train Acc: 0.9470 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.2041 | Train Acc: 0.9470 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.1950 | Train Acc: 0.9394 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2162 | Train Acc: 0.9394 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.1898 | Train Acc: 0.9318 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2209 | Train Acc: 0.9545 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2037 | Train Acc: 0.9470 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2710 | Train Acc: 0.9545 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.1956 | Train Acc: 0.9470 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2344 | Train Acc: 0.9470 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.1922 | Train Acc: 0.9545 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2067 | Train Acc: 0.9394 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2124 | Train Acc: 0.9470 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.1822 | Train Acc: 0.9621 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2130 | Train Acc: 0.9621 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2043 | Train Acc: 0.9545 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.1777 | Train Acc: 0.9470 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2250 | Train Acc: 0.9545 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2248 | Train Acc: 0.9621 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2377 | Train Acc: 0.9545 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.1913 | Train Acc: 0.9470 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.1897 | Train Acc: 0.9470 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2146 | Train Acc: 0.9318 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2124 | Train Acc: 0.9470 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.1940 | Train Acc: 0.9545 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2089 | Train Acc: 0.9621 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2044 | Train Acc: 0.9545 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.1891 | Train Acc: 0.9470 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2217 | Train Acc: 0.9470 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.1659 | Train Acc: 0.9697 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.1723 | Train Acc: 0.9697 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.1935 | Train Acc: 0.9697 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.1634 | Train Acc: 0.9470 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2038 | Train Acc: 0.9621 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2183 | Train Acc: 0.9697 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.1783 | Train Acc: 0.9621 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2031 | Train Acc: 0.9621 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2252 | Train Acc: 0.9545 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.1350 | Train Acc: 0.9545 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2108 | Train Acc: 0.9545 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.1579 | Train Acc: 0.9621 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.1474 | Train Acc: 0.9621 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.1784 | Train Acc: 0.9621 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.1720 | Train Acc: 0.9621 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2022 | Train Acc: 0.9545 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.1980 | Train Acc: 0.9621 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.1666 | Train Acc: 0.9621 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.1809 | Train Acc: 0.9697 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.1594 | Train Acc: 0.9697 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.1858 | Train Acc: 0.9773 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.1799 | Train Acc: 0.9318 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.1592 | Train Acc: 0.9545 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.1856 | Train Acc: 0.9697 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.1716 | Train Acc: 0.9621 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2031 | Train Acc: 0.9621 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2191 | Train Acc: 0.9470 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.1862 | Train Acc: 0.9621 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.1475 | Train Acc: 0.9697 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.1729 | Train Acc: 0.9697 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.1819 | Train Acc: 0.9545 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.1777 | Train Acc: 0.9545 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.1915 | Train Acc: 0.9621 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.1649 | Train Acc: 0.9697 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.1662 | Train Acc: 0.9697 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.1692 | Train Acc: 0.9545 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.1751 | Train Acc: 0.9545 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.1919 | Train Acc: 0.9697 | Test Acc: 0.1389\n",
      "\n",
      "--- Fold 3 ---\n",
      "Fold 3 | Iteration 500 | Loss: 2.3883 | Train Acc: 0.1364 | Test Acc: 0.1389\n",
      "Fold 3 | Iteration 500 | Loss: 2.3694 | Train Acc: 0.1364 | Test Acc: 0.1389\n",
      "Fold 3 | Iteration 500 | Loss: 2.3596 | Train Acc: 0.1364 | Test Acc: 0.1389\n",
      "Fold 3 | Iteration 500 | Loss: 2.3473 | Train Acc: 0.1364 | Test Acc: 0.1389\n",
      "Fold 3 | Iteration 500 | Loss: 2.3304 | Train Acc: 0.1364 | Test Acc: 0.1389\n",
      "Fold 3 | Iteration 500 | Loss: 2.3140 | Train Acc: 0.1364 | Test Acc: 0.1389\n",
      "Fold 3 | Iteration 500 | Loss: 2.2994 | Train Acc: 0.1364 | Test Acc: 0.1389\n",
      "Fold 3 | Iteration 500 | Loss: 2.2849 | Train Acc: 0.1364 | Test Acc: 0.1389\n",
      "Fold 3 | Iteration 500 | Loss: 2.2530 | Train Acc: 0.1364 | Test Acc: 0.1389\n",
      "Fold 3 | Iteration 500 | Loss: 2.2200 | Train Acc: 0.1364 | Test Acc: 0.1389\n",
      "Fold 3 | Iteration 500 | Loss: 2.1934 | Train Acc: 0.1591 | Test Acc: 0.1389\n",
      "Fold 3 | Iteration 500 | Loss: 2.1642 | Train Acc: 0.1591 | Test Acc: 0.1389\n",
      "Fold 3 | Iteration 500 | Loss: 2.1193 | Train Acc: 0.1667 | Test Acc: 0.1389\n",
      "Fold 3 | Iteration 500 | Loss: 2.0755 | Train Acc: 0.1742 | Test Acc: 0.1389\n",
      "Fold 3 | Iteration 500 | Loss: 2.0361 | Train Acc: 0.2348 | Test Acc: 0.1111\n",
      "Fold 3 | Iteration 500 | Loss: 2.0022 | Train Acc: 0.2955 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 1.9604 | Train Acc: 0.2879 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 1.9486 | Train Acc: 0.2879 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 1.8964 | Train Acc: 0.3030 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 1.9013 | Train Acc: 0.2879 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.9015 | Train Acc: 0.3106 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.8883 | Train Acc: 0.3106 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 1.9042 | Train Acc: 0.3106 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.8942 | Train Acc: 0.3182 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 1.8652 | Train Acc: 0.3182 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 1.8566 | Train Acc: 0.3333 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 1.8292 | Train Acc: 0.3333 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 1.8454 | Train Acc: 0.3333 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 1.8157 | Train Acc: 0.3258 | Test Acc: 0.1389\n",
      "Fold 3 | Iteration 500 | Loss: 1.8461 | Train Acc: 0.3333 | Test Acc: 0.1389\n",
      "Fold 3 | Iteration 500 | Loss: 1.8263 | Train Acc: 0.3561 | Test Acc: 0.1389\n",
      "Fold 3 | Iteration 500 | Loss: 1.7950 | Train Acc: 0.3561 | Test Acc: 0.1389\n",
      "Fold 3 | Iteration 500 | Loss: 1.8036 | Train Acc: 0.3636 | Test Acc: 0.1389\n",
      "Fold 3 | Iteration 500 | Loss: 1.7943 | Train Acc: 0.3636 | Test Acc: 0.1389\n",
      "Fold 3 | Iteration 500 | Loss: 1.8193 | Train Acc: 0.3636 | Test Acc: 0.1389\n",
      "Fold 3 | Iteration 500 | Loss: 1.7853 | Train Acc: 0.3561 | Test Acc: 0.1389\n",
      "Fold 3 | Iteration 500 | Loss: 1.7853 | Train Acc: 0.3485 | Test Acc: 0.1389\n",
      "Fold 3 | Iteration 500 | Loss: 1.8100 | Train Acc: 0.3409 | Test Acc: 0.1389\n",
      "Fold 3 | Iteration 500 | Loss: 1.7552 | Train Acc: 0.3409 | Test Acc: 0.1389\n",
      "Fold 3 | Iteration 500 | Loss: 1.7712 | Train Acc: 0.3409 | Test Acc: 0.1389\n",
      "Fold 3 | Iteration 500 | Loss: 1.7312 | Train Acc: 0.3409 | Test Acc: 0.1389\n",
      "Fold 3 | Iteration 500 | Loss: 1.7341 | Train Acc: 0.3485 | Test Acc: 0.1389\n",
      "Fold 3 | Iteration 500 | Loss: 1.7627 | Train Acc: 0.3409 | Test Acc: 0.1389\n",
      "Fold 3 | Iteration 500 | Loss: 1.7768 | Train Acc: 0.3409 | Test Acc: 0.1389\n",
      "Fold 3 | Iteration 500 | Loss: 1.7487 | Train Acc: 0.3409 | Test Acc: 0.1389\n",
      "Fold 3 | Iteration 500 | Loss: 1.7495 | Train Acc: 0.3409 | Test Acc: 0.1389\n",
      "Fold 3 | Iteration 500 | Loss: 1.7254 | Train Acc: 0.3333 | Test Acc: 0.1389\n",
      "Fold 3 | Iteration 500 | Loss: 1.7091 | Train Acc: 0.3409 | Test Acc: 0.1389\n",
      "Fold 3 | Iteration 500 | Loss: 1.7105 | Train Acc: 0.3485 | Test Acc: 0.1389\n",
      "Fold 3 | Iteration 500 | Loss: 1.7332 | Train Acc: 0.3561 | Test Acc: 0.1389\n",
      "Fold 3 | Iteration 500 | Loss: 1.7287 | Train Acc: 0.3636 | Test Acc: 0.1389\n",
      "Fold 3 | Iteration 500 | Loss: 1.7288 | Train Acc: 0.3712 | Test Acc: 0.1389\n",
      "Fold 3 | Iteration 500 | Loss: 1.7402 | Train Acc: 0.3712 | Test Acc: 0.1389\n",
      "Fold 3 | Iteration 500 | Loss: 1.6990 | Train Acc: 0.3788 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 1.6901 | Train Acc: 0.3788 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 1.7152 | Train Acc: 0.3788 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 1.7062 | Train Acc: 0.3864 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 1.6719 | Train Acc: 0.3939 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 1.6794 | Train Acc: 0.4015 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 1.6520 | Train Acc: 0.4091 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 1.6883 | Train Acc: 0.4091 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 1.6489 | Train Acc: 0.4167 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 1.6305 | Train Acc: 0.4015 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 1.6392 | Train Acc: 0.4091 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 1.6467 | Train Acc: 0.4242 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 1.6439 | Train Acc: 0.4015 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 1.6328 | Train Acc: 0.4015 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 1.6235 | Train Acc: 0.4015 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 1.6239 | Train Acc: 0.4015 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 1.5849 | Train Acc: 0.4091 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.6055 | Train Acc: 0.4242 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.6231 | Train Acc: 0.4167 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.6392 | Train Acc: 0.4167 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.5707 | Train Acc: 0.4091 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.5765 | Train Acc: 0.4091 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.5677 | Train Acc: 0.4167 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.5530 | Train Acc: 0.4242 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.5776 | Train Acc: 0.4242 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.5917 | Train Acc: 0.4167 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.5872 | Train Acc: 0.4167 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.5987 | Train Acc: 0.4091 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.5390 | Train Acc: 0.4167 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.5360 | Train Acc: 0.4167 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.5159 | Train Acc: 0.4167 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.5116 | Train Acc: 0.4167 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.5179 | Train Acc: 0.4167 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.4987 | Train Acc: 0.4167 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.5385 | Train Acc: 0.4242 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.4954 | Train Acc: 0.4242 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.4932 | Train Acc: 0.4318 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.5006 | Train Acc: 0.4242 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.4683 | Train Acc: 0.4242 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.4924 | Train Acc: 0.4242 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.4567 | Train Acc: 0.4394 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.4914 | Train Acc: 0.4621 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.4493 | Train Acc: 0.4697 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.4320 | Train Acc: 0.4773 | Test Acc: 0.2500\n",
      "Fold 3 | Iteration 500 | Loss: 1.4518 | Train Acc: 0.4470 | Test Acc: 0.2500\n",
      "Fold 3 | Iteration 500 | Loss: 1.4449 | Train Acc: 0.4545 | Test Acc: 0.2500\n",
      "Fold 3 | Iteration 500 | Loss: 1.4178 | Train Acc: 0.4621 | Test Acc: 0.2500\n",
      "Fold 3 | Iteration 500 | Loss: 1.4560 | Train Acc: 0.4697 | Test Acc: 0.2500\n",
      "Fold 3 | Iteration 500 | Loss: 1.4246 | Train Acc: 0.4773 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.3783 | Train Acc: 0.4697 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.4060 | Train Acc: 0.4848 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.3970 | Train Acc: 0.4848 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.3710 | Train Acc: 0.4848 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.3176 | Train Acc: 0.5000 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.3340 | Train Acc: 0.5000 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.3395 | Train Acc: 0.5000 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.3536 | Train Acc: 0.5000 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.3164 | Train Acc: 0.5000 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.3339 | Train Acc: 0.5227 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 1.3279 | Train Acc: 0.5076 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 1.3087 | Train Acc: 0.5076 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 1.3010 | Train Acc: 0.5303 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 1.3082 | Train Acc: 0.5455 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.2898 | Train Acc: 0.5303 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.2641 | Train Acc: 0.5455 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.2749 | Train Acc: 0.5606 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 1.2516 | Train Acc: 0.5682 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 1.2527 | Train Acc: 0.5455 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 1.2554 | Train Acc: 0.5833 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 1.2496 | Train Acc: 0.5682 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 1.2212 | Train Acc: 0.5833 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 1.2057 | Train Acc: 0.5758 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 1.2154 | Train Acc: 0.5682 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 1.2110 | Train Acc: 0.5606 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 1.1958 | Train Acc: 0.5530 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 1.2103 | Train Acc: 0.5606 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 1.2145 | Train Acc: 0.5606 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 1.1515 | Train Acc: 0.5833 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 1.1740 | Train Acc: 0.5758 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 1.1667 | Train Acc: 0.5758 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 1.1645 | Train Acc: 0.5530 | Test Acc: 0.1389\n",
      "Fold 3 | Iteration 500 | Loss: 1.1842 | Train Acc: 0.5530 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 1.1495 | Train Acc: 0.5606 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 1.1264 | Train Acc: 0.5833 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 1.1473 | Train Acc: 0.5758 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 1.1072 | Train Acc: 0.5758 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 1.0842 | Train Acc: 0.5985 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 1.1197 | Train Acc: 0.5985 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 1.0919 | Train Acc: 0.5909 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 1.0852 | Train Acc: 0.5985 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 1.0562 | Train Acc: 0.5833 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 1.0451 | Train Acc: 0.6212 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 1.0585 | Train Acc: 0.6136 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 1.0560 | Train Acc: 0.5909 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 1.0310 | Train Acc: 0.6061 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 1.0693 | Train Acc: 0.6288 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 1.0357 | Train Acc: 0.6212 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 1.0205 | Train Acc: 0.6136 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 1.0130 | Train Acc: 0.6364 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.9731 | Train Acc: 0.6212 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 1.0076 | Train Acc: 0.6515 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.9505 | Train Acc: 0.6515 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 1.0075 | Train Acc: 0.6439 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.9432 | Train Acc: 0.6591 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.9712 | Train Acc: 0.6439 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.9221 | Train Acc: 0.6288 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.9679 | Train Acc: 0.6439 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.8705 | Train Acc: 0.6364 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.9269 | Train Acc: 0.6439 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.9144 | Train Acc: 0.6591 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.9122 | Train Acc: 0.6970 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.9213 | Train Acc: 0.7045 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.9029 | Train Acc: 0.6818 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.8965 | Train Acc: 0.6439 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.8884 | Train Acc: 0.6515 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.9014 | Train Acc: 0.6970 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.8813 | Train Acc: 0.6894 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.8850 | Train Acc: 0.6894 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.9056 | Train Acc: 0.7045 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.9311 | Train Acc: 0.6970 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.8622 | Train Acc: 0.7045 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.9059 | Train Acc: 0.6818 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.7641 | Train Acc: 0.6894 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.8509 | Train Acc: 0.7121 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.8216 | Train Acc: 0.7273 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.8569 | Train Acc: 0.7121 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.8521 | Train Acc: 0.7045 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.8287 | Train Acc: 0.6970 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.8180 | Train Acc: 0.7197 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.7818 | Train Acc: 0.7121 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.7715 | Train Acc: 0.7121 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.7750 | Train Acc: 0.7273 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.8102 | Train Acc: 0.7348 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.7638 | Train Acc: 0.7500 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.7607 | Train Acc: 0.7348 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.7631 | Train Acc: 0.7424 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.7454 | Train Acc: 0.7576 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.7183 | Train Acc: 0.7576 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.7438 | Train Acc: 0.7652 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.7348 | Train Acc: 0.7727 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.7450 | Train Acc: 0.7803 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.7290 | Train Acc: 0.8030 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.7510 | Train Acc: 0.7803 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.6632 | Train Acc: 0.7727 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.6627 | Train Acc: 0.7803 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.6742 | Train Acc: 0.7727 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.6657 | Train Acc: 0.8182 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.6799 | Train Acc: 0.7727 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.6996 | Train Acc: 0.8030 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.6599 | Train Acc: 0.8030 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.6762 | Train Acc: 0.7727 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.6893 | Train Acc: 0.7803 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.7166 | Train Acc: 0.7879 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.6430 | Train Acc: 0.7879 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.6708 | Train Acc: 0.8182 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.6551 | Train Acc: 0.8182 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.6090 | Train Acc: 0.7727 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.6693 | Train Acc: 0.7879 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.6553 | Train Acc: 0.7955 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.6215 | Train Acc: 0.8106 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.6261 | Train Acc: 0.8182 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.6441 | Train Acc: 0.8182 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.5979 | Train Acc: 0.8106 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.6247 | Train Acc: 0.7955 | Test Acc: 0.2500\n",
      "Fold 3 | Iteration 500 | Loss: 0.6353 | Train Acc: 0.7955 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.6270 | Train Acc: 0.8258 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.6099 | Train Acc: 0.8333 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.6042 | Train Acc: 0.8409 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.5946 | Train Acc: 0.8409 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.6164 | Train Acc: 0.8333 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.5842 | Train Acc: 0.8409 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.5783 | Train Acc: 0.8258 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.5750 | Train Acc: 0.8409 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.6016 | Train Acc: 0.8409 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.5507 | Train Acc: 0.8409 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.5235 | Train Acc: 0.8333 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.5645 | Train Acc: 0.8409 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.5842 | Train Acc: 0.8409 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.5306 | Train Acc: 0.8333 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.5524 | Train Acc: 0.8333 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.5325 | Train Acc: 0.8409 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.5637 | Train Acc: 0.8258 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.5565 | Train Acc: 0.8561 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.5314 | Train Acc: 0.8561 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.5294 | Train Acc: 0.8636 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.5296 | Train Acc: 0.8788 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.5517 | Train Acc: 0.8712 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.5280 | Train Acc: 0.8561 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.5248 | Train Acc: 0.8788 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.4760 | Train Acc: 0.8561 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.5363 | Train Acc: 0.8561 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.4711 | Train Acc: 0.8939 | Test Acc: 0.2500\n",
      "Fold 3 | Iteration 500 | Loss: 0.4865 | Train Acc: 0.8712 | Test Acc: 0.2500\n",
      "Fold 3 | Iteration 500 | Loss: 0.5111 | Train Acc: 0.8561 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.5227 | Train Acc: 0.8636 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.5093 | Train Acc: 0.8864 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.4796 | Train Acc: 0.8788 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.4992 | Train Acc: 0.8485 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.4852 | Train Acc: 0.8561 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.4992 | Train Acc: 0.8939 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.4518 | Train Acc: 0.8788 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.5044 | Train Acc: 0.8636 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.4465 | Train Acc: 0.8864 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.4614 | Train Acc: 0.8864 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.4718 | Train Acc: 0.8939 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.4010 | Train Acc: 0.8864 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.4209 | Train Acc: 0.8788 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.4556 | Train Acc: 0.8939 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.4121 | Train Acc: 0.8939 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.4333 | Train Acc: 0.8939 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.4299 | Train Acc: 0.8939 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.4814 | Train Acc: 0.8788 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.4737 | Train Acc: 0.8939 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.4190 | Train Acc: 0.9015 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.4101 | Train Acc: 0.8939 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.3937 | Train Acc: 0.9015 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.4029 | Train Acc: 0.9015 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.4281 | Train Acc: 0.8939 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.4289 | Train Acc: 0.8939 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.4315 | Train Acc: 0.8939 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.4435 | Train Acc: 0.8939 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.4056 | Train Acc: 0.9015 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.4287 | Train Acc: 0.9015 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.4140 | Train Acc: 0.9015 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.3650 | Train Acc: 0.9015 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.3464 | Train Acc: 0.8788 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.4275 | Train Acc: 0.8864 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.3584 | Train Acc: 0.9091 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.4221 | Train Acc: 0.9167 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.3988 | Train Acc: 0.9167 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.4195 | Train Acc: 0.9318 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.3771 | Train Acc: 0.9091 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.3801 | Train Acc: 0.9015 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.3535 | Train Acc: 0.8864 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.3776 | Train Acc: 0.9015 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.3568 | Train Acc: 0.9091 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.3772 | Train Acc: 0.9167 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.3546 | Train Acc: 0.9091 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.3677 | Train Acc: 0.9242 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.3752 | Train Acc: 0.9318 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.3627 | Train Acc: 0.9242 | Test Acc: 0.2500\n",
      "Fold 3 | Iteration 500 | Loss: 0.3678 | Train Acc: 0.9318 | Test Acc: 0.2500\n",
      "Fold 3 | Iteration 500 | Loss: 0.3642 | Train Acc: 0.9091 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.3710 | Train Acc: 0.9167 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.3434 | Train Acc: 0.9167 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.3696 | Train Acc: 0.9091 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2995 | Train Acc: 0.9091 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.3324 | Train Acc: 0.9242 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.2835 | Train Acc: 0.9318 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.3307 | Train Acc: 0.9318 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.3163 | Train Acc: 0.9318 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.3211 | Train Acc: 0.9318 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.3442 | Train Acc: 0.9242 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.3508 | Train Acc: 0.9167 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.3426 | Train Acc: 0.9318 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.3536 | Train Acc: 0.9242 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.3133 | Train Acc: 0.9318 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.3131 | Train Acc: 0.9394 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.3402 | Train Acc: 0.9242 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.3378 | Train Acc: 0.9242 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.3100 | Train Acc: 0.9242 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2934 | Train Acc: 0.9242 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.3164 | Train Acc: 0.9242 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.3382 | Train Acc: 0.9318 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2484 | Train Acc: 0.9318 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.3180 | Train Acc: 0.9318 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.3178 | Train Acc: 0.9394 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.3134 | Train Acc: 0.9167 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.3511 | Train Acc: 0.9318 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2902 | Train Acc: 0.9318 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2997 | Train Acc: 0.9318 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.3246 | Train Acc: 0.9394 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2727 | Train Acc: 0.9394 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2700 | Train Acc: 0.9394 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2978 | Train Acc: 0.9394 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.3011 | Train Acc: 0.9394 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2634 | Train Acc: 0.9470 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2737 | Train Acc: 0.9394 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.2860 | Train Acc: 0.9394 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2845 | Train Acc: 0.9015 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.3391 | Train Acc: 0.9318 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.3100 | Train Acc: 0.9318 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.2190 | Train Acc: 0.9318 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.2806 | Train Acc: 0.9394 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2598 | Train Acc: 0.9318 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.3143 | Train Acc: 0.9394 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2564 | Train Acc: 0.9394 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.2991 | Train Acc: 0.9318 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.3075 | Train Acc: 0.9470 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2811 | Train Acc: 0.9470 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.3052 | Train Acc: 0.9394 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.3219 | Train Acc: 0.9470 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2409 | Train Acc: 0.9470 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2676 | Train Acc: 0.9545 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.2887 | Train Acc: 0.9545 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.2526 | Train Acc: 0.9470 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.2410 | Train Acc: 0.9470 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2796 | Train Acc: 0.9545 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2613 | Train Acc: 0.9470 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2368 | Train Acc: 0.9545 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2692 | Train Acc: 0.9470 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2713 | Train Acc: 0.9394 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2351 | Train Acc: 0.9470 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2115 | Train Acc: 0.9545 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2725 | Train Acc: 0.9621 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.2456 | Train Acc: 0.9545 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.2516 | Train Acc: 0.9545 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2713 | Train Acc: 0.9318 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2445 | Train Acc: 0.9394 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2564 | Train Acc: 0.9545 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2217 | Train Acc: 0.9394 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2301 | Train Acc: 0.9470 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2014 | Train Acc: 0.9470 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2543 | Train Acc: 0.9545 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2289 | Train Acc: 0.9545 | Test Acc: 0.1389\n",
      "Fold 3 | Iteration 500 | Loss: 0.2411 | Train Acc: 0.9470 | Test Acc: 0.1389\n",
      "Fold 3 | Iteration 500 | Loss: 0.2535 | Train Acc: 0.9545 | Test Acc: 0.1389\n",
      "Fold 3 | Iteration 500 | Loss: 0.2030 | Train Acc: 0.9394 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.2141 | Train Acc: 0.9318 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.2385 | Train Acc: 0.9470 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.2353 | Train Acc: 0.9470 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.2398 | Train Acc: 0.9470 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.2331 | Train Acc: 0.9470 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2552 | Train Acc: 0.9545 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2214 | Train Acc: 0.9545 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.2438 | Train Acc: 0.9394 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.2822 | Train Acc: 0.9470 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2465 | Train Acc: 0.9545 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.2068 | Train Acc: 0.9470 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.2475 | Train Acc: 0.9470 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2389 | Train Acc: 0.9394 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2245 | Train Acc: 0.9470 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2416 | Train Acc: 0.9621 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.2461 | Train Acc: 0.9545 | Test Acc: 0.1389\n",
      "Fold 3 | Iteration 500 | Loss: 0.2324 | Train Acc: 0.9470 | Test Acc: 0.1389\n",
      "Fold 3 | Iteration 500 | Loss: 0.2236 | Train Acc: 0.9545 | Test Acc: 0.1389\n",
      "Fold 3 | Iteration 500 | Loss: 0.1909 | Train Acc: 0.9621 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.1981 | Train Acc: 0.9545 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2699 | Train Acc: 0.9470 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2134 | Train Acc: 0.9545 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.1993 | Train Acc: 0.9621 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.2061 | Train Acc: 0.9621 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.1917 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.2066 | Train Acc: 0.9621 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2052 | Train Acc: 0.9545 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2297 | Train Acc: 0.9621 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2266 | Train Acc: 0.9545 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2170 | Train Acc: 0.9545 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.2270 | Train Acc: 0.9697 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2050 | Train Acc: 0.9621 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1813 | Train Acc: 0.9621 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2429 | Train Acc: 0.9697 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1825 | Train Acc: 0.9697 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1833 | Train Acc: 0.9697 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1665 | Train Acc: 0.9621 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.2150 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.1746 | Train Acc: 0.9621 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.1768 | Train Acc: 0.9621 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.2020 | Train Acc: 0.9621 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.1710 | Train Acc: 0.9545 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.1937 | Train Acc: 0.9621 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.1758 | Train Acc: 0.9545 | Test Acc: 0.1389\n",
      "Fold 3 | Iteration 500 | Loss: 0.1603 | Train Acc: 0.9621 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1626 | Train Acc: 0.9697 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.1644 | Train Acc: 0.9697 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.2060 | Train Acc: 0.9697 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.1793 | Train Acc: 0.9697 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.1755 | Train Acc: 0.9697 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1312 | Train Acc: 0.9697 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2061 | Train Acc: 0.9621 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.1455 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.1679 | Train Acc: 0.9621 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.1738 | Train Acc: 0.9621 | Test Acc: 0.1389\n",
      "Fold 3 | Iteration 500 | Loss: 0.2194 | Train Acc: 0.9621 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.1831 | Train Acc: 0.9621 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.1844 | Train Acc: 0.9621 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.1730 | Train Acc: 0.9545 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.1383 | Train Acc: 0.9545 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.1825 | Train Acc: 0.9621 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.1685 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.1864 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.1726 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.1590 | Train Acc: 0.9621 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.1816 | Train Acc: 0.9621 | Test Acc: 0.1389\n",
      "Fold 3 | Iteration 500 | Loss: 0.1787 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.1689 | Train Acc: 0.9545 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.1709 | Train Acc: 0.9621 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.1959 | Train Acc: 0.9621 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1479 | Train Acc: 0.9697 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1598 | Train Acc: 0.9545 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.1672 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.1883 | Train Acc: 0.9621 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.1842 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1568 | Train Acc: 0.9697 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1811 | Train Acc: 0.9697 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1761 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.1657 | Train Acc: 0.9545 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1676 | Train Acc: 0.9697 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1650 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.1458 | Train Acc: 0.9697 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1303 | Train Acc: 0.9621 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1588 | Train Acc: 0.9697 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1458 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1866 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1365 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.1592 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.1671 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.1503 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.1794 | Train Acc: 0.9697 | Test Acc: 0.1389\n",
      "Fold 3 | Iteration 500 | Loss: 0.1817 | Train Acc: 0.9697 | Test Acc: 0.1389\n",
      "Fold 3 | Iteration 500 | Loss: 0.1438 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.1728 | Train Acc: 0.9697 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1480 | Train Acc: 0.9621 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1655 | Train Acc: 0.9545 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1725 | Train Acc: 0.9621 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1364 | Train Acc: 0.9697 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1585 | Train Acc: 0.9697 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1664 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1554 | Train Acc: 0.9621 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.1456 | Train Acc: 0.9621 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.1427 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.1753 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.1472 | Train Acc: 0.9621 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.1660 | Train Acc: 0.9621 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1519 | Train Acc: 0.9697 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1373 | Train Acc: 0.9697 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1496 | Train Acc: 0.9621 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1669 | Train Acc: 0.9621 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1588 | Train Acc: 0.9697 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1080 | Train Acc: 0.9773 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.1148 | Train Acc: 0.9773 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.1400 | Train Acc: 0.9773 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.1174 | Train Acc: 0.9773 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.1350 | Train Acc: 0.9697 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.1524 | Train Acc: 0.9621 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.1373 | Train Acc: 0.9697 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.1370 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1616 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1514 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1281 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1435 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1551 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1326 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1128 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1291 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1304 | Train Acc: 0.9697 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1471 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "\n",
      "--- Fold 4 ---\n",
      "Fold 4 | Iteration 500 | Loss: 2.4149 | Train Acc: 0.0606 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 2.4020 | Train Acc: 0.0682 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 2.3882 | Train Acc: 0.1439 | Test Acc: 0.2222\n",
      "Fold 4 | Iteration 500 | Loss: 2.3777 | Train Acc: 0.1894 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 2.3644 | Train Acc: 0.1894 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 2.3509 | Train Acc: 0.1894 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 2.3382 | Train Acc: 0.1894 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 2.3153 | Train Acc: 0.1894 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 2.2963 | Train Acc: 0.1894 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 2.2681 | Train Acc: 0.1970 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 2.2408 | Train Acc: 0.2045 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 2.1947 | Train Acc: 0.2273 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 2.1509 | Train Acc: 0.2652 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 2.1064 | Train Acc: 0.3182 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 2.0610 | Train Acc: 0.2652 | Test Acc: 0.2778\n",
      "Fold 4 | Iteration 500 | Loss: 1.9938 | Train Acc: 0.2727 | Test Acc: 0.2500\n",
      "Fold 4 | Iteration 500 | Loss: 1.9625 | Train Acc: 0.2727 | Test Acc: 0.2500\n",
      "Fold 4 | Iteration 500 | Loss: 1.9329 | Train Acc: 0.2727 | Test Acc: 0.2500\n",
      "Fold 4 | Iteration 500 | Loss: 1.9091 | Train Acc: 0.2727 | Test Acc: 0.2500\n",
      "Fold 4 | Iteration 500 | Loss: 1.9118 | Train Acc: 0.2727 | Test Acc: 0.2500\n",
      "Fold 4 | Iteration 500 | Loss: 1.9414 | Train Acc: 0.2727 | Test Acc: 0.2500\n",
      "Fold 4 | Iteration 500 | Loss: 1.8849 | Train Acc: 0.2803 | Test Acc: 0.2500\n",
      "Fold 4 | Iteration 500 | Loss: 1.9018 | Train Acc: 0.2803 | Test Acc: 0.2500\n",
      "Fold 4 | Iteration 500 | Loss: 1.8480 | Train Acc: 0.2803 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.8891 | Train Acc: 0.2955 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.8750 | Train Acc: 0.3258 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.8628 | Train Acc: 0.3106 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.8485 | Train Acc: 0.3106 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.8176 | Train Acc: 0.3106 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.8107 | Train Acc: 0.3106 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.7985 | Train Acc: 0.2955 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.7966 | Train Acc: 0.2955 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.8164 | Train Acc: 0.2955 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.7904 | Train Acc: 0.2955 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.7681 | Train Acc: 0.3182 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.7603 | Train Acc: 0.3182 | Test Acc: 0.2222\n",
      "Fold 4 | Iteration 500 | Loss: 1.7680 | Train Acc: 0.3182 | Test Acc: 0.2222\n",
      "Fold 4 | Iteration 500 | Loss: 1.7529 | Train Acc: 0.3258 | Test Acc: 0.2222\n",
      "Fold 4 | Iteration 500 | Loss: 1.7816 | Train Acc: 0.3409 | Test Acc: 0.2222\n",
      "Fold 4 | Iteration 500 | Loss: 1.7314 | Train Acc: 0.3333 | Test Acc: 0.2222\n",
      "Fold 4 | Iteration 500 | Loss: 1.7541 | Train Acc: 0.3258 | Test Acc: 0.2222\n",
      "Fold 4 | Iteration 500 | Loss: 1.7726 | Train Acc: 0.3333 | Test Acc: 0.2222\n",
      "Fold 4 | Iteration 500 | Loss: 1.7272 | Train Acc: 0.3409 | Test Acc: 0.2500\n",
      "Fold 4 | Iteration 500 | Loss: 1.7073 | Train Acc: 0.3409 | Test Acc: 0.2500\n",
      "Fold 4 | Iteration 500 | Loss: 1.7186 | Train Acc: 0.3409 | Test Acc: 0.2500\n",
      "Fold 4 | Iteration 500 | Loss: 1.7079 | Train Acc: 0.3333 | Test Acc: 0.2500\n",
      "Fold 4 | Iteration 500 | Loss: 1.7294 | Train Acc: 0.3485 | Test Acc: 0.2222\n",
      "Fold 4 | Iteration 500 | Loss: 1.7319 | Train Acc: 0.3561 | Test Acc: 0.2222\n",
      "Fold 4 | Iteration 500 | Loss: 1.7138 | Train Acc: 0.3636 | Test Acc: 0.2222\n",
      "Fold 4 | Iteration 500 | Loss: 1.7118 | Train Acc: 0.3712 | Test Acc: 0.2222\n",
      "Fold 4 | Iteration 500 | Loss: 1.6870 | Train Acc: 0.3712 | Test Acc: 0.2500\n",
      "Fold 4 | Iteration 500 | Loss: 1.7175 | Train Acc: 0.3788 | Test Acc: 0.2222\n",
      "Fold 4 | Iteration 500 | Loss: 1.7053 | Train Acc: 0.3788 | Test Acc: 0.2222\n",
      "Fold 4 | Iteration 500 | Loss: 1.6647 | Train Acc: 0.4015 | Test Acc: 0.2222\n",
      "Fold 4 | Iteration 500 | Loss: 1.6607 | Train Acc: 0.4091 | Test Acc: 0.2222\n",
      "Fold 4 | Iteration 500 | Loss: 1.6730 | Train Acc: 0.4015 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.6587 | Train Acc: 0.3939 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.6491 | Train Acc: 0.3864 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.6291 | Train Acc: 0.3939 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.6270 | Train Acc: 0.3939 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.6284 | Train Acc: 0.4091 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.6069 | Train Acc: 0.4167 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.6161 | Train Acc: 0.4318 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.6019 | Train Acc: 0.4167 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.6007 | Train Acc: 0.4091 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.5567 | Train Acc: 0.4242 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.5871 | Train Acc: 0.4242 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.5581 | Train Acc: 0.4318 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.5705 | Train Acc: 0.4318 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.5411 | Train Acc: 0.4394 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.5457 | Train Acc: 0.4470 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.4947 | Train Acc: 0.4621 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.5069 | Train Acc: 0.4621 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.5330 | Train Acc: 0.4545 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.4914 | Train Acc: 0.4621 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.5047 | Train Acc: 0.4697 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.4741 | Train Acc: 0.4697 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.4826 | Train Acc: 0.4697 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.4604 | Train Acc: 0.4848 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.4687 | Train Acc: 0.4848 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.4714 | Train Acc: 0.4924 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.4601 | Train Acc: 0.4848 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.4703 | Train Acc: 0.4697 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.4397 | Train Acc: 0.4697 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.3880 | Train Acc: 0.5000 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.4019 | Train Acc: 0.5076 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.3944 | Train Acc: 0.5152 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.3690 | Train Acc: 0.5152 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.4104 | Train Acc: 0.5076 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.3720 | Train Acc: 0.5076 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 1.4115 | Train Acc: 0.5076 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.3706 | Train Acc: 0.5152 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.3186 | Train Acc: 0.5076 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.3655 | Train Acc: 0.5303 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.4001 | Train Acc: 0.5303 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.3346 | Train Acc: 0.5455 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.3292 | Train Acc: 0.5227 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.3416 | Train Acc: 0.5227 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.3283 | Train Acc: 0.5303 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.3358 | Train Acc: 0.5455 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.2818 | Train Acc: 0.5303 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 1.3164 | Train Acc: 0.5303 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 1.3185 | Train Acc: 0.5379 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 1.2942 | Train Acc: 0.5530 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.2760 | Train Acc: 0.5530 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.3068 | Train Acc: 0.5606 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.2602 | Train Acc: 0.5530 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.2713 | Train Acc: 0.5455 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.2668 | Train Acc: 0.5606 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 1.2347 | Train Acc: 0.5833 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 1.2475 | Train Acc: 0.5682 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.2472 | Train Acc: 0.5530 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.2286 | Train Acc: 0.5606 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.1765 | Train Acc: 0.5682 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 1.2113 | Train Acc: 0.5758 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 1.2049 | Train Acc: 0.5758 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 1.2230 | Train Acc: 0.5833 | Test Acc: 0.2222\n",
      "Fold 4 | Iteration 500 | Loss: 1.2109 | Train Acc: 0.6061 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.1935 | Train Acc: 0.5909 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.1375 | Train Acc: 0.5909 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.1397 | Train Acc: 0.5833 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.1619 | Train Acc: 0.5985 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.1527 | Train Acc: 0.5985 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.1200 | Train Acc: 0.6136 | Test Acc: 0.2222\n",
      "Fold 4 | Iteration 500 | Loss: 1.1539 | Train Acc: 0.6136 | Test Acc: 0.2222\n",
      "Fold 4 | Iteration 500 | Loss: 1.1898 | Train Acc: 0.6212 | Test Acc: 0.2222\n",
      "Fold 4 | Iteration 500 | Loss: 1.1103 | Train Acc: 0.6212 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.0628 | Train Acc: 0.6136 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.1085 | Train Acc: 0.6212 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.0924 | Train Acc: 0.6288 | Test Acc: 0.2222\n",
      "Fold 4 | Iteration 500 | Loss: 1.0679 | Train Acc: 0.6212 | Test Acc: 0.2222\n",
      "Fold 4 | Iteration 500 | Loss: 1.0934 | Train Acc: 0.6212 | Test Acc: 0.2222\n",
      "Fold 4 | Iteration 500 | Loss: 1.0522 | Train Acc: 0.6439 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.0449 | Train Acc: 0.6136 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.1281 | Train Acc: 0.6288 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.0701 | Train Acc: 0.6439 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.0569 | Train Acc: 0.6364 | Test Acc: 0.2222\n",
      "Fold 4 | Iteration 500 | Loss: 1.0531 | Train Acc: 0.6515 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 0.9922 | Train Acc: 0.6439 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.0297 | Train Acc: 0.6591 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.0205 | Train Acc: 0.6439 | Test Acc: 0.2222\n",
      "Fold 4 | Iteration 500 | Loss: 1.0343 | Train Acc: 0.6515 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.0038 | Train Acc: 0.6591 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.0119 | Train Acc: 0.6591 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.0023 | Train Acc: 0.6364 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 0.9704 | Train Acc: 0.6439 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 0.9654 | Train Acc: 0.6288 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 0.9745 | Train Acc: 0.6667 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 0.9639 | Train Acc: 0.6742 | Test Acc: 0.2222\n",
      "Fold 4 | Iteration 500 | Loss: 0.9287 | Train Acc: 0.6742 | Test Acc: 0.2222\n",
      "Fold 4 | Iteration 500 | Loss: 0.9577 | Train Acc: 0.6742 | Test Acc: 0.2222\n",
      "Fold 4 | Iteration 500 | Loss: 0.9532 | Train Acc: 0.6818 | Test Acc: 0.2222\n",
      "Fold 4 | Iteration 500 | Loss: 0.9400 | Train Acc: 0.6818 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 0.9555 | Train Acc: 0.6818 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 0.8735 | Train Acc: 0.6742 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 0.8623 | Train Acc: 0.6894 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.9354 | Train Acc: 0.6894 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 0.9330 | Train Acc: 0.6894 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.9357 | Train Acc: 0.6970 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.9300 | Train Acc: 0.6970 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 0.8767 | Train Acc: 0.6818 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 0.8574 | Train Acc: 0.6818 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 0.8883 | Train Acc: 0.6818 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 0.8454 | Train Acc: 0.6894 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.8306 | Train Acc: 0.7121 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.8518 | Train Acc: 0.7273 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 0.8024 | Train Acc: 0.7273 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 0.8445 | Train Acc: 0.7273 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.8422 | Train Acc: 0.7273 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.8432 | Train Acc: 0.7121 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.8202 | Train Acc: 0.7197 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.8140 | Train Acc: 0.7197 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.8383 | Train Acc: 0.7348 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.8041 | Train Acc: 0.7197 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.8240 | Train Acc: 0.7197 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.7586 | Train Acc: 0.7348 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.8174 | Train Acc: 0.7348 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.7461 | Train Acc: 0.7273 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.8490 | Train Acc: 0.7273 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.7845 | Train Acc: 0.7500 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.8070 | Train Acc: 0.7500 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.7542 | Train Acc: 0.7348 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.7371 | Train Acc: 0.7424 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.7823 | Train Acc: 0.7500 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.7281 | Train Acc: 0.7500 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.7546 | Train Acc: 0.7500 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.7212 | Train Acc: 0.7652 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.6963 | Train Acc: 0.7803 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.6965 | Train Acc: 0.7727 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.7700 | Train Acc: 0.7803 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.7497 | Train Acc: 0.7727 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.7466 | Train Acc: 0.7576 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.7094 | Train Acc: 0.7652 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.6965 | Train Acc: 0.7727 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.6977 | Train Acc: 0.7652 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.6697 | Train Acc: 0.7879 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.7000 | Train Acc: 0.7955 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 0.6898 | Train Acc: 0.7803 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 0.6661 | Train Acc: 0.7803 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 0.6695 | Train Acc: 0.7879 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 0.6556 | Train Acc: 0.7879 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 0.6344 | Train Acc: 0.7955 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 0.6291 | Train Acc: 0.8030 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 0.6531 | Train Acc: 0.8106 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 0.6515 | Train Acc: 0.8030 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 0.6192 | Train Acc: 0.7955 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 0.6486 | Train Acc: 0.8106 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 0.6185 | Train Acc: 0.8258 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.6189 | Train Acc: 0.8258 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.6494 | Train Acc: 0.8258 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 0.6368 | Train Acc: 0.8030 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.6047 | Train Acc: 0.8030 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.5931 | Train Acc: 0.8030 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.5776 | Train Acc: 0.8258 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 0.5618 | Train Acc: 0.8333 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 0.5692 | Train Acc: 0.8409 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 0.5976 | Train Acc: 0.8106 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.5705 | Train Acc: 0.8106 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.5673 | Train Acc: 0.8182 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 0.5698 | Train Acc: 0.8258 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 0.6039 | Train Acc: 0.8258 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 0.5257 | Train Acc: 0.8182 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.5676 | Train Acc: 0.8258 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 0.5966 | Train Acc: 0.8258 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 0.5683 | Train Acc: 0.8333 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 0.5337 | Train Acc: 0.8258 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 0.5577 | Train Acc: 0.8409 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 0.5013 | Train Acc: 0.8485 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 0.5497 | Train Acc: 0.8636 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 0.5127 | Train Acc: 0.8485 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.5411 | Train Acc: 0.8561 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 0.5061 | Train Acc: 0.8561 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 0.4962 | Train Acc: 0.8561 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 0.5252 | Train Acc: 0.8409 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.5462 | Train Acc: 0.8561 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.5166 | Train Acc: 0.8636 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.4939 | Train Acc: 0.8561 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.5118 | Train Acc: 0.8409 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.4851 | Train Acc: 0.8333 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.4733 | Train Acc: 0.8561 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 0.4963 | Train Acc: 0.8561 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 0.4981 | Train Acc: 0.8561 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 0.5081 | Train Acc: 0.8636 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.4649 | Train Acc: 0.8636 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.4686 | Train Acc: 0.8636 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 0.4983 | Train Acc: 0.8636 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 0.4628 | Train Acc: 0.8636 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.4579 | Train Acc: 0.8636 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.4490 | Train Acc: 0.8485 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.4801 | Train Acc: 0.8788 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.4551 | Train Acc: 0.8864 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 0.4428 | Train Acc: 0.8864 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 0.4200 | Train Acc: 0.8864 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 0.4446 | Train Acc: 0.8788 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.4398 | Train Acc: 0.8636 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.4460 | Train Acc: 0.8561 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.4105 | Train Acc: 0.8788 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.4213 | Train Acc: 0.8864 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.4412 | Train Acc: 0.8864 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.4444 | Train Acc: 0.8939 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 0.4440 | Train Acc: 0.8788 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.4405 | Train Acc: 0.8864 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.4096 | Train Acc: 0.8864 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.3607 | Train Acc: 0.9015 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.3827 | Train Acc: 0.8939 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 0.4204 | Train Acc: 0.8939 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.3820 | Train Acc: 0.9015 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.4141 | Train Acc: 0.9091 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.4043 | Train Acc: 0.8939 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.3938 | Train Acc: 0.9015 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.3964 | Train Acc: 0.9167 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 0.4151 | Train Acc: 0.9091 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.3826 | Train Acc: 0.9091 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 0.3396 | Train Acc: 0.9091 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.3879 | Train Acc: 0.9091 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 0.3813 | Train Acc: 0.8939 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.3538 | Train Acc: 0.9091 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.3408 | Train Acc: 0.9167 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.3524 | Train Acc: 0.9091 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.3553 | Train Acc: 0.9167 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.3761 | Train Acc: 0.9318 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.3333 | Train Acc: 0.9167 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.3354 | Train Acc: 0.9091 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.3665 | Train Acc: 0.9091 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.3811 | Train Acc: 0.9167 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.3410 | Train Acc: 0.9167 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.3503 | Train Acc: 0.9242 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.3571 | Train Acc: 0.9242 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.3158 | Train Acc: 0.9242 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.3414 | Train Acc: 0.9091 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.3210 | Train Acc: 0.9242 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.3289 | Train Acc: 0.9318 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.2878 | Train Acc: 0.9167 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 0.3192 | Train Acc: 0.9394 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.3152 | Train Acc: 0.9318 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.3384 | Train Acc: 0.9318 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.3222 | Train Acc: 0.9318 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.3523 | Train Acc: 0.9242 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.2827 | Train Acc: 0.9242 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.3050 | Train Acc: 0.9091 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.3154 | Train Acc: 0.9318 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.3078 | Train Acc: 0.9318 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.3174 | Train Acc: 0.9242 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.2782 | Train Acc: 0.9318 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.3403 | Train Acc: 0.9394 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.2894 | Train Acc: 0.9394 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.3149 | Train Acc: 0.9318 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.2919 | Train Acc: 0.9167 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.3076 | Train Acc: 0.9394 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.2945 | Train Acc: 0.9394 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.3002 | Train Acc: 0.9394 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.2686 | Train Acc: 0.9394 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.2879 | Train Acc: 0.9394 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.2664 | Train Acc: 0.9318 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.2669 | Train Acc: 0.9318 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.2686 | Train Acc: 0.9318 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.2698 | Train Acc: 0.9394 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.2839 | Train Acc: 0.9394 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.2610 | Train Acc: 0.9394 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.2800 | Train Acc: 0.9394 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.2349 | Train Acc: 0.9394 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.2552 | Train Acc: 0.9394 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.2515 | Train Acc: 0.9394 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.2430 | Train Acc: 0.9394 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.2794 | Train Acc: 0.9394 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.2915 | Train Acc: 0.9394 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.2628 | Train Acc: 0.9394 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.2480 | Train Acc: 0.9394 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.2554 | Train Acc: 0.9394 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.2396 | Train Acc: 0.9394 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.2307 | Train Acc: 0.9470 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.2446 | Train Acc: 0.9470 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.2753 | Train Acc: 0.9470 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.2378 | Train Acc: 0.9470 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.2391 | Train Acc: 0.9394 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.2427 | Train Acc: 0.9394 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.2328 | Train Acc: 0.9470 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.2320 | Train Acc: 0.9394 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.2085 | Train Acc: 0.9470 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.2220 | Train Acc: 0.9394 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.2224 | Train Acc: 0.9394 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.2250 | Train Acc: 0.9394 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.2100 | Train Acc: 0.9394 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.2239 | Train Acc: 0.9394 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1995 | Train Acc: 0.9470 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.2182 | Train Acc: 0.9470 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.2102 | Train Acc: 0.9318 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.2267 | Train Acc: 0.9470 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.2017 | Train Acc: 0.9545 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.2464 | Train Acc: 0.9470 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.2114 | Train Acc: 0.9470 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.2317 | Train Acc: 0.9545 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.2153 | Train Acc: 0.9470 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.2184 | Train Acc: 0.9394 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.2104 | Train Acc: 0.9394 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.1894 | Train Acc: 0.9470 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.1911 | Train Acc: 0.9394 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.2029 | Train Acc: 0.9545 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.2221 | Train Acc: 0.9545 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.2244 | Train Acc: 0.9545 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.2037 | Train Acc: 0.9545 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1944 | Train Acc: 0.9394 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.2079 | Train Acc: 0.9545 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1844 | Train Acc: 0.9545 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.1865 | Train Acc: 0.9545 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.1590 | Train Acc: 0.9545 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.1771 | Train Acc: 0.9545 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.2159 | Train Acc: 0.9545 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1840 | Train Acc: 0.9621 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.1815 | Train Acc: 0.9545 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.1888 | Train Acc: 0.9470 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.1675 | Train Acc: 0.9621 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.2012 | Train Acc: 0.9621 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1889 | Train Acc: 0.9697 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.2059 | Train Acc: 0.9697 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.1822 | Train Acc: 0.9545 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1999 | Train Acc: 0.9545 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.1833 | Train Acc: 0.9545 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.1739 | Train Acc: 0.9621 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1784 | Train Acc: 0.9621 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.2052 | Train Acc: 0.9545 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1726 | Train Acc: 0.9621 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.1865 | Train Acc: 0.9545 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.1732 | Train Acc: 0.9545 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.1717 | Train Acc: 0.9545 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1590 | Train Acc: 0.9621 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1520 | Train Acc: 0.9697 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.1810 | Train Acc: 0.9697 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.1749 | Train Acc: 0.9697 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.1680 | Train Acc: 0.9697 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.1431 | Train Acc: 0.9697 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1334 | Train Acc: 0.9697 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1416 | Train Acc: 0.9621 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1605 | Train Acc: 0.9697 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1649 | Train Acc: 0.9697 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1577 | Train Acc: 0.9773 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.1669 | Train Acc: 0.9848 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.1689 | Train Acc: 0.9848 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.1655 | Train Acc: 0.9773 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.1565 | Train Acc: 0.9773 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.1355 | Train Acc: 0.9848 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.1597 | Train Acc: 0.9848 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1838 | Train Acc: 0.9697 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.1330 | Train Acc: 0.9773 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.1639 | Train Acc: 0.9773 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1550 | Train Acc: 0.9697 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.1825 | Train Acc: 0.9697 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1609 | Train Acc: 0.9773 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.1409 | Train Acc: 0.9773 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.1403 | Train Acc: 0.9773 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.1514 | Train Acc: 0.9697 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.1180 | Train Acc: 0.9621 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.1495 | Train Acc: 0.9773 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1644 | Train Acc: 0.9773 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.1617 | Train Acc: 0.9773 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.1578 | Train Acc: 0.9848 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1451 | Train Acc: 0.9848 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1340 | Train Acc: 0.9848 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1740 | Train Acc: 0.9848 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.1657 | Train Acc: 0.9773 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.1352 | Train Acc: 0.9773 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.1145 | Train Acc: 0.9773 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.1419 | Train Acc: 0.9773 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.1233 | Train Acc: 0.9773 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.1281 | Train Acc: 0.9773 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.1406 | Train Acc: 0.9848 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.1323 | Train Acc: 0.9848 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.1383 | Train Acc: 0.9848 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1345 | Train Acc: 0.9621 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1426 | Train Acc: 0.9848 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1152 | Train Acc: 0.9621 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1437 | Train Acc: 0.9697 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.1295 | Train Acc: 0.9621 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.1161 | Train Acc: 0.9621 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.1255 | Train Acc: 0.9848 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.1318 | Train Acc: 0.9848 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.1373 | Train Acc: 0.9848 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1208 | Train Acc: 0.9773 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.1300 | Train Acc: 0.9848 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.1106 | Train Acc: 0.9924 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1317 | Train Acc: 0.9773 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.1341 | Train Acc: 0.9621 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.1253 | Train Acc: 0.9621 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 0.1315 | Train Acc: 0.9773 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1236 | Train Acc: 0.9848 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1089 | Train Acc: 0.9848 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1184 | Train Acc: 0.9848 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.1132 | Train Acc: 0.9848 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1187 | Train Acc: 0.9848 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.1337 | Train Acc: 0.9848 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.1525 | Train Acc: 0.9848 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.0970 | Train Acc: 0.9773 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.1038 | Train Acc: 0.9773 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0966 | Train Acc: 0.9773 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1263 | Train Acc: 0.9924 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1327 | Train Acc: 0.9848 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.1432 | Train Acc: 0.9848 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.1038 | Train Acc: 0.9773 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.1542 | Train Acc: 0.9848 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.1059 | Train Acc: 0.9848 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1163 | Train Acc: 0.9848 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.1281 | Train Acc: 0.9848 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.1019 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 0.1165 | Train Acc: 0.9848 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1004 | Train Acc: 0.9848 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.1098 | Train Acc: 0.9848 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.0997 | Train Acc: 0.9848 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.1041 | Train Acc: 0.9848 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.0996 | Train Acc: 0.9848 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.0880 | Train Acc: 0.9924 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.1033 | Train Acc: 0.9924 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.1158 | Train Acc: 0.9848 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.0765 | Train Acc: 0.9848 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.0884 | Train Acc: 0.9848 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1011 | Train Acc: 0.9848 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.0954 | Train Acc: 0.9773 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1133 | Train Acc: 0.9773 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.0970 | Train Acc: 0.9773 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.0911 | Train Acc: 0.9848 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.1166 | Train Acc: 0.9924 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.0824 | Train Acc: 0.9848 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.0894 | Train Acc: 0.9848 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.1191 | Train Acc: 0.9848 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.0638 | Train Acc: 0.9848 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.0919 | Train Acc: 0.9924 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1020 | Train Acc: 0.9924 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1046 | Train Acc: 0.9924 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.0756 | Train Acc: 0.9848 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.0882 | Train Acc: 0.9848 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.0837 | Train Acc: 0.9773 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.1086 | Train Acc: 0.9848 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.1005 | Train Acc: 0.9924 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.0882 | Train Acc: 0.9848 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.0860 | Train Acc: 0.9773 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.0958 | Train Acc: 0.9924 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.0760 | Train Acc: 0.9924 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1069 | Train Acc: 0.9924 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0784 | Train Acc: 0.9848 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.0759 | Train Acc: 0.9848 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 0.0844 | Train Acc: 0.9848 | Test Acc: 0.1111\n",
      "\n",
      "--- Fold 5 ---\n",
      "Fold 5 | Iteration 500 | Loss: 2.3976 | Train Acc: 0.1944 | Test Acc: 0.0833\n",
      "Fold 5 | Iteration 500 | Loss: 2.3809 | Train Acc: 0.1944 | Test Acc: 0.1250\n",
      "Fold 5 | Iteration 500 | Loss: 2.3574 | Train Acc: 0.1944 | Test Acc: 0.1250\n",
      "Fold 5 | Iteration 500 | Loss: 2.3425 | Train Acc: 0.2014 | Test Acc: 0.1250\n",
      "Fold 5 | Iteration 500 | Loss: 2.3108 | Train Acc: 0.2292 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 2.2798 | Train Acc: 0.2847 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 2.2500 | Train Acc: 0.2847 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 2.2066 | Train Acc: 0.2778 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 2.1621 | Train Acc: 0.2847 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 2.1277 | Train Acc: 0.2917 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 2.0575 | Train Acc: 0.3056 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 2.0303 | Train Acc: 0.2917 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 2.0076 | Train Acc: 0.2847 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 1.9964 | Train Acc: 0.2778 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 1.9783 | Train Acc: 0.2778 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 1.9866 | Train Acc: 0.2778 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 1.9556 | Train Acc: 0.2847 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 1.9509 | Train Acc: 0.2986 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.9303 | Train Acc: 0.3125 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.9188 | Train Acc: 0.2986 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.9012 | Train Acc: 0.3056 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.8963 | Train Acc: 0.2847 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.8961 | Train Acc: 0.2847 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 1.8814 | Train Acc: 0.2986 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.8824 | Train Acc: 0.3056 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.8789 | Train Acc: 0.3125 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.8835 | Train Acc: 0.3194 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.8729 | Train Acc: 0.3056 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 1.8625 | Train Acc: 0.2917 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.8507 | Train Acc: 0.2778 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.8340 | Train Acc: 0.2847 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.8619 | Train Acc: 0.2847 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.8389 | Train Acc: 0.2847 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.8301 | Train Acc: 0.2986 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.8144 | Train Acc: 0.3056 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.8388 | Train Acc: 0.3056 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.8251 | Train Acc: 0.3056 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.8145 | Train Acc: 0.3056 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.7710 | Train Acc: 0.3125 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.8050 | Train Acc: 0.3264 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.7995 | Train Acc: 0.3264 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.8042 | Train Acc: 0.3264 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.7791 | Train Acc: 0.3194 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.8155 | Train Acc: 0.3194 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.8264 | Train Acc: 0.3264 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.7669 | Train Acc: 0.3125 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.7744 | Train Acc: 0.3194 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.7778 | Train Acc: 0.3125 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.7841 | Train Acc: 0.3194 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.7906 | Train Acc: 0.3194 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.7741 | Train Acc: 0.3264 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.7687 | Train Acc: 0.3403 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.7806 | Train Acc: 0.3472 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.7797 | Train Acc: 0.3472 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.7476 | Train Acc: 0.3403 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.7609 | Train Acc: 0.3472 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.7578 | Train Acc: 0.3403 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.7127 | Train Acc: 0.3403 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.7187 | Train Acc: 0.3403 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.7219 | Train Acc: 0.3403 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.7494 | Train Acc: 0.3472 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.7102 | Train Acc: 0.3472 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.7039 | Train Acc: 0.3472 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.7009 | Train Acc: 0.3472 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.6943 | Train Acc: 0.3403 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.6634 | Train Acc: 0.3403 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.7096 | Train Acc: 0.3472 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.6751 | Train Acc: 0.3542 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.6841 | Train Acc: 0.3472 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.6989 | Train Acc: 0.3542 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.6740 | Train Acc: 0.3611 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.6709 | Train Acc: 0.3681 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.6783 | Train Acc: 0.3750 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.6572 | Train Acc: 0.3681 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.6507 | Train Acc: 0.3681 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.6207 | Train Acc: 0.3750 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.6256 | Train Acc: 0.3681 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.6311 | Train Acc: 0.3681 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.6360 | Train Acc: 0.3542 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.5822 | Train Acc: 0.3542 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 1.6137 | Train Acc: 0.3542 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 1.5791 | Train Acc: 0.3750 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 1.5845 | Train Acc: 0.3819 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 1.5838 | Train Acc: 0.3889 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 1.5571 | Train Acc: 0.3611 | Test Acc: 0.3333\n",
      "Fold 5 | Iteration 500 | Loss: 1.5748 | Train Acc: 0.3681 | Test Acc: 0.3333\n",
      "Fold 5 | Iteration 500 | Loss: 1.5693 | Train Acc: 0.3681 | Test Acc: 0.3333\n",
      "Fold 5 | Iteration 500 | Loss: 1.5623 | Train Acc: 0.3750 | Test Acc: 0.3333\n",
      "Fold 5 | Iteration 500 | Loss: 1.5990 | Train Acc: 0.3750 | Test Acc: 0.3333\n",
      "Fold 5 | Iteration 500 | Loss: 1.5590 | Train Acc: 0.3681 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 1.5638 | Train Acc: 0.3681 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 1.5032 | Train Acc: 0.3889 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 1.5118 | Train Acc: 0.3819 | Test Acc: 0.3750\n",
      "Fold 5 | Iteration 500 | Loss: 1.5197 | Train Acc: 0.3819 | Test Acc: 0.3750\n",
      "Fold 5 | Iteration 500 | Loss: 1.5142 | Train Acc: 0.4167 | Test Acc: 0.3333\n",
      "Fold 5 | Iteration 500 | Loss: 1.5089 | Train Acc: 0.4167 | Test Acc: 0.3333\n",
      "Fold 5 | Iteration 500 | Loss: 1.4851 | Train Acc: 0.4028 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 1.4912 | Train Acc: 0.4375 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 1.4355 | Train Acc: 0.4514 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 1.4664 | Train Acc: 0.4583 | Test Acc: 0.3333\n",
      "Fold 5 | Iteration 500 | Loss: 1.4680 | Train Acc: 0.4236 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 1.4238 | Train Acc: 0.4306 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 1.4311 | Train Acc: 0.4306 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 1.3862 | Train Acc: 0.4444 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 1.4147 | Train Acc: 0.4792 | Test Acc: 0.3333\n",
      "Fold 5 | Iteration 500 | Loss: 1.4310 | Train Acc: 0.4583 | Test Acc: 0.3333\n",
      "Fold 5 | Iteration 500 | Loss: 1.4292 | Train Acc: 0.4653 | Test Acc: 0.3333\n",
      "Fold 5 | Iteration 500 | Loss: 1.3800 | Train Acc: 0.4931 | Test Acc: 0.3333\n",
      "Fold 5 | Iteration 500 | Loss: 1.3946 | Train Acc: 0.4931 | Test Acc: 0.3333\n",
      "Fold 5 | Iteration 500 | Loss: 1.3934 | Train Acc: 0.5069 | Test Acc: 0.3333\n",
      "Fold 5 | Iteration 500 | Loss: 1.3852 | Train Acc: 0.5000 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 1.3385 | Train Acc: 0.4792 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 1.3538 | Train Acc: 0.5000 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 1.3777 | Train Acc: 0.5139 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 1.3123 | Train Acc: 0.5208 | Test Acc: 0.3333\n",
      "Fold 5 | Iteration 500 | Loss: 1.3115 | Train Acc: 0.5278 | Test Acc: 0.3333\n",
      "Fold 5 | Iteration 500 | Loss: 1.3281 | Train Acc: 0.5000 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 1.2796 | Train Acc: 0.5000 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 1.2982 | Train Acc: 0.5069 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 1.2652 | Train Acc: 0.5069 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 1.2533 | Train Acc: 0.5278 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 1.2894 | Train Acc: 0.5278 | Test Acc: 0.3333\n",
      "Fold 5 | Iteration 500 | Loss: 1.2275 | Train Acc: 0.5278 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 1.2818 | Train Acc: 0.5625 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 1.2242 | Train Acc: 0.5764 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 1.2501 | Train Acc: 0.5625 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 1.2504 | Train Acc: 0.5625 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 1.1953 | Train Acc: 0.5556 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 1.1854 | Train Acc: 0.5556 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 1.2077 | Train Acc: 0.5556 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 1.1870 | Train Acc: 0.5833 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 1.1941 | Train Acc: 0.5833 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 1.1503 | Train Acc: 0.6042 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 1.2045 | Train Acc: 0.5903 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 1.1831 | Train Acc: 0.5972 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 1.1235 | Train Acc: 0.5625 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 1.1819 | Train Acc: 0.5833 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 1.1327 | Train Acc: 0.6181 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 1.0901 | Train Acc: 0.5903 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.1218 | Train Acc: 0.5972 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 1.1015 | Train Acc: 0.6042 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 1.1024 | Train Acc: 0.6042 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 1.0506 | Train Acc: 0.6042 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 1.0716 | Train Acc: 0.6250 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 1.0296 | Train Acc: 0.6181 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 1.0753 | Train Acc: 0.6250 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 1.0391 | Train Acc: 0.6389 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 1.0749 | Train Acc: 0.6389 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 1.0302 | Train Acc: 0.6458 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 1.0153 | Train Acc: 0.6528 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.0082 | Train Acc: 0.6736 | Test Acc: 0.1250\n",
      "Fold 5 | Iteration 500 | Loss: 0.9743 | Train Acc: 0.6667 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.9912 | Train Acc: 0.6597 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.9768 | Train Acc: 0.6528 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.9745 | Train Acc: 0.6736 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.9293 | Train Acc: 0.6736 | Test Acc: 0.1250\n",
      "Fold 5 | Iteration 500 | Loss: 0.9647 | Train Acc: 0.6875 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.9857 | Train Acc: 0.6875 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.9241 | Train Acc: 0.6875 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.9577 | Train Acc: 0.6875 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.9258 | Train Acc: 0.6875 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.9639 | Train Acc: 0.7014 | Test Acc: 0.1250\n",
      "Fold 5 | Iteration 500 | Loss: 0.9343 | Train Acc: 0.7083 | Test Acc: 0.0833\n",
      "Fold 5 | Iteration 500 | Loss: 0.9545 | Train Acc: 0.7153 | Test Acc: 0.0833\n",
      "Fold 5 | Iteration 500 | Loss: 0.9352 | Train Acc: 0.7014 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.9217 | Train Acc: 0.7014 | Test Acc: 0.3333\n",
      "Fold 5 | Iteration 500 | Loss: 0.9368 | Train Acc: 0.7361 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.8563 | Train Acc: 0.7431 | Test Acc: 0.1250\n",
      "Fold 5 | Iteration 500 | Loss: 0.8373 | Train Acc: 0.7222 | Test Acc: 0.0833\n",
      "Fold 5 | Iteration 500 | Loss: 0.8822 | Train Acc: 0.7292 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.9023 | Train Acc: 0.7431 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.8589 | Train Acc: 0.7361 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.8521 | Train Acc: 0.7292 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.8842 | Train Acc: 0.7569 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.8066 | Train Acc: 0.7708 | Test Acc: 0.0833\n",
      "Fold 5 | Iteration 500 | Loss: 0.8402 | Train Acc: 0.7569 | Test Acc: 0.0833\n",
      "Fold 5 | Iteration 500 | Loss: 0.8083 | Train Acc: 0.7431 | Test Acc: 0.1250\n",
      "Fold 5 | Iteration 500 | Loss: 0.8310 | Train Acc: 0.7431 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.8031 | Train Acc: 0.7431 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.8030 | Train Acc: 0.7708 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.7917 | Train Acc: 0.7708 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.8278 | Train Acc: 0.7847 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.7742 | Train Acc: 0.7778 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.8151 | Train Acc: 0.7778 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.7463 | Train Acc: 0.7708 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.7807 | Train Acc: 0.7708 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.7768 | Train Acc: 0.7569 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.7448 | Train Acc: 0.7708 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.7421 | Train Acc: 0.7917 | Test Acc: 0.1250\n",
      "Fold 5 | Iteration 500 | Loss: 0.7453 | Train Acc: 0.7778 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.6995 | Train Acc: 0.7639 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.7350 | Train Acc: 0.7569 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.6988 | Train Acc: 0.7917 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.6826 | Train Acc: 0.7986 | Test Acc: 0.1250\n",
      "Fold 5 | Iteration 500 | Loss: 0.7400 | Train Acc: 0.7847 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.7203 | Train Acc: 0.7708 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.7006 | Train Acc: 0.7917 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.6955 | Train Acc: 0.8194 | Test Acc: 0.1250\n",
      "Fold 5 | Iteration 500 | Loss: 0.7187 | Train Acc: 0.8056 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.6961 | Train Acc: 0.7847 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.6538 | Train Acc: 0.8056 | Test Acc: 0.1250\n",
      "Fold 5 | Iteration 500 | Loss: 0.6824 | Train Acc: 0.7986 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.6715 | Train Acc: 0.8056 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.6861 | Train Acc: 0.7708 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.7084 | Train Acc: 0.8333 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.6274 | Train Acc: 0.7986 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.6490 | Train Acc: 0.8056 | Test Acc: 0.1250\n",
      "Fold 5 | Iteration 500 | Loss: 0.6364 | Train Acc: 0.7917 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.6258 | Train Acc: 0.7986 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.6232 | Train Acc: 0.8264 | Test Acc: 0.1250\n",
      "Fold 5 | Iteration 500 | Loss: 0.6562 | Train Acc: 0.8194 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.6229 | Train Acc: 0.7986 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.6033 | Train Acc: 0.8194 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.6224 | Train Acc: 0.8333 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.6195 | Train Acc: 0.8264 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.5980 | Train Acc: 0.8125 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.6493 | Train Acc: 0.8194 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.5900 | Train Acc: 0.8333 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.6031 | Train Acc: 0.8264 | Test Acc: 0.1250\n",
      "Fold 5 | Iteration 500 | Loss: 0.5982 | Train Acc: 0.8403 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.5742 | Train Acc: 0.8542 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.6105 | Train Acc: 0.8403 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.5987 | Train Acc: 0.8472 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.5667 | Train Acc: 0.8403 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.5731 | Train Acc: 0.8333 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.5583 | Train Acc: 0.8403 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.5220 | Train Acc: 0.8403 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.6049 | Train Acc: 0.8472 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.5533 | Train Acc: 0.8472 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.5311 | Train Acc: 0.8472 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.5428 | Train Acc: 0.8611 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.5308 | Train Acc: 0.8681 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.5071 | Train Acc: 0.8750 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.5250 | Train Acc: 0.8681 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.5323 | Train Acc: 0.8750 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.4943 | Train Acc: 0.8750 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.4990 | Train Acc: 0.8750 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.5358 | Train Acc: 0.8611 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.5313 | Train Acc: 0.8333 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.5435 | Train Acc: 0.8542 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.5092 | Train Acc: 0.8472 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.5395 | Train Acc: 0.8542 | Test Acc: 0.1250\n",
      "Fold 5 | Iteration 500 | Loss: 0.5426 | Train Acc: 0.8611 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.5175 | Train Acc: 0.8750 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.4859 | Train Acc: 0.8542 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.4709 | Train Acc: 0.8611 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.4482 | Train Acc: 0.8819 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.4636 | Train Acc: 0.8750 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.5172 | Train Acc: 0.8681 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.4783 | Train Acc: 0.8750 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.4283 | Train Acc: 0.8681 | Test Acc: 0.0833\n",
      "Fold 5 | Iteration 500 | Loss: 0.4841 | Train Acc: 0.8611 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.4530 | Train Acc: 0.8681 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.4767 | Train Acc: 0.8750 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.4356 | Train Acc: 0.8750 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.4703 | Train Acc: 0.8889 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.4385 | Train Acc: 0.8750 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.4315 | Train Acc: 0.8819 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.4874 | Train Acc: 0.8750 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.3990 | Train Acc: 0.8750 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.3973 | Train Acc: 0.8819 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.4422 | Train Acc: 0.8819 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.4297 | Train Acc: 0.8750 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.4227 | Train Acc: 0.8750 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.4311 | Train Acc: 0.8889 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.4180 | Train Acc: 0.8819 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.4385 | Train Acc: 0.8889 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.3853 | Train Acc: 0.8889 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.3788 | Train Acc: 0.8819 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.4150 | Train Acc: 0.8819 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.3975 | Train Acc: 0.8750 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.3809 | Train Acc: 0.8681 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.4135 | Train Acc: 0.8889 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.3743 | Train Acc: 0.8819 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.3624 | Train Acc: 0.8819 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.3804 | Train Acc: 0.8958 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.3577 | Train Acc: 0.8819 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.3533 | Train Acc: 0.8819 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.4048 | Train Acc: 0.8958 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.3636 | Train Acc: 0.9028 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.3573 | Train Acc: 0.9028 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.3562 | Train Acc: 0.9097 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.3737 | Train Acc: 0.8889 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.3555 | Train Acc: 0.8819 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.3536 | Train Acc: 0.9028 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.3486 | Train Acc: 0.9028 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.3290 | Train Acc: 0.8958 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.3490 | Train Acc: 0.9028 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.3779 | Train Acc: 0.9028 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.3577 | Train Acc: 0.9028 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.3359 | Train Acc: 0.8889 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.3629 | Train Acc: 0.9028 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.2989 | Train Acc: 0.9028 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.3193 | Train Acc: 0.8958 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.2914 | Train Acc: 0.8958 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.3064 | Train Acc: 0.8958 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.3634 | Train Acc: 0.9028 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.3078 | Train Acc: 0.9167 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.2985 | Train Acc: 0.9028 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.3186 | Train Acc: 0.9236 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.3547 | Train Acc: 0.8958 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.3304 | Train Acc: 0.9028 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.3384 | Train Acc: 0.9028 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.3388 | Train Acc: 0.9306 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.2601 | Train Acc: 0.9167 | Test Acc: 0.3333\n",
      "Fold 5 | Iteration 500 | Loss: 0.2760 | Train Acc: 0.9097 | Test Acc: 0.3333\n",
      "Fold 5 | Iteration 500 | Loss: 0.3182 | Train Acc: 0.8958 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.3133 | Train Acc: 0.9097 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.3143 | Train Acc: 0.9236 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.2854 | Train Acc: 0.9167 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.2918 | Train Acc: 0.9167 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.3301 | Train Acc: 0.9167 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.3115 | Train Acc: 0.9097 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.2772 | Train Acc: 0.9236 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.3041 | Train Acc: 0.9167 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.2740 | Train Acc: 0.9306 | Test Acc: 0.3333\n",
      "Fold 5 | Iteration 500 | Loss: 0.2908 | Train Acc: 0.9306 | Test Acc: 0.3333\n",
      "Fold 5 | Iteration 500 | Loss: 0.2924 | Train Acc: 0.9236 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.2493 | Train Acc: 0.9236 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.2489 | Train Acc: 0.9236 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.2536 | Train Acc: 0.9236 | Test Acc: 0.3333\n",
      "Fold 5 | Iteration 500 | Loss: 0.2870 | Train Acc: 0.9167 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.3102 | Train Acc: 0.9097 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.2540 | Train Acc: 0.9236 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.2408 | Train Acc: 0.9236 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.2721 | Train Acc: 0.9236 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.2757 | Train Acc: 0.9236 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.2788 | Train Acc: 0.9306 | Test Acc: 0.3333\n",
      "Fold 5 | Iteration 500 | Loss: 0.2942 | Train Acc: 0.9306 | Test Acc: 0.3333\n",
      "Fold 5 | Iteration 500 | Loss: 0.3011 | Train Acc: 0.9444 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.2700 | Train Acc: 0.9097 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.2875 | Train Acc: 0.9236 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.3258 | Train Acc: 0.9375 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.3010 | Train Acc: 0.9514 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.2630 | Train Acc: 0.9375 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.2432 | Train Acc: 0.9167 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.3780 | Train Acc: 0.9306 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.2858 | Train Acc: 0.9167 | Test Acc: 0.3750\n",
      "Fold 5 | Iteration 500 | Loss: 0.2627 | Train Acc: 0.9444 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.2516 | Train Acc: 0.9097 | Test Acc: 0.0833\n",
      "Fold 5 | Iteration 500 | Loss: 0.2853 | Train Acc: 0.9028 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.2770 | Train Acc: 0.9306 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.2606 | Train Acc: 0.9375 | Test Acc: 0.1250\n",
      "Fold 5 | Iteration 500 | Loss: 0.2295 | Train Acc: 0.9444 | Test Acc: 0.1250\n",
      "Fold 5 | Iteration 500 | Loss: 0.2972 | Train Acc: 0.9306 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.2558 | Train Acc: 0.9306 | Test Acc: 0.3750\n",
      "Fold 5 | Iteration 500 | Loss: 0.2905 | Train Acc: 0.9236 | Test Acc: 0.3750\n",
      "Fold 5 | Iteration 500 | Loss: 0.2699 | Train Acc: 0.9236 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.2509 | Train Acc: 0.9375 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.2578 | Train Acc: 0.9583 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.2446 | Train Acc: 0.9306 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.2443 | Train Acc: 0.9167 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.2840 | Train Acc: 0.9514 | Test Acc: 0.1250\n",
      "Fold 5 | Iteration 500 | Loss: 0.2373 | Train Acc: 0.9375 | Test Acc: 0.1250\n",
      "Fold 5 | Iteration 500 | Loss: 0.2473 | Train Acc: 0.9444 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.2257 | Train Acc: 0.9375 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.2572 | Train Acc: 0.9444 | Test Acc: 0.3333\n",
      "Fold 5 | Iteration 500 | Loss: 0.2299 | Train Acc: 0.9514 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1936 | Train Acc: 0.9514 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.2111 | Train Acc: 0.9375 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.2151 | Train Acc: 0.9375 | Test Acc: 0.3333\n",
      "Fold 5 | Iteration 500 | Loss: 0.2463 | Train Acc: 0.9444 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.2486 | Train Acc: 0.9444 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.2245 | Train Acc: 0.9583 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1911 | Train Acc: 0.9722 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.2305 | Train Acc: 0.9514 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.2315 | Train Acc: 0.9583 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1880 | Train Acc: 0.9514 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.2094 | Train Acc: 0.9514 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1980 | Train Acc: 0.9444 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.2016 | Train Acc: 0.9375 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.2334 | Train Acc: 0.9514 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.2372 | Train Acc: 0.9514 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.2040 | Train Acc: 0.9444 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.2177 | Train Acc: 0.9444 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.1725 | Train Acc: 0.9514 | Test Acc: 0.3333\n",
      "Fold 5 | Iteration 500 | Loss: 0.2045 | Train Acc: 0.9514 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.1987 | Train Acc: 0.9583 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1817 | Train Acc: 0.9653 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.2020 | Train Acc: 0.9722 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.2092 | Train Acc: 0.9583 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.2043 | Train Acc: 0.9583 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1696 | Train Acc: 0.9583 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.2040 | Train Acc: 0.9444 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.2001 | Train Acc: 0.9583 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1757 | Train Acc: 0.9583 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.2021 | Train Acc: 0.9583 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.2024 | Train Acc: 0.9722 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1924 | Train Acc: 0.9583 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1947 | Train Acc: 0.9583 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1722 | Train Acc: 0.9583 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1764 | Train Acc: 0.9653 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1976 | Train Acc: 0.9653 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.1657 | Train Acc: 0.9583 | Test Acc: 0.1250\n",
      "Fold 5 | Iteration 500 | Loss: 0.1546 | Train Acc: 0.9722 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.1829 | Train Acc: 0.9722 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1648 | Train Acc: 0.9653 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1787 | Train Acc: 0.9722 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1269 | Train Acc: 0.9722 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.1673 | Train Acc: 0.9722 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1347 | Train Acc: 0.9653 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1591 | Train Acc: 0.9653 | Test Acc: 0.1250\n",
      "Fold 5 | Iteration 500 | Loss: 0.1643 | Train Acc: 0.9653 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.1660 | Train Acc: 0.9722 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1441 | Train Acc: 0.9722 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1446 | Train Acc: 0.9653 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1717 | Train Acc: 0.9653 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.1467 | Train Acc: 0.9722 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1706 | Train Acc: 0.9722 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1380 | Train Acc: 0.9653 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1331 | Train Acc: 0.9722 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1710 | Train Acc: 0.9722 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1671 | Train Acc: 0.9722 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1574 | Train Acc: 0.9653 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1428 | Train Acc: 0.9722 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1287 | Train Acc: 0.9722 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1961 | Train Acc: 0.9722 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1644 | Train Acc: 0.9722 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1475 | Train Acc: 0.9722 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.1422 | Train Acc: 0.9792 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.1560 | Train Acc: 0.9792 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1589 | Train Acc: 0.9722 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1143 | Train Acc: 0.9722 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1369 | Train Acc: 0.9722 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1663 | Train Acc: 0.9722 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1486 | Train Acc: 0.9583 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1414 | Train Acc: 0.9653 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1430 | Train Acc: 0.9722 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.1380 | Train Acc: 0.9653 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.1779 | Train Acc: 0.9722 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1579 | Train Acc: 0.9722 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1440 | Train Acc: 0.9722 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.1285 | Train Acc: 0.9792 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1192 | Train Acc: 0.9792 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1275 | Train Acc: 0.9792 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1044 | Train Acc: 0.9792 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1450 | Train Acc: 0.9722 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1492 | Train Acc: 0.9722 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1425 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1217 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1436 | Train Acc: 0.9722 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1242 | Train Acc: 0.9722 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.1564 | Train Acc: 0.9722 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.1667 | Train Acc: 0.9792 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1342 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1193 | Train Acc: 0.9792 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.1556 | Train Acc: 0.9583 | Test Acc: 0.1250\n",
      "Fold 5 | Iteration 500 | Loss: 0.1237 | Train Acc: 0.9653 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.1340 | Train Acc: 0.9792 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1645 | Train Acc: 0.9722 | Test Acc: 0.1250\n",
      "Fold 5 | Iteration 500 | Loss: 0.1411 | Train Acc: 0.9722 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1298 | Train Acc: 0.9792 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1479 | Train Acc: 0.9583 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1685 | Train Acc: 0.9722 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1264 | Train Acc: 0.9722 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1130 | Train Acc: 0.9792 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.1652 | Train Acc: 0.9792 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1208 | Train Acc: 0.9722 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.0943 | Train Acc: 0.9792 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1419 | Train Acc: 0.9792 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1068 | Train Acc: 0.9792 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1117 | Train Acc: 0.9792 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1032 | Train Acc: 0.9792 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1334 | Train Acc: 0.9722 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1082 | Train Acc: 0.9722 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1517 | Train Acc: 0.9792 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1000 | Train Acc: 0.9792 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1170 | Train Acc: 0.9792 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.0952 | Train Acc: 0.9792 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.0980 | Train Acc: 0.9792 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1234 | Train Acc: 0.9792 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1234 | Train Acc: 0.9792 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1307 | Train Acc: 0.9792 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1058 | Train Acc: 0.9792 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.0988 | Train Acc: 0.9792 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1199 | Train Acc: 0.9792 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.0962 | Train Acc: 0.9792 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.1272 | Train Acc: 0.9722 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.1168 | Train Acc: 0.9722 | Test Acc: 0.1250\n",
      "Fold 5 | Iteration 500 | Loss: 0.1144 | Train Acc: 0.9792 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.1032 | Train Acc: 0.9792 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.1023 | Train Acc: 0.9792 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1204 | Train Acc: 0.9792 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1031 | Train Acc: 0.9722 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1237 | Train Acc: 0.9792 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1291 | Train Acc: 0.9792 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.0975 | Train Acc: 0.9792 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1173 | Train Acc: 0.9722 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.1252 | Train Acc: 0.9722 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1218 | Train Acc: 0.9792 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.0946 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0945 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1092 | Train Acc: 0.9722 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1200 | Train Acc: 0.9722 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1180 | Train Acc: 0.9722 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 0.0901 | Train Acc: 0.9792 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1294 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0893 | Train Acc: 0.9722 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1155 | Train Acc: 0.9792 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1152 | Train Acc: 0.9792 | Test Acc: 0.1667\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… ANN Average Test Accuracy: 0.1667\n",
      "\n",
      "âœ… ANN Average Training Accuracy: 0.7500\n",
      "\n",
      "âœ… ANN Average Generalization Error: 0.5300\n"
     ]
    }
   ],
   "source": [
    "class ANNClassifier(torch.nn.Module):\n",
    "    def __init__(self,input_dim, hidden_units, output_dim):\n",
    "        super(ANNClassifier,self).__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim,hidden_units),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_units, hidden_units),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_units, hidden_units),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_units, hidden_units),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.3),\n",
    "            torch.nn.Linear(hidden_units,output_dim)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = x.shape[1]\n",
    "hidden_units = 132\n",
    "output_dim = 11\n",
    "iteration = 500\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Group K-Fold\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "# Lists to store metrics per fold\n",
    "all_train_losses = []\n",
    "all_train_accuracies = []\n",
    "all_test_accuracies = []\n",
    "all_generalization_errors = []\n",
    "ann_accuracy = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(gkf.split(x, y, groups=groups)):\n",
    "    print(f\"\\n--- Fold {fold+1} ---\")\n",
    "\n",
    "    model = ANNClassifier(input_dim, hidden_units, output_dim)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-10)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    X_train, X_test = x[train_idx], x[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    fold_train_losses = []\n",
    "    fold_train_accuracies = []\n",
    "    fold_test_accuracies = []\n",
    "    fold_generalization_errors = []\n",
    "\n",
    "    for iterations in range(iteration):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            train_preds = model(X_train)\n",
    "            train_labels = torch.argmax(train_preds, dim=1)\n",
    "            train_acc = accuracy_score(y_train.numpy(), train_labels.numpy())\n",
    "\n",
    "            test_preds = model(X_test)\n",
    "            test_labels = torch.argmax(test_preds, dim=1)\n",
    "            test_acc = accuracy_score(y_test.numpy(), test_labels.numpy())\n",
    "\n",
    "        fold_train_losses.append(loss.item())\n",
    "        fold_train_accuracies.append(train_acc)\n",
    "        fold_test_accuracies.append(test_acc)\n",
    "        fold_generalization_errors.append(abs(train_acc - test_acc))\n",
    "\n",
    "        if iteration % 100 == 0:\n",
    "            print(f\"Fold {fold+1} | Iteration {iteration} | Loss: {loss.item():.4f} | Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "    all_train_losses.append(fold_train_losses)\n",
    "    all_train_accuracies.append(fold_train_accuracies)\n",
    "    all_test_accuracies.append(fold_test_accuracies)\n",
    "    all_generalization_errors.append(fold_generalization_errors)\n",
    "    ann_accuracy.append(fold_test_accuracies[-1])\n",
    "\n",
    "# Average metrics over folds\n",
    "train_losses_avg = np.mean(all_train_losses, axis=0)\n",
    "train_accuracies_avg = np.mean(all_train_accuracies, axis=0)\n",
    "test_accuracies_avg = np.mean(all_test_accuracies, axis=0)\n",
    "generalization_errors_avg = np.mean(all_generalization_errors, axis=0)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(train_losses_avg, label='Avg Train Loss')\n",
    "plt.plot(train_accuracies_avg, label='Avg Train Accuracy')\n",
    "plt.plot(test_accuracies_avg, label='Avg Test Accuracy')\n",
    "plt.plot(generalization_errors_avg, label='Avg Generalization Error')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.title('Avg Test-, Avg Train ACC, Avg Train Loss and Avg Generalization Error For the ANN')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ… ANN Average Test Accuracy: {np.mean(ann_accuracy):.4f}\")\n",
    "print(f\"\\nâœ… ANN Average Training Accuracy: {np.mean(fold_train_accuracies):.4f}\")\n",
    "print(f\"\\nâœ… ANN Average Generalization Error: {np.mean(fold_generalization_errors):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8a5dba",
   "metadata": {},
   "source": [
    "### Model 2 Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659ae21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| Fold 1|\n",
      "Train Acc: 0.3333 | Test Acc: 0.1111 | Gen Error: 0.2222\n",
      "\n",
      "| Fold 2|\n",
      "Train Acc: 0.3258 | Test Acc: 0.2222 | Gen Error: 0.1035\n",
      "\n",
      "| Fold 3|\n",
      "Train Acc: 0.3258 | Test Acc: 0.1389 | Gen Error: 0.1869\n",
      "\n",
      "| Fold 4|\n",
      "Train Acc: 0.3182 | Test Acc: 0.1944 | Gen Error: 0.1237\n",
      "\n",
      "| Fold 5|\n",
      "Train Acc: 0.3125 | Test Acc: 0.2917 | Gen Error: 0.0208\n",
      "\n",
      " Logistic Regression Summary (GroupKFold):\n",
      "Average Train Accuracy: 0.3231\n",
      "Average Test Accuracy: 0.1917\n",
      "Average Gen Error: 0.1314\n"
     ]
    }
   ],
   "source": [
    "# GroupKFold setup\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "# Store metrics\n",
    "lr_train_accuracies = []\n",
    "lr_test_accuracies = []\n",
    "lr_gen_errors = []\n",
    "\n",
    "# Cross-validation loop\n",
    "for fold, (train_idx, test_idx) in enumerate(gkf.split(x, y, groups=groups)):\n",
    "    print(f\"\\n| Fold {fold+1}|\")\n",
    "\n",
    "    X_train, X_test = x[train_idx], x[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "\n",
    "    # Updated Logistic Regression model (no deprecated 'multi_class')\n",
    "    model = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict and evaluate\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "    gen_error = train_acc - test_acc\n",
    "\n",
    "    # Store metrics\n",
    "    lr_train_accuracies.append(train_acc)\n",
    "    lr_test_accuracies.append(test_acc)\n",
    "    lr_gen_errors.append(gen_error)\n",
    "\n",
    "    print(f\"Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f} | Gen Error: {gen_error:.4f}\")\n",
    "\n",
    "# Report averages\n",
    "avg_train_acc = np.mean(lr_train_accuracies)\n",
    "avg_test_acc = np.mean(lr_test_accuracies)\n",
    "avg_gen_error = np.mean(lr_gen_errors)\n",
    "\n",
    "print(\"\\n Logistic Regression Summary (GroupKFold):\")\n",
    "print(f\"Average Train Accuracy: {avg_train_acc:.4f}\")\n",
    "print(f\"Average Test Accuracy: {avg_test_acc:.4f}\")\n",
    "print(f\"Average Gen Error: {avg_gen_error:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee6ffc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dtu02450",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
