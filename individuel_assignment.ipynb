{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21d66b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import svd\n",
    "from dtuimldmtools import *\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import torch\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from dtuimldmtools import rlr_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1df2ad1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>HR_Mean</th>\n",
       "      <th>HR_Median</th>\n",
       "      <th>HR_std</th>\n",
       "      <th>HR_Min</th>\n",
       "      <th>HR_Max</th>\n",
       "      <th>HR_AUC</th>\n",
       "      <th>Round</th>\n",
       "      <th>Phase</th>\n",
       "      <th>Individual</th>\n",
       "      <th>Puzzler</th>\n",
       "      <th>Frustrated</th>\n",
       "      <th>Cohort</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>77.965186</td>\n",
       "      <td>78.000</td>\n",
       "      <td>3.345290</td>\n",
       "      <td>73.23</td>\n",
       "      <td>83.37</td>\n",
       "      <td>22924.945</td>\n",
       "      <td>round_3</td>\n",
       "      <td>phase3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>D1_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>70.981097</td>\n",
       "      <td>70.570</td>\n",
       "      <td>2.517879</td>\n",
       "      <td>67.12</td>\n",
       "      <td>78.22</td>\n",
       "      <td>21930.400</td>\n",
       "      <td>round_3</td>\n",
       "      <td>phase2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>D1_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>73.371959</td>\n",
       "      <td>73.360</td>\n",
       "      <td>3.259569</td>\n",
       "      <td>67.88</td>\n",
       "      <td>80.22</td>\n",
       "      <td>21647.085</td>\n",
       "      <td>round_3</td>\n",
       "      <td>phase1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>D1_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>78.916822</td>\n",
       "      <td>77.880</td>\n",
       "      <td>4.054595</td>\n",
       "      <td>72.32</td>\n",
       "      <td>84.92</td>\n",
       "      <td>25258.905</td>\n",
       "      <td>round_2</td>\n",
       "      <td>phase3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>D1_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>77.322226</td>\n",
       "      <td>74.550</td>\n",
       "      <td>6.047603</td>\n",
       "      <td>70.52</td>\n",
       "      <td>90.15</td>\n",
       "      <td>23890.565</td>\n",
       "      <td>round_2</td>\n",
       "      <td>phase2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>D1_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>163</td>\n",
       "      <td>73.594539</td>\n",
       "      <td>72.380</td>\n",
       "      <td>9.474556</td>\n",
       "      <td>57.43</td>\n",
       "      <td>93.53</td>\n",
       "      <td>21482.985</td>\n",
       "      <td>round_4</td>\n",
       "      <td>phase2</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>D1_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>164</td>\n",
       "      <td>57.839897</td>\n",
       "      <td>54.130</td>\n",
       "      <td>6.796647</td>\n",
       "      <td>52.97</td>\n",
       "      <td>74.14</td>\n",
       "      <td>16825.740</td>\n",
       "      <td>round_4</td>\n",
       "      <td>phase1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>D1_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>165</td>\n",
       "      <td>64.237295</td>\n",
       "      <td>65.195</td>\n",
       "      <td>3.589241</td>\n",
       "      <td>58.97</td>\n",
       "      <td>72.63</td>\n",
       "      <td>18691.065</td>\n",
       "      <td>round_1</td>\n",
       "      <td>phase3</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>D1_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>166</td>\n",
       "      <td>70.834320</td>\n",
       "      <td>70.440</td>\n",
       "      <td>2.391160</td>\n",
       "      <td>66.65</td>\n",
       "      <td>76.07</td>\n",
       "      <td>20753.005</td>\n",
       "      <td>round_1</td>\n",
       "      <td>phase2</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>D1_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>167</td>\n",
       "      <td>71.133878</td>\n",
       "      <td>69.225</td>\n",
       "      <td>14.069337</td>\n",
       "      <td>57.17</td>\n",
       "      <td>114.33</td>\n",
       "      <td>20820.320</td>\n",
       "      <td>round_1</td>\n",
       "      <td>phase1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>D1_2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>168 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0    HR_Mean  HR_Median     HR_std  HR_Min  HR_Max     HR_AUC  \\\n",
       "0             0  77.965186     78.000   3.345290   73.23   83.37  22924.945   \n",
       "1             1  70.981097     70.570   2.517879   67.12   78.22  21930.400   \n",
       "2             2  73.371959     73.360   3.259569   67.88   80.22  21647.085   \n",
       "3             3  78.916822     77.880   4.054595   72.32   84.92  25258.905   \n",
       "4             4  77.322226     74.550   6.047603   70.52   90.15  23890.565   \n",
       "..          ...        ...        ...        ...     ...     ...        ...   \n",
       "163         163  73.594539     72.380   9.474556   57.43   93.53  21482.985   \n",
       "164         164  57.839897     54.130   6.796647   52.97   74.14  16825.740   \n",
       "165         165  64.237295     65.195   3.589241   58.97   72.63  18691.065   \n",
       "166         166  70.834320     70.440   2.391160   66.65   76.07  20753.005   \n",
       "167         167  71.133878     69.225  14.069337   57.17  114.33  20820.320   \n",
       "\n",
       "       Round   Phase  Individual  Puzzler  Frustrated Cohort  \n",
       "0    round_3  phase3           1        1           1   D1_1  \n",
       "1    round_3  phase2           1        1           5   D1_1  \n",
       "2    round_3  phase1           1        1           0   D1_1  \n",
       "3    round_2  phase3           1        1           1   D1_1  \n",
       "4    round_2  phase2           1        1           5   D1_1  \n",
       "..       ...     ...         ...      ...         ...    ...  \n",
       "163  round_4  phase2          14        0           8   D1_2  \n",
       "164  round_4  phase1          14        0           0   D1_2  \n",
       "165  round_1  phase3          14        0           1   D1_2  \n",
       "166  round_1  phase2          14        0           4   D1_2  \n",
       "167  round_1  phase1          14        0           0   D1_2  \n",
       "\n",
       "[168 rows x 13 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\malth\\OneDrive - Danmarks Tekniske Universitet\\Porgrammering\\02445---Individuel-assignment-\\HR_data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27eed65c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HR_Mean</th>\n",
       "      <th>HR_Median</th>\n",
       "      <th>HR_std</th>\n",
       "      <th>HR_Min</th>\n",
       "      <th>HR_Max</th>\n",
       "      <th>HR_AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.260749</td>\n",
       "      <td>0.392342</td>\n",
       "      <td>-0.631771</td>\n",
       "      <td>0.824668</td>\n",
       "      <td>-0.445924</td>\n",
       "      <td>-0.131939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.670301</td>\n",
       "      <td>-0.524777</td>\n",
       "      <td>-0.825667</td>\n",
       "      <td>-0.042982</td>\n",
       "      <td>-0.849385</td>\n",
       "      <td>-0.409001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.351575</td>\n",
       "      <td>-0.180394</td>\n",
       "      <td>-0.651859</td>\n",
       "      <td>0.064942</td>\n",
       "      <td>-0.692701</td>\n",
       "      <td>-0.487928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.387612</td>\n",
       "      <td>0.377530</td>\n",
       "      <td>-0.465551</td>\n",
       "      <td>0.695444</td>\n",
       "      <td>-0.324494</td>\n",
       "      <td>0.518262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.175036</td>\n",
       "      <td>-0.033507</td>\n",
       "      <td>0.001493</td>\n",
       "      <td>0.439835</td>\n",
       "      <td>0.085234</td>\n",
       "      <td>0.137066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>-0.321902</td>\n",
       "      <td>-0.301360</td>\n",
       "      <td>0.804569</td>\n",
       "      <td>-1.419009</td>\n",
       "      <td>0.350030</td>\n",
       "      <td>-0.533643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>-2.422157</td>\n",
       "      <td>-2.554041</td>\n",
       "      <td>0.177024</td>\n",
       "      <td>-2.052351</td>\n",
       "      <td>-1.169020</td>\n",
       "      <td>-1.831071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>-1.569319</td>\n",
       "      <td>-1.188238</td>\n",
       "      <td>-0.574603</td>\n",
       "      <td>-1.200322</td>\n",
       "      <td>-1.287316</td>\n",
       "      <td>-1.311424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>-0.689868</td>\n",
       "      <td>-0.540823</td>\n",
       "      <td>-0.855363</td>\n",
       "      <td>-0.109724</td>\n",
       "      <td>-1.017820</td>\n",
       "      <td>-0.737003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>-0.649934</td>\n",
       "      <td>-0.690796</td>\n",
       "      <td>1.881316</td>\n",
       "      <td>-1.455931</td>\n",
       "      <td>1.979542</td>\n",
       "      <td>-0.718250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>168 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      HR_Mean  HR_Median    HR_std    HR_Min    HR_Max    HR_AUC\n",
       "0    0.260749   0.392342 -0.631771  0.824668 -0.445924 -0.131939\n",
       "1   -0.670301  -0.524777 -0.825667 -0.042982 -0.849385 -0.409001\n",
       "2   -0.351575  -0.180394 -0.651859  0.064942 -0.692701 -0.487928\n",
       "3    0.387612   0.377530 -0.465551  0.695444 -0.324494  0.518262\n",
       "4    0.175036  -0.033507  0.001493  0.439835  0.085234  0.137066\n",
       "..        ...        ...       ...       ...       ...       ...\n",
       "163 -0.321902  -0.301360  0.804569 -1.419009  0.350030 -0.533643\n",
       "164 -2.422157  -2.554041  0.177024 -2.052351 -1.169020 -1.831071\n",
       "165 -1.569319  -1.188238 -0.574603 -1.200322 -1.287316 -1.311424\n",
       "166 -0.689868  -0.540823 -0.855363 -0.109724 -1.017820 -0.737003\n",
       "167 -0.649934  -0.690796  1.881316 -1.455931  1.979542 -0.718250\n",
       "\n",
       "[168 rows x 6 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = df[['HR_Mean','HR_Median','HR_std','HR_Min','HR_Max','HR_AUC']]\n",
    "x_standardized = (features - features.mean()) / features.std()\n",
    "x = torch.tensor(x_standardized.values, dtype=torch.float32)\n",
    "y = torch.tensor(df['Frustrated'].values, dtype=torch.long)\n",
    "groups = df['Individual']\n",
    "x_standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27bf1493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ANNClassifier(torch.nn.Module):\n",
    "#     def __init__(self,input_dim, hidden_units, output_dim):\n",
    "#         super(ANNClassifier,self).__init__()\n",
    "#         self.net = torch.nn.Sequential(\n",
    "#             torch.nn.Linear(input_dim,hidden_units),\n",
    "#             torch.nn.ReLU(),\n",
    "#             torch.nn.Linear(hidden_units, hidden_units),\n",
    "#             torch.nn.ReLU(),\n",
    "#             torch.nn.Linear(hidden_units, hidden_units),\n",
    "#             torch.nn.ReLU(),\n",
    "#             torch.nn.Linear(hidden_units, hidden_units),\n",
    "#             torch.nn.ReLU(),\n",
    "#             torch.nn.Dropout(0.3),\n",
    "#             torch.nn.Linear(hidden_units,output_dim)\n",
    "#         )\n",
    "#     def forward(self,x):\n",
    "#         return self.net(x)\n",
    "    \n",
    "# #Hyperparameters:\n",
    "# input_dim = x.shape[1]\n",
    "# hidden_units = 132\n",
    "# output_dim = 10\n",
    "\n",
    "# epochs = 5000\n",
    "# learning_rate = 0.001\n",
    "\n",
    "# # Group K-Fold:\n",
    "# gkf = GroupKFold(n_splits=5)\n",
    "# ann_accuracy = []\n",
    "\n",
    "# for train_idx, test_idx in gkf.split(x,y, groups = groups):\n",
    "#     model = ANNClassifier(input_dim,hidden_units,output_dim)\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "#     criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "#     X_train, X_test = x[train_idx], x[test_idx]\n",
    "#     y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "#     for epoch in range(epochs):\n",
    "#         model.train()\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(X_train)\n",
    "#         loss = criterion(outputs, y_train)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         if epoch % 10 == 0:\n",
    "#             print(f\"Epoch {epoch}: loss = {loss.item():.4f}\")\n",
    "\n",
    "#     #Evaluation of the model:\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         y_pred = model(X_test)\n",
    "#         y_pred_labels = torch.argmax(y_pred,dim = 1)\n",
    "#         acc = accuracy_score(y_test.numpy(), y_pred_labels.numpy())\n",
    "#         ann_accuracy.append(acc)\n",
    "\n",
    "# print(f\"ANN Average Accuracy: {np.mean(ann_accuracy):3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5f361d",
   "metadata": {},
   "source": [
    "ChatGPT was used for some of this code, it was used for helping to make the cross-validation and the plots, where it also was used to help with the debugging. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "189f70c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fold 1 ---\n",
      "Fold 1 | Iteration 500 | Loss: 2.4044 | Train Acc: 0.0455 | Test Acc: 0.0833\n",
      "Fold 1 | Iteration 500 | Loss: 2.3870 | Train Acc: 0.2727 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 2.3705 | Train Acc: 0.3030 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 2.3533 | Train Acc: 0.3030 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 2.3344 | Train Acc: 0.3030 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 2.3129 | Train Acc: 0.3030 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 2.2878 | Train Acc: 0.3030 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 2.2585 | Train Acc: 0.3030 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 2.2237 | Train Acc: 0.3030 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 2.1834 | Train Acc: 0.3030 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 2.1372 | Train Acc: 0.3030 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 2.0866 | Train Acc: 0.3030 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 2.0345 | Train Acc: 0.3030 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.9869 | Train Acc: 0.3030 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.9512 | Train Acc: 0.3030 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.9310 | Train Acc: 0.3030 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.9184 | Train Acc: 0.3030 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.9021 | Train Acc: 0.3030 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.8786 | Train Acc: 0.3030 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.8516 | Train Acc: 0.3030 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.8259 | Train Acc: 0.3030 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.8052 | Train Acc: 0.3030 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.7913 | Train Acc: 0.3106 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.7837 | Train Acc: 0.2955 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 1.7804 | Train Acc: 0.2727 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 1.7781 | Train Acc: 0.2803 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 1.7728 | Train Acc: 0.2879 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 1.7638 | Train Acc: 0.2803 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 1.7526 | Train Acc: 0.3106 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.7417 | Train Acc: 0.3106 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.7333 | Train Acc: 0.3030 | Test Acc: 0.1389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\malth\\Miniconda3\\envs\\dtu02450\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 | Iteration 500 | Loss: 1.7272 | Train Acc: 0.3030 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.7223 | Train Acc: 0.3030 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.7174 | Train Acc: 0.3030 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.7118 | Train Acc: 0.3030 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.7062 | Train Acc: 0.3409 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.7006 | Train Acc: 0.3561 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.6946 | Train Acc: 0.3258 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.6879 | Train Acc: 0.3258 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.6809 | Train Acc: 0.3333 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.6739 | Train Acc: 0.3409 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.6671 | Train Acc: 0.3561 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.6606 | Train Acc: 0.3561 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.6546 | Train Acc: 0.3788 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.6483 | Train Acc: 0.3788 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.6411 | Train Acc: 0.3712 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.6331 | Train Acc: 0.3561 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.6253 | Train Acc: 0.3636 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.6183 | Train Acc: 0.3636 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.6118 | Train Acc: 0.3712 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.6053 | Train Acc: 0.3712 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.5984 | Train Acc: 0.3712 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.5910 | Train Acc: 0.3712 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.5837 | Train Acc: 0.3712 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 1.5767 | Train Acc: 0.3864 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.5697 | Train Acc: 0.3864 | Test Acc: 0.0833\n",
      "Fold 1 | Iteration 500 | Loss: 1.5624 | Train Acc: 0.3864 | Test Acc: 0.0833\n",
      "Fold 1 | Iteration 500 | Loss: 1.5547 | Train Acc: 0.3939 | Test Acc: 0.0833\n",
      "Fold 1 | Iteration 500 | Loss: 1.5470 | Train Acc: 0.3939 | Test Acc: 0.0833\n",
      "Fold 1 | Iteration 500 | Loss: 1.5393 | Train Acc: 0.3939 | Test Acc: 0.0833\n",
      "Fold 1 | Iteration 500 | Loss: 1.5313 | Train Acc: 0.3864 | Test Acc: 0.0833\n",
      "Fold 1 | Iteration 500 | Loss: 1.5229 | Train Acc: 0.3864 | Test Acc: 0.0833\n",
      "Fold 1 | Iteration 500 | Loss: 1.5144 | Train Acc: 0.3939 | Test Acc: 0.0833\n",
      "Fold 1 | Iteration 500 | Loss: 1.5061 | Train Acc: 0.3864 | Test Acc: 0.0833\n",
      "Fold 1 | Iteration 500 | Loss: 1.4981 | Train Acc: 0.4015 | Test Acc: 0.0833\n",
      "Fold 1 | Iteration 500 | Loss: 1.4898 | Train Acc: 0.4167 | Test Acc: 0.0833\n",
      "Fold 1 | Iteration 500 | Loss: 1.4815 | Train Acc: 0.4091 | Test Acc: 0.0556\n",
      "Fold 1 | Iteration 500 | Loss: 1.4732 | Train Acc: 0.4167 | Test Acc: 0.0833\n",
      "Fold 1 | Iteration 500 | Loss: 1.4653 | Train Acc: 0.4242 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.4570 | Train Acc: 0.4394 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.4488 | Train Acc: 0.4394 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.4410 | Train Acc: 0.4242 | Test Acc: 0.0833\n",
      "Fold 1 | Iteration 500 | Loss: 1.4327 | Train Acc: 0.4167 | Test Acc: 0.0833\n",
      "Fold 1 | Iteration 500 | Loss: 1.4243 | Train Acc: 0.4242 | Test Acc: 0.0833\n",
      "Fold 1 | Iteration 500 | Loss: 1.4161 | Train Acc: 0.4242 | Test Acc: 0.0833\n",
      "Fold 1 | Iteration 500 | Loss: 1.4078 | Train Acc: 0.4242 | Test Acc: 0.0833\n",
      "Fold 1 | Iteration 500 | Loss: 1.3992 | Train Acc: 0.4318 | Test Acc: 0.0833\n",
      "Fold 1 | Iteration 500 | Loss: 1.3905 | Train Acc: 0.4470 | Test Acc: 0.0833\n",
      "Fold 1 | Iteration 500 | Loss: 1.3818 | Train Acc: 0.4394 | Test Acc: 0.0833\n",
      "Fold 1 | Iteration 500 | Loss: 1.3726 | Train Acc: 0.4394 | Test Acc: 0.0833\n",
      "Fold 1 | Iteration 500 | Loss: 1.3634 | Train Acc: 0.4470 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.3541 | Train Acc: 0.4697 | Test Acc: 0.0833\n",
      "Fold 1 | Iteration 500 | Loss: 1.3444 | Train Acc: 0.4773 | Test Acc: 0.0833\n",
      "Fold 1 | Iteration 500 | Loss: 1.3344 | Train Acc: 0.4697 | Test Acc: 0.0833\n",
      "Fold 1 | Iteration 500 | Loss: 1.3242 | Train Acc: 0.4773 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.3139 | Train Acc: 0.4773 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.3037 | Train Acc: 0.4773 | Test Acc: 0.0833\n",
      "Fold 1 | Iteration 500 | Loss: 1.2932 | Train Acc: 0.4848 | Test Acc: 0.0833\n",
      "Fold 1 | Iteration 500 | Loss: 1.2826 | Train Acc: 0.5000 | Test Acc: 0.0833\n",
      "Fold 1 | Iteration 500 | Loss: 1.2721 | Train Acc: 0.5076 | Test Acc: 0.0833\n",
      "Fold 1 | Iteration 500 | Loss: 1.2614 | Train Acc: 0.5152 | Test Acc: 0.0556\n",
      "Fold 1 | Iteration 500 | Loss: 1.2508 | Train Acc: 0.5227 | Test Acc: 0.0556\n",
      "Fold 1 | Iteration 500 | Loss: 1.2399 | Train Acc: 0.5303 | Test Acc: 0.0556\n",
      "Fold 1 | Iteration 500 | Loss: 1.2288 | Train Acc: 0.5379 | Test Acc: 0.0556\n",
      "Fold 1 | Iteration 500 | Loss: 1.2173 | Train Acc: 0.5379 | Test Acc: 0.0556\n",
      "Fold 1 | Iteration 500 | Loss: 1.2058 | Train Acc: 0.5379 | Test Acc: 0.0556\n",
      "Fold 1 | Iteration 500 | Loss: 1.1944 | Train Acc: 0.5303 | Test Acc: 0.0556\n",
      "Fold 1 | Iteration 500 | Loss: 1.1830 | Train Acc: 0.5379 | Test Acc: 0.0556\n",
      "Fold 1 | Iteration 500 | Loss: 1.1716 | Train Acc: 0.5379 | Test Acc: 0.0556\n",
      "Fold 1 | Iteration 500 | Loss: 1.1599 | Train Acc: 0.5303 | Test Acc: 0.0556\n",
      "Fold 1 | Iteration 500 | Loss: 1.1480 | Train Acc: 0.5455 | Test Acc: 0.0833\n",
      "Fold 1 | Iteration 500 | Loss: 1.1363 | Train Acc: 0.5455 | Test Acc: 0.0556\n",
      "Fold 1 | Iteration 500 | Loss: 1.1244 | Train Acc: 0.5682 | Test Acc: 0.0556\n",
      "Fold 1 | Iteration 500 | Loss: 1.1123 | Train Acc: 0.5606 | Test Acc: 0.0556\n",
      "Fold 1 | Iteration 500 | Loss: 1.1005 | Train Acc: 0.5909 | Test Acc: 0.0556\n",
      "Fold 1 | Iteration 500 | Loss: 1.0886 | Train Acc: 0.5833 | Test Acc: 0.0556\n",
      "Fold 1 | Iteration 500 | Loss: 1.0769 | Train Acc: 0.5758 | Test Acc: 0.0833\n",
      "Fold 1 | Iteration 500 | Loss: 1.0653 | Train Acc: 0.5909 | Test Acc: 0.0556\n",
      "Fold 1 | Iteration 500 | Loss: 1.0536 | Train Acc: 0.6061 | Test Acc: 0.0833\n",
      "Fold 1 | Iteration 500 | Loss: 1.0418 | Train Acc: 0.5909 | Test Acc: 0.0833\n",
      "Fold 1 | Iteration 500 | Loss: 1.0301 | Train Acc: 0.6212 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.0185 | Train Acc: 0.6061 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 1.0075 | Train Acc: 0.6136 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 0.9987 | Train Acc: 0.5909 | Test Acc: 0.0556\n",
      "Fold 1 | Iteration 500 | Loss: 0.9904 | Train Acc: 0.6212 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 0.9758 | Train Acc: 0.6364 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 0.9617 | Train Acc: 0.6364 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 0.9542 | Train Acc: 0.6591 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.9458 | Train Acc: 0.6439 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 0.9328 | Train Acc: 0.6515 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 0.9215 | Train Acc: 0.6970 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.9140 | Train Acc: 0.6818 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 0.9044 | Train Acc: 0.6818 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.8916 | Train Acc: 0.6970 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.8818 | Train Acc: 0.6894 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 0.8742 | Train Acc: 0.6894 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.8649 | Train Acc: 0.7045 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 0.8544 | Train Acc: 0.6970 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 0.8450 | Train Acc: 0.7045 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.8359 | Train Acc: 0.7197 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 0.8272 | Train Acc: 0.7121 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.8190 | Train Acc: 0.7273 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 0.8092 | Train Acc: 0.7121 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.7989 | Train Acc: 0.7197 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.7895 | Train Acc: 0.7348 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 0.7809 | Train Acc: 0.7197 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.7728 | Train Acc: 0.7500 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 0.7653 | Train Acc: 0.7348 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.7572 | Train Acc: 0.7576 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 0.7491 | Train Acc: 0.7500 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.7406 | Train Acc: 0.7576 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 0.7315 | Train Acc: 0.7727 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.7220 | Train Acc: 0.7576 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 0.7124 | Train Acc: 0.7652 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 0.7038 | Train Acc: 0.7727 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.6959 | Train Acc: 0.7727 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 0.6889 | Train Acc: 0.7879 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.6827 | Train Acc: 0.7727 | Test Acc: 0.1111\n",
      "Fold 1 | Iteration 500 | Loss: 0.6780 | Train Acc: 0.8182 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.6737 | Train Acc: 0.7879 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.6672 | Train Acc: 0.8030 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 0.6537 | Train Acc: 0.7879 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.6420 | Train Acc: 0.7727 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.6371 | Train Acc: 0.8182 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.6340 | Train Acc: 0.7879 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.6252 | Train Acc: 0.8258 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.6134 | Train Acc: 0.8182 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.6066 | Train Acc: 0.7955 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.6032 | Train Acc: 0.8409 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.5968 | Train Acc: 0.7955 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.5873 | Train Acc: 0.8258 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.5785 | Train Acc: 0.8485 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.5734 | Train Acc: 0.8030 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 0.5687 | Train Acc: 0.8485 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.5616 | Train Acc: 0.8258 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.5523 | Train Acc: 0.8333 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.5450 | Train Acc: 0.8485 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.5404 | Train Acc: 0.8333 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.5352 | Train Acc: 0.8485 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.5280 | Train Acc: 0.8409 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.5194 | Train Acc: 0.8485 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.5117 | Train Acc: 0.8561 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.5061 | Train Acc: 0.8409 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.5009 | Train Acc: 0.8485 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.4953 | Train Acc: 0.8485 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.4897 | Train Acc: 0.8561 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.4835 | Train Acc: 0.8561 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.4774 | Train Acc: 0.8636 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.4727 | Train Acc: 0.8485 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.4699 | Train Acc: 0.8561 | Test Acc: 0.1389\n",
      "Fold 1 | Iteration 500 | Loss: 0.4658 | Train Acc: 0.8636 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.4521 | Train Acc: 0.8864 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.4449 | Train Acc: 0.8788 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.4451 | Train Acc: 0.8864 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.4366 | Train Acc: 0.8939 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.4277 | Train Acc: 0.8712 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.4252 | Train Acc: 0.8788 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.4210 | Train Acc: 0.8864 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.4122 | Train Acc: 0.8864 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.4051 | Train Acc: 0.9015 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.3998 | Train Acc: 0.8864 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.3938 | Train Acc: 0.8864 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.3880 | Train Acc: 0.9015 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.3841 | Train Acc: 0.8939 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.3788 | Train Acc: 0.8939 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.3732 | Train Acc: 0.9015 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.3697 | Train Acc: 0.8864 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.3669 | Train Acc: 0.9015 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.3629 | Train Acc: 0.8864 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.3575 | Train Acc: 0.9015 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.3529 | Train Acc: 0.8939 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.3465 | Train Acc: 0.9015 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.3384 | Train Acc: 0.8939 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.3329 | Train Acc: 0.9015 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.3289 | Train Acc: 0.9015 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.3251 | Train Acc: 0.8864 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.3226 | Train Acc: 0.9091 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.3206 | Train Acc: 0.8939 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.3165 | Train Acc: 0.9091 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.3108 | Train Acc: 0.9015 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.3062 | Train Acc: 0.9091 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.3004 | Train Acc: 0.9091 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.2942 | Train Acc: 0.9091 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.2894 | Train Acc: 0.9091 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.2883 | Train Acc: 0.9167 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.2869 | Train Acc: 0.9167 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.2821 | Train Acc: 0.9167 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.2762 | Train Acc: 0.9167 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.2703 | Train Acc: 0.9167 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.2666 | Train Acc: 0.9394 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.2641 | Train Acc: 0.9167 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.2605 | Train Acc: 0.9242 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.2570 | Train Acc: 0.9318 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.2545 | Train Acc: 0.9167 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.2500 | Train Acc: 0.9242 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.2440 | Train Acc: 0.9242 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.2403 | Train Acc: 0.9242 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.2384 | Train Acc: 0.9394 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.2342 | Train Acc: 0.9394 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.2300 | Train Acc: 0.9242 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.2278 | Train Acc: 0.9545 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.2253 | Train Acc: 0.9318 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.2219 | Train Acc: 0.9470 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.2191 | Train Acc: 0.9394 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.2171 | Train Acc: 0.9545 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.2161 | Train Acc: 0.9318 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.2152 | Train Acc: 0.9470 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.2152 | Train Acc: 0.9318 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.2110 | Train Acc: 0.9621 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.2049 | Train Acc: 0.9470 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1989 | Train Acc: 0.9470 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.1966 | Train Acc: 0.9621 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.1963 | Train Acc: 0.9545 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1958 | Train Acc: 0.9545 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.1935 | Train Acc: 0.9621 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.1888 | Train Acc: 0.9545 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.1840 | Train Acc: 0.9545 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.1808 | Train Acc: 0.9697 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1797 | Train Acc: 0.9621 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.1785 | Train Acc: 0.9545 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1763 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.1737 | Train Acc: 0.9470 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1701 | Train Acc: 0.9621 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.1672 | Train Acc: 0.9697 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.1656 | Train Acc: 0.9545 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1642 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.1630 | Train Acc: 0.9545 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1606 | Train Acc: 0.9697 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.1582 | Train Acc: 0.9697 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.1551 | Train Acc: 0.9621 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.1527 | Train Acc: 0.9621 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.1513 | Train Acc: 0.9697 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.1496 | Train Acc: 0.9697 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.1481 | Train Acc: 0.9621 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1466 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.1447 | Train Acc: 0.9697 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.1426 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.1407 | Train Acc: 0.9697 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.1384 | Train Acc: 0.9621 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.1361 | Train Acc: 0.9697 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1345 | Train Acc: 0.9697 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.1334 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.1324 | Train Acc: 0.9697 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.1312 | Train Acc: 0.9697 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.1302 | Train Acc: 0.9697 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.1278 | Train Acc: 0.9697 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1252 | Train Acc: 0.9773 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1233 | Train Acc: 0.9697 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.1219 | Train Acc: 0.9697 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.1208 | Train Acc: 0.9697 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.1198 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 1 | Iteration 500 | Loss: 0.1186 | Train Acc: 0.9697 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.1173 | Train Acc: 0.9697 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.1159 | Train Acc: 0.9697 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.1137 | Train Acc: 0.9697 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.1119 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.1105 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.1092 | Train Acc: 0.9773 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1079 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.1066 | Train Acc: 0.9697 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1057 | Train Acc: 0.9697 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.1046 | Train Acc: 0.9697 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1035 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.1024 | Train Acc: 0.9697 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.1011 | Train Acc: 0.9848 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0997 | Train Acc: 0.9773 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0983 | Train Acc: 0.9924 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0968 | Train Acc: 0.9848 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0955 | Train Acc: 0.9848 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0945 | Train Acc: 0.9848 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0936 | Train Acc: 0.9773 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0933 | Train Acc: 0.9848 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0931 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0923 | Train Acc: 0.9848 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0910 | Train Acc: 0.9924 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0892 | Train Acc: 0.9924 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0874 | Train Acc: 0.9924 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0862 | Train Acc: 0.9924 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0856 | Train Acc: 0.9924 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0852 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0842 | Train Acc: 0.9924 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0827 | Train Acc: 0.9924 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0814 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0804 | Train Acc: 0.9924 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0796 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0790 | Train Acc: 0.9924 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0782 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0773 | Train Acc: 0.9924 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0762 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0750 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0742 | Train Acc: 0.9924 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0736 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0728 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0719 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0708 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0699 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0693 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0684 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0677 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0669 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0663 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0656 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0651 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0646 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0641 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0631 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0622 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0614 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0606 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0601 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0599 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0595 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0589 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0578 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0569 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0562 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0559 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0556 | Train Acc: 1.0000 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.0555 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0548 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0538 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0528 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0525 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0524 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0519 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0508 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0499 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0495 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0496 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0494 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0484 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0474 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0470 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0468 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0464 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0456 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0449 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0444 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0440 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0436 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0431 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0426 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0421 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0417 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0412 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0407 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0403 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0400 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0396 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0390 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0386 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0382 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0378 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0373 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0370 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0366 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0362 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0358 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0355 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0351 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0347 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0344 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0340 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0336 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0333 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0330 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0327 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0323 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0320 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0317 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0313 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0310 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0307 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0304 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0300 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0297 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0294 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0291 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0287 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0285 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0281 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0279 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0276 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0273 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0270 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0268 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0265 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0262 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0260 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0257 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0255 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0251 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0249 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0246 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0244 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0242 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0239 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0237 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0234 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0232 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0229 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0228 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0225 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0223 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0221 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0218 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0216 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0214 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0212 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0210 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0208 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0206 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0204 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0201 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0199 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0197 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0196 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0194 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0192 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0190 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0189 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0187 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0185 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0183 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0181 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0180 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0178 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0176 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0175 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0173 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0171 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0170 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0168 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0166 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0165 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0163 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0162 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0160 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0159 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0157 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0156 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0154 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0152 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0151 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0149 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0148 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0147 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0145 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0144 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0143 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0141 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0140 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0139 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0137 | Train Acc: 1.0000 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.0136 | Train Acc: 1.0000 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.0135 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0134 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0133 | Train Acc: 1.0000 | Test Acc: 0.1944\n",
      "Fold 1 | Iteration 500 | Loss: 0.0131 | Train Acc: 1.0000 | Test Acc: 0.2222\n",
      "Fold 1 | Iteration 500 | Loss: 0.0130 | Train Acc: 1.0000 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.0129 | Train Acc: 1.0000 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.0128 | Train Acc: 1.0000 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.0127 | Train Acc: 1.0000 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.0126 | Train Acc: 1.0000 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.0125 | Train Acc: 1.0000 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.0124 | Train Acc: 1.0000 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.0123 | Train Acc: 1.0000 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.0122 | Train Acc: 1.0000 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.0121 | Train Acc: 1.0000 | Test Acc: 0.2500\n",
      "Fold 1 | Iteration 500 | Loss: 0.0120 | Train Acc: 1.0000 | Test Acc: 0.2500\n",
      "\n",
      "--- Fold 2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\malth\\Miniconda3\\envs\\dtu02450\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 | Iteration 500 | Loss: 2.4155 | Train Acc: 0.0758 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 2.3977 | Train Acc: 0.1515 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 2.3803 | Train Acc: 0.2348 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 2.3621 | Train Acc: 0.2576 | Test Acc: 0.0556\n",
      "Fold 2 | Iteration 500 | Loss: 2.3413 | Train Acc: 0.2500 | Test Acc: 0.0278\n",
      "Fold 2 | Iteration 500 | Loss: 2.3168 | Train Acc: 0.2424 | Test Acc: 0.0278\n",
      "Fold 2 | Iteration 500 | Loss: 2.2872 | Train Acc: 0.2348 | Test Acc: 0.0278\n",
      "Fold 2 | Iteration 500 | Loss: 2.2518 | Train Acc: 0.2348 | Test Acc: 0.0556\n",
      "Fold 2 | Iteration 500 | Loss: 2.2101 | Train Acc: 0.2348 | Test Acc: 0.0556\n",
      "Fold 2 | Iteration 500 | Loss: 2.1625 | Train Acc: 0.2348 | Test Acc: 0.0556\n",
      "Fold 2 | Iteration 500 | Loss: 2.1107 | Train Acc: 0.2348 | Test Acc: 0.0556\n",
      "Fold 2 | Iteration 500 | Loss: 2.0580 | Train Acc: 0.2348 | Test Acc: 0.0556\n",
      "Fold 2 | Iteration 500 | Loss: 2.0106 | Train Acc: 0.2500 | Test Acc: 0.0556\n",
      "Fold 2 | Iteration 500 | Loss: 1.9748 | Train Acc: 0.2576 | Test Acc: 0.0556\n",
      "Fold 2 | Iteration 500 | Loss: 1.9529 | Train Acc: 0.2424 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 1.9400 | Train Acc: 0.2424 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 1.9297 | Train Acc: 0.2652 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 1.9195 | Train Acc: 0.2727 | Test Acc: 0.3333\n",
      "Fold 2 | Iteration 500 | Loss: 1.9094 | Train Acc: 0.2424 | Test Acc: 0.4167\n",
      "Fold 2 | Iteration 500 | Loss: 1.8975 | Train Acc: 0.2348 | Test Acc: 0.4167\n",
      "Fold 2 | Iteration 500 | Loss: 1.8820 | Train Acc: 0.2576 | Test Acc: 0.4167\n",
      "Fold 2 | Iteration 500 | Loss: 1.8646 | Train Acc: 0.2576 | Test Acc: 0.4167\n",
      "Fold 2 | Iteration 500 | Loss: 1.8492 | Train Acc: 0.2879 | Test Acc: 0.3333\n",
      "Fold 2 | Iteration 500 | Loss: 1.8381 | Train Acc: 0.2803 | Test Acc: 0.2500\n",
      "Fold 2 | Iteration 500 | Loss: 1.8313 | Train Acc: 0.2727 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 1.8273 | Train Acc: 0.2652 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 1.8234 | Train Acc: 0.2500 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 1.8180 | Train Acc: 0.2652 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 1.8103 | Train Acc: 0.2652 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 1.8007 | Train Acc: 0.2727 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 1.7899 | Train Acc: 0.2576 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 1.7787 | Train Acc: 0.2879 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 1.7678 | Train Acc: 0.3030 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 1.7581 | Train Acc: 0.3106 | Test Acc: 0.2500\n",
      "Fold 2 | Iteration 500 | Loss: 1.7502 | Train Acc: 0.3182 | Test Acc: 0.3056\n",
      "Fold 2 | Iteration 500 | Loss: 1.7432 | Train Acc: 0.3258 | Test Acc: 0.2778\n",
      "Fold 2 | Iteration 500 | Loss: 1.7360 | Train Acc: 0.3258 | Test Acc: 0.2222\n",
      "Fold 2 | Iteration 500 | Loss: 1.7277 | Train Acc: 0.3409 | Test Acc: 0.2222\n",
      "Fold 2 | Iteration 500 | Loss: 1.7184 | Train Acc: 0.3333 | Test Acc: 0.2500\n",
      "Fold 2 | Iteration 500 | Loss: 1.7085 | Train Acc: 0.3106 | Test Acc: 0.2500\n",
      "Fold 2 | Iteration 500 | Loss: 1.6985 | Train Acc: 0.3030 | Test Acc: 0.2500\n",
      "Fold 2 | Iteration 500 | Loss: 1.6882 | Train Acc: 0.3106 | Test Acc: 0.2500\n",
      "Fold 2 | Iteration 500 | Loss: 1.6782 | Train Acc: 0.3106 | Test Acc: 0.2500\n",
      "Fold 2 | Iteration 500 | Loss: 1.6686 | Train Acc: 0.3258 | Test Acc: 0.2222\n",
      "Fold 2 | Iteration 500 | Loss: 1.6595 | Train Acc: 0.3333 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 1.6509 | Train Acc: 0.3485 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 1.6418 | Train Acc: 0.3561 | Test Acc: 0.2500\n",
      "Fold 2 | Iteration 500 | Loss: 1.6319 | Train Acc: 0.3485 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 1.6218 | Train Acc: 0.3561 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 1.6115 | Train Acc: 0.3712 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 1.6017 | Train Acc: 0.3712 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 1.5920 | Train Acc: 0.3864 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 1.5819 | Train Acc: 0.3864 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 1.5717 | Train Acc: 0.3939 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 1.5612 | Train Acc: 0.4015 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 1.5503 | Train Acc: 0.3939 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 1.5392 | Train Acc: 0.3939 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 1.5277 | Train Acc: 0.3788 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 1.5157 | Train Acc: 0.3864 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 1.5032 | Train Acc: 0.3939 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 1.4905 | Train Acc: 0.4015 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 1.4778 | Train Acc: 0.4015 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 1.4648 | Train Acc: 0.4242 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 1.4514 | Train Acc: 0.4394 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 1.4380 | Train Acc: 0.4545 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 1.4243 | Train Acc: 0.4621 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 1.4104 | Train Acc: 0.4697 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 1.3964 | Train Acc: 0.4848 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 1.3824 | Train Acc: 0.4848 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 1.3682 | Train Acc: 0.4924 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 1.3541 | Train Acc: 0.5000 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 1.3399 | Train Acc: 0.4924 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 1.3256 | Train Acc: 0.5076 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 1.3113 | Train Acc: 0.5152 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 1.2968 | Train Acc: 0.5227 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 1.2825 | Train Acc: 0.5152 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 1.2686 | Train Acc: 0.5303 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 1.2544 | Train Acc: 0.5303 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 1.2401 | Train Acc: 0.5455 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 1.2256 | Train Acc: 0.5379 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 1.2108 | Train Acc: 0.5606 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 1.1962 | Train Acc: 0.5530 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 1.1817 | Train Acc: 0.5606 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 1.1674 | Train Acc: 0.5606 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 1.1531 | Train Acc: 0.5682 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 1.1388 | Train Acc: 0.5758 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 1.1250 | Train Acc: 0.5682 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 1.1117 | Train Acc: 0.5682 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 1.0977 | Train Acc: 0.5833 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 1.0818 | Train Acc: 0.5758 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 1.0662 | Train Acc: 0.5682 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 1.0533 | Train Acc: 0.5985 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 1.0412 | Train Acc: 0.5682 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 1.0272 | Train Acc: 0.5909 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 1.0128 | Train Acc: 0.6136 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 1.0000 | Train Acc: 0.5909 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.9883 | Train Acc: 0.6288 | Test Acc: 0.0556\n",
      "Fold 2 | Iteration 500 | Loss: 0.9755 | Train Acc: 0.6136 | Test Acc: 0.0556\n",
      "Fold 2 | Iteration 500 | Loss: 0.9621 | Train Acc: 0.6288 | Test Acc: 0.0556\n",
      "Fold 2 | Iteration 500 | Loss: 0.9498 | Train Acc: 0.6364 | Test Acc: 0.0556\n",
      "Fold 2 | Iteration 500 | Loss: 0.9385 | Train Acc: 0.6136 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.9271 | Train Acc: 0.6515 | Test Acc: 0.0556\n",
      "Fold 2 | Iteration 500 | Loss: 0.9150 | Train Acc: 0.6515 | Test Acc: 0.0556\n",
      "Fold 2 | Iteration 500 | Loss: 0.9035 | Train Acc: 0.6515 | Test Acc: 0.0556\n",
      "Fold 2 | Iteration 500 | Loss: 0.8931 | Train Acc: 0.6591 | Test Acc: 0.0556\n",
      "Fold 2 | Iteration 500 | Loss: 0.8843 | Train Acc: 0.6591 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.8756 | Train Acc: 0.6591 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.8630 | Train Acc: 0.6742 | Test Acc: 0.0556\n",
      "Fold 2 | Iteration 500 | Loss: 0.8503 | Train Acc: 0.6667 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.8419 | Train Acc: 0.6818 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.8317 | Train Acc: 0.6818 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.8191 | Train Acc: 0.6818 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.8108 | Train Acc: 0.6818 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.8020 | Train Acc: 0.6894 | Test Acc: 0.0556\n",
      "Fold 2 | Iteration 500 | Loss: 0.7898 | Train Acc: 0.6894 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.7798 | Train Acc: 0.6970 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.7716 | Train Acc: 0.6970 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.7614 | Train Acc: 0.7121 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.7514 | Train Acc: 0.6894 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.7432 | Train Acc: 0.7121 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.7336 | Train Acc: 0.6970 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.7232 | Train Acc: 0.6970 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.7141 | Train Acc: 0.7121 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.7058 | Train Acc: 0.7045 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.6965 | Train Acc: 0.7273 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.6866 | Train Acc: 0.7045 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.6783 | Train Acc: 0.7500 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.6714 | Train Acc: 0.7348 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.6672 | Train Acc: 0.7348 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.6653 | Train Acc: 0.7500 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.6529 | Train Acc: 0.7500 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.6359 | Train Acc: 0.7652 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.6303 | Train Acc: 0.7576 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.6278 | Train Acc: 0.7652 | Test Acc: 0.0833\n",
      "Fold 2 | Iteration 500 | Loss: 0.6140 | Train Acc: 0.7803 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.6045 | Train Acc: 0.7576 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.6032 | Train Acc: 0.7803 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.5917 | Train Acc: 0.7803 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.5813 | Train Acc: 0.8030 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.5801 | Train Acc: 0.7955 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.5701 | Train Acc: 0.8106 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.5595 | Train Acc: 0.8106 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.5568 | Train Acc: 0.8106 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.5487 | Train Acc: 0.8182 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.5393 | Train Acc: 0.8182 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.5354 | Train Acc: 0.8106 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.5280 | Train Acc: 0.8409 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.5188 | Train Acc: 0.8258 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.5147 | Train Acc: 0.8485 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.5081 | Train Acc: 0.8561 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.4993 | Train Acc: 0.8409 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.4944 | Train Acc: 0.8561 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.4886 | Train Acc: 0.8561 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.4800 | Train Acc: 0.8485 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.4738 | Train Acc: 0.8636 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.4694 | Train Acc: 0.8485 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.4626 | Train Acc: 0.8712 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.4562 | Train Acc: 0.8561 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.4554 | Train Acc: 0.8636 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.4597 | Train Acc: 0.8333 | Test Acc: 0.1111\n",
      "Fold 2 | Iteration 500 | Loss: 0.4652 | Train Acc: 0.8788 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.4435 | Train Acc: 0.8788 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.4276 | Train Acc: 0.8561 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.4338 | Train Acc: 0.9015 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.4265 | Train Acc: 0.8636 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.4121 | Train Acc: 0.8636 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.4108 | Train Acc: 0.9015 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.4100 | Train Acc: 0.8636 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.3979 | Train Acc: 0.8712 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.3936 | Train Acc: 0.9015 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.3932 | Train Acc: 0.8636 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.3848 | Train Acc: 0.8788 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.3772 | Train Acc: 0.8939 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.3765 | Train Acc: 0.8788 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.3713 | Train Acc: 0.8864 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.3638 | Train Acc: 0.8939 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.3605 | Train Acc: 0.8712 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.3572 | Train Acc: 0.8864 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.3513 | Train Acc: 0.8939 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.3468 | Train Acc: 0.8939 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.3422 | Train Acc: 0.9015 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.3381 | Train Acc: 0.9015 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.3345 | Train Acc: 0.8864 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.3293 | Train Acc: 0.9015 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.3242 | Train Acc: 0.9015 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.3211 | Train Acc: 0.8939 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.3182 | Train Acc: 0.9015 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.3137 | Train Acc: 0.9091 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.3087 | Train Acc: 0.8939 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.3048 | Train Acc: 0.9091 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.3012 | Train Acc: 0.9015 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2970 | Train Acc: 0.9015 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2933 | Train Acc: 0.9091 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2906 | Train Acc: 0.8939 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2881 | Train Acc: 0.9091 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2841 | Train Acc: 0.8939 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2799 | Train Acc: 0.9015 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2765 | Train Acc: 0.9015 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2737 | Train Acc: 0.9015 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.2698 | Train Acc: 0.9091 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2651 | Train Acc: 0.9091 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.2614 | Train Acc: 0.9091 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.2580 | Train Acc: 0.9167 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2550 | Train Acc: 0.9167 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.2518 | Train Acc: 0.9242 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.2488 | Train Acc: 0.9318 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.2461 | Train Acc: 0.9318 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.2446 | Train Acc: 0.9242 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2463 | Train Acc: 0.9167 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.2529 | Train Acc: 0.8939 | Test Acc: 0.1389\n",
      "Fold 2 | Iteration 500 | Loss: 0.2656 | Train Acc: 0.9242 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.2614 | Train Acc: 0.9242 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.2430 | Train Acc: 0.9318 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.2265 | Train Acc: 0.9318 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.2388 | Train Acc: 0.9015 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.2443 | Train Acc: 0.9394 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.2222 | Train Acc: 0.9394 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.2215 | Train Acc: 0.9167 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.2317 | Train Acc: 0.9394 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.2165 | Train Acc: 0.9394 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.2123 | Train Acc: 0.9394 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.2198 | Train Acc: 0.9470 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.2092 | Train Acc: 0.9470 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.2055 | Train Acc: 0.9394 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.2094 | Train Acc: 0.9470 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.2016 | Train Acc: 0.9545 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1991 | Train Acc: 0.9470 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.2008 | Train Acc: 0.9545 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1946 | Train Acc: 0.9545 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1934 | Train Acc: 0.9470 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1937 | Train Acc: 0.9545 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1883 | Train Acc: 0.9545 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1878 | Train Acc: 0.9470 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1868 | Train Acc: 0.9621 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1824 | Train Acc: 0.9545 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1816 | Train Acc: 0.9545 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1803 | Train Acc: 0.9621 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1769 | Train Acc: 0.9621 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1757 | Train Acc: 0.9545 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1746 | Train Acc: 0.9621 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1717 | Train Acc: 0.9621 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1702 | Train Acc: 0.9545 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1692 | Train Acc: 0.9621 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1667 | Train Acc: 0.9621 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1651 | Train Acc: 0.9621 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1644 | Train Acc: 0.9621 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1622 | Train Acc: 0.9621 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1603 | Train Acc: 0.9621 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1594 | Train Acc: 0.9621 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1577 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1558 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1547 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1532 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1514 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1500 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1489 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1473 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1458 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1445 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1433 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1418 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1404 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1392 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1380 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1367 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1354 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1343 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1331 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1317 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1305 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1295 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1283 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1271 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1260 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1249 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1237 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1225 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1214 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1204 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1195 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1186 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1177 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1168 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1159 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1147 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1136 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1126 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1115 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1104 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1095 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1086 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1077 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1069 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1061 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1055 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1048 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1041 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1034 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1026 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1015 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.1004 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0993 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0985 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0978 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0970 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0966 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0961 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0956 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0953 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0946 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0932 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0921 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0910 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0904 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0899 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0895 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0890 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0884 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0876 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0868 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0859 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0852 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0847 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0842 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0837 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0833 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0829 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0824 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0818 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0811 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0805 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0798 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0791 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0785 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0780 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0775 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0771 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0767 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0763 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0759 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0756 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0753 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0750 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0747 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0742 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0735 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0728 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0722 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0718 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0713 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0708 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0703 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0700 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0697 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0693 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0688 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0685 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0682 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0679 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0677 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0677 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0677 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0675 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0672 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0667 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0660 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0655 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0648 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0643 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0639 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0635 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0633 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0630 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0629 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0627 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0626 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0624 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0624 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0622 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0621 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0618 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0614 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0608 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0603 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0598 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0595 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0592 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0590 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0588 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0587 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0587 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0587 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0587 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0585 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0584 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0582 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0579 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0574 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0569 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0565 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0562 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0559 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0556 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0554 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0552 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0551 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0550 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0550 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0550 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0551 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 0.0552 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0553 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 0.0552 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0549 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 0.0545 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0540 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 0.0535 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0531 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 0.0527 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0525 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0523 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 0.0522 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0521 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 0.0520 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0519 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 0.0519 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0519 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 0.0520 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0521 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 0.0523 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0525 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 0.0524 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0522 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 0.0518 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 0.0514 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 0.0507 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 0.0503 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 0.0499 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 0.0497 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 0.0496 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 0.0495 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 0.0494 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 0.0494 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 0.0495 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 0.0497 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0500 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 0.0502 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0503 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 0.0501 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 0.0498 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 0.0493 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 0.0488 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 0.0483 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 0.0479 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 0.0477 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 0.0476 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 0.0475 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 0.0475 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 0.0475 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 0.0476 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 0.0478 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 0.0480 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 0.0482 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 0.0483 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 0.0483 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 0.0481 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 0.0477 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 0.0473 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 0.0469 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 0.0465 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 0.0462 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 0.0460 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 0.0458 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 0.0457 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 0.0457 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 0.0457 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 0.0457 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 0.0459 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 0.0462 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0464 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0467 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0469 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0468 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0467 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0463 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0458 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0453 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0449 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0446 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0444 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0443 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0443 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0444 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0446 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0448 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 0.0450 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0452 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 0.0453 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 2 | Iteration 500 | Loss: 0.0454 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 2 | Iteration 500 | Loss: 0.0452 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "\n",
      "--- Fold 3 ---\n",
      "Fold 3 | Iteration 500 | Loss: 2.4071 | Train Acc: 0.0227 | Test Acc: 0.0000\n",
      "Fold 3 | Iteration 500 | Loss: 2.3899 | Train Acc: 0.1515 | Test Acc: 0.0278\n",
      "Fold 3 | Iteration 500 | Loss: 2.3741 | Train Acc: 0.2727 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 2.3581 | Train Acc: 0.2803 | Test Acc: 0.2500\n",
      "Fold 3 | Iteration 500 | Loss: 2.3398 | Train Acc: 0.2879 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 2.3185 | Train Acc: 0.3030 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 2.2930 | Train Acc: 0.2803 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 2.2622 | Train Acc: 0.2955 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 2.2251 | Train Acc: 0.3106 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 2.1810 | Train Acc: 0.3333 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 2.1297 | Train Acc: 0.3333 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 2.0728 | Train Acc: 0.3409 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 2.0137 | Train Acc: 0.3409 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 1.9578 | Train Acc: 0.3258 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 1.9119 | Train Acc: 0.3333 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 1.8811 | Train Acc: 0.3106 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 1.8648 | Train Acc: 0.3030 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 1.8585 | Train Acc: 0.2727 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 1.8547 | Train Acc: 0.3030 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 1.8487 | Train Acc: 0.2879 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 1.8395 | Train Acc: 0.3106 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 1.8292 | Train Acc: 0.3258 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.8198 | Train Acc: 0.3333 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 1.8122 | Train Acc: 0.3333 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 1.8044 | Train Acc: 0.3333 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 1.7948 | Train Acc: 0.3333 | Test Acc: 0.1944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\malth\\Miniconda3\\envs\\dtu02450\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 | Iteration 500 | Loss: 1.7836 | Train Acc: 0.3333 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 1.7726 | Train Acc: 0.3333 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 1.7638 | Train Acc: 0.3409 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 1.7578 | Train Acc: 0.3409 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 1.7533 | Train Acc: 0.3485 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 1.7492 | Train Acc: 0.3485 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 1.7442 | Train Acc: 0.3485 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 1.7383 | Train Acc: 0.3485 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 1.7322 | Train Acc: 0.3333 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 1.7262 | Train Acc: 0.3333 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 1.7200 | Train Acc: 0.3333 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 1.7137 | Train Acc: 0.3409 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 1.7075 | Train Acc: 0.3409 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 1.7021 | Train Acc: 0.3409 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 1.6972 | Train Acc: 0.3409 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 1.6925 | Train Acc: 0.3409 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 1.6873 | Train Acc: 0.3409 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 1.6819 | Train Acc: 0.3485 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 1.6763 | Train Acc: 0.3485 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 1.6706 | Train Acc: 0.3712 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 1.6646 | Train Acc: 0.3712 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 1.6586 | Train Acc: 0.3788 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 1.6526 | Train Acc: 0.3939 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 1.6463 | Train Acc: 0.4091 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 1.6402 | Train Acc: 0.4091 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 1.6343 | Train Acc: 0.4015 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.6283 | Train Acc: 0.4167 | Test Acc: 0.2500\n",
      "Fold 3 | Iteration 500 | Loss: 1.6223 | Train Acc: 0.4091 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.6166 | Train Acc: 0.4091 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.6107 | Train Acc: 0.4091 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.6046 | Train Acc: 0.4015 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.5982 | Train Acc: 0.4167 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.5916 | Train Acc: 0.4091 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.5849 | Train Acc: 0.4091 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.5781 | Train Acc: 0.4091 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.5712 | Train Acc: 0.4091 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.5643 | Train Acc: 0.4091 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.5571 | Train Acc: 0.4091 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.5496 | Train Acc: 0.4167 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.5420 | Train Acc: 0.4242 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.5341 | Train Acc: 0.4242 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.5260 | Train Acc: 0.4242 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.5177 | Train Acc: 0.4394 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.5093 | Train Acc: 0.4394 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.5005 | Train Acc: 0.4394 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.4915 | Train Acc: 0.4394 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.4822 | Train Acc: 0.4545 | Test Acc: 0.2500\n",
      "Fold 3 | Iteration 500 | Loss: 1.4726 | Train Acc: 0.4621 | Test Acc: 0.2500\n",
      "Fold 3 | Iteration 500 | Loss: 1.4626 | Train Acc: 0.4697 | Test Acc: 0.2500\n",
      "Fold 3 | Iteration 500 | Loss: 1.4522 | Train Acc: 0.4697 | Test Acc: 0.2500\n",
      "Fold 3 | Iteration 500 | Loss: 1.4417 | Train Acc: 0.4621 | Test Acc: 0.2500\n",
      "Fold 3 | Iteration 500 | Loss: 1.4307 | Train Acc: 0.4621 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.4193 | Train Acc: 0.4621 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.4073 | Train Acc: 0.4773 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.3947 | Train Acc: 0.4848 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.3817 | Train Acc: 0.4848 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.3686 | Train Acc: 0.4848 | Test Acc: 0.2500\n",
      "Fold 3 | Iteration 500 | Loss: 1.3550 | Train Acc: 0.4848 | Test Acc: 0.2500\n",
      "Fold 3 | Iteration 500 | Loss: 1.3412 | Train Acc: 0.4848 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.3272 | Train Acc: 0.4924 | Test Acc: 0.2500\n",
      "Fold 3 | Iteration 500 | Loss: 1.3128 | Train Acc: 0.5000 | Test Acc: 0.2500\n",
      "Fold 3 | Iteration 500 | Loss: 1.2979 | Train Acc: 0.5076 | Test Acc: 0.2500\n",
      "Fold 3 | Iteration 500 | Loss: 1.2827 | Train Acc: 0.5152 | Test Acc: 0.2500\n",
      "Fold 3 | Iteration 500 | Loss: 1.2672 | Train Acc: 0.5227 | Test Acc: 0.2500\n",
      "Fold 3 | Iteration 500 | Loss: 1.2517 | Train Acc: 0.5303 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.2370 | Train Acc: 0.5303 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 1.2230 | Train Acc: 0.5379 | Test Acc: 0.2500\n",
      "Fold 3 | Iteration 500 | Loss: 1.2077 | Train Acc: 0.5682 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 1.1896 | Train Acc: 0.5833 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 1.1727 | Train Acc: 0.5833 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.1592 | Train Acc: 0.5606 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.1457 | Train Acc: 0.5833 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.1286 | Train Acc: 0.5833 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.1112 | Train Acc: 0.5985 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.0964 | Train Acc: 0.5833 | Test Acc: 0.2500\n",
      "Fold 3 | Iteration 500 | Loss: 1.0825 | Train Acc: 0.6212 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.0684 | Train Acc: 0.5985 | Test Acc: 0.2500\n",
      "Fold 3 | Iteration 500 | Loss: 1.0520 | Train Acc: 0.6364 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.0348 | Train Acc: 0.6364 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 1.0190 | Train Acc: 0.6061 | Test Acc: 0.2500\n",
      "Fold 3 | Iteration 500 | Loss: 1.0044 | Train Acc: 0.6439 | Test Acc: 0.2500\n",
      "Fold 3 | Iteration 500 | Loss: 0.9902 | Train Acc: 0.6439 | Test Acc: 0.2500\n",
      "Fold 3 | Iteration 500 | Loss: 0.9756 | Train Acc: 0.6515 | Test Acc: 0.2500\n",
      "Fold 3 | Iteration 500 | Loss: 0.9609 | Train Acc: 0.6742 | Test Acc: 0.2500\n",
      "Fold 3 | Iteration 500 | Loss: 0.9455 | Train Acc: 0.6818 | Test Acc: 0.2500\n",
      "Fold 3 | Iteration 500 | Loss: 0.9299 | Train Acc: 0.6818 | Test Acc: 0.2500\n",
      "Fold 3 | Iteration 500 | Loss: 0.9145 | Train Acc: 0.6894 | Test Acc: 0.2500\n",
      "Fold 3 | Iteration 500 | Loss: 0.8994 | Train Acc: 0.6970 | Test Acc: 0.2500\n",
      "Fold 3 | Iteration 500 | Loss: 0.8847 | Train Acc: 0.6970 | Test Acc: 0.2500\n",
      "Fold 3 | Iteration 500 | Loss: 0.8708 | Train Acc: 0.7121 | Test Acc: 0.2500\n",
      "Fold 3 | Iteration 500 | Loss: 0.8578 | Train Acc: 0.6894 | Test Acc: 0.2500\n",
      "Fold 3 | Iteration 500 | Loss: 0.8476 | Train Acc: 0.7424 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.8423 | Train Acc: 0.6818 | Test Acc: 0.2778\n",
      "Fold 3 | Iteration 500 | Loss: 0.8361 | Train Acc: 0.7424 | Test Acc: 0.2500\n",
      "Fold 3 | Iteration 500 | Loss: 0.8124 | Train Acc: 0.7273 | Test Acc: 0.2500\n",
      "Fold 3 | Iteration 500 | Loss: 0.7933 | Train Acc: 0.6894 | Test Acc: 0.2500\n",
      "Fold 3 | Iteration 500 | Loss: 0.7923 | Train Acc: 0.7424 | Test Acc: 0.2500\n",
      "Fold 3 | Iteration 500 | Loss: 0.7783 | Train Acc: 0.7424 | Test Acc: 0.2500\n",
      "Fold 3 | Iteration 500 | Loss: 0.7596 | Train Acc: 0.7045 | Test Acc: 0.2500\n",
      "Fold 3 | Iteration 500 | Loss: 0.7556 | Train Acc: 0.7576 | Test Acc: 0.2500\n",
      "Fold 3 | Iteration 500 | Loss: 0.7452 | Train Acc: 0.7348 | Test Acc: 0.2500\n",
      "Fold 3 | Iteration 500 | Loss: 0.7280 | Train Acc: 0.7273 | Test Acc: 0.2500\n",
      "Fold 3 | Iteration 500 | Loss: 0.7219 | Train Acc: 0.7576 | Test Acc: 0.2500\n",
      "Fold 3 | Iteration 500 | Loss: 0.7128 | Train Acc: 0.7424 | Test Acc: 0.2500\n",
      "Fold 3 | Iteration 500 | Loss: 0.6976 | Train Acc: 0.7424 | Test Acc: 0.2500\n",
      "Fold 3 | Iteration 500 | Loss: 0.6908 | Train Acc: 0.7652 | Test Acc: 0.2500\n",
      "Fold 3 | Iteration 500 | Loss: 0.6823 | Train Acc: 0.7576 | Test Acc: 0.2500\n",
      "Fold 3 | Iteration 500 | Loss: 0.6680 | Train Acc: 0.7576 | Test Acc: 0.2500\n",
      "Fold 3 | Iteration 500 | Loss: 0.6598 | Train Acc: 0.7652 | Test Acc: 0.2500\n",
      "Fold 3 | Iteration 500 | Loss: 0.6519 | Train Acc: 0.7576 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.6394 | Train Acc: 0.7576 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.6302 | Train Acc: 0.7727 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.6228 | Train Acc: 0.7803 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.6123 | Train Acc: 0.7803 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.6017 | Train Acc: 0.7879 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.5942 | Train Acc: 0.7803 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.5856 | Train Acc: 0.7955 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.5752 | Train Acc: 0.8030 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.5656 | Train Acc: 0.8030 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.5578 | Train Acc: 0.8106 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.5498 | Train Acc: 0.8106 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.5413 | Train Acc: 0.8258 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.5331 | Train Acc: 0.8182 | Test Acc: 0.2222\n",
      "Fold 3 | Iteration 500 | Loss: 0.5265 | Train Acc: 0.8258 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.5192 | Train Acc: 0.8258 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.5090 | Train Acc: 0.8333 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.4978 | Train Acc: 0.8333 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.4895 | Train Acc: 0.8409 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.4834 | Train Acc: 0.8333 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.4769 | Train Acc: 0.8561 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.4685 | Train Acc: 0.8409 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.4587 | Train Acc: 0.8409 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.4504 | Train Acc: 0.8636 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.4439 | Train Acc: 0.8409 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.4374 | Train Acc: 0.8712 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.4300 | Train Acc: 0.8636 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.4224 | Train Acc: 0.8788 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.4147 | Train Acc: 0.8788 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.4078 | Train Acc: 0.8788 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.4015 | Train Acc: 0.8864 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.3956 | Train Acc: 0.8864 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.3903 | Train Acc: 0.8864 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.3856 | Train Acc: 0.8788 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.3816 | Train Acc: 0.8939 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.3764 | Train Acc: 0.8788 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.3712 | Train Acc: 0.8712 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.3646 | Train Acc: 0.8864 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.3574 | Train Acc: 0.8864 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.3486 | Train Acc: 0.9015 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.3429 | Train Acc: 0.8864 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.3399 | Train Acc: 0.9015 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.3341 | Train Acc: 0.9015 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.3256 | Train Acc: 0.8939 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.3215 | Train Acc: 0.9091 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.3183 | Train Acc: 0.9091 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.3107 | Train Acc: 0.9015 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.3052 | Train Acc: 0.9091 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.3028 | Train Acc: 0.9091 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2979 | Train Acc: 0.9091 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2908 | Train Acc: 0.9167 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2871 | Train Acc: 0.9167 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2844 | Train Acc: 0.9167 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2788 | Train Acc: 0.9242 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2736 | Train Acc: 0.9242 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2709 | Train Acc: 0.9242 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2668 | Train Acc: 0.9242 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2618 | Train Acc: 0.9242 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2583 | Train Acc: 0.9242 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2550 | Train Acc: 0.9242 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2504 | Train Acc: 0.9242 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2467 | Train Acc: 0.9242 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2436 | Train Acc: 0.9242 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2398 | Train Acc: 0.9242 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2357 | Train Acc: 0.9242 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2327 | Train Acc: 0.9242 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2293 | Train Acc: 0.9242 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2256 | Train Acc: 0.9242 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2225 | Train Acc: 0.9242 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2197 | Train Acc: 0.9318 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2165 | Train Acc: 0.9394 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2131 | Train Acc: 0.9394 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2102 | Train Acc: 0.9470 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2075 | Train Acc: 0.9318 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2046 | Train Acc: 0.9470 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.2019 | Train Acc: 0.9318 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1997 | Train Acc: 0.9470 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1976 | Train Acc: 0.9394 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1956 | Train Acc: 0.9470 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1932 | Train Acc: 0.9394 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1907 | Train Acc: 0.9470 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1870 | Train Acc: 0.9394 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1834 | Train Acc: 0.9470 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1804 | Train Acc: 0.9470 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1782 | Train Acc: 0.9394 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1767 | Train Acc: 0.9545 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1752 | Train Acc: 0.9394 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1735 | Train Acc: 0.9545 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1706 | Train Acc: 0.9394 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1676 | Train Acc: 0.9545 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1646 | Train Acc: 0.9545 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1623 | Train Acc: 0.9545 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1606 | Train Acc: 0.9545 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1591 | Train Acc: 0.9470 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1572 | Train Acc: 0.9545 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1548 | Train Acc: 0.9545 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1524 | Train Acc: 0.9545 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1504 | Train Acc: 0.9545 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1485 | Train Acc: 0.9545 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1468 | Train Acc: 0.9545 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1452 | Train Acc: 0.9545 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1438 | Train Acc: 0.9545 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1421 | Train Acc: 0.9545 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1402 | Train Acc: 0.9545 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1383 | Train Acc: 0.9545 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1364 | Train Acc: 0.9545 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1346 | Train Acc: 0.9545 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1332 | Train Acc: 0.9545 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1319 | Train Acc: 0.9545 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1305 | Train Acc: 0.9545 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1291 | Train Acc: 0.9545 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1275 | Train Acc: 0.9545 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1259 | Train Acc: 0.9545 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1242 | Train Acc: 0.9545 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1226 | Train Acc: 0.9545 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1212 | Train Acc: 0.9545 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1199 | Train Acc: 0.9545 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1186 | Train Acc: 0.9621 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1173 | Train Acc: 0.9545 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1161 | Train Acc: 0.9621 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1149 | Train Acc: 0.9545 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1137 | Train Acc: 0.9621 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1126 | Train Acc: 0.9545 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1114 | Train Acc: 0.9621 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1101 | Train Acc: 0.9621 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1088 | Train Acc: 0.9621 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1077 | Train Acc: 0.9621 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1066 | Train Acc: 0.9621 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1055 | Train Acc: 0.9621 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1043 | Train Acc: 0.9621 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1032 | Train Acc: 0.9621 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1022 | Train Acc: 0.9621 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.1011 | Train Acc: 0.9621 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.1001 | Train Acc: 0.9621 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0991 | Train Acc: 0.9621 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0982 | Train Acc: 0.9621 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0973 | Train Acc: 0.9621 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0964 | Train Acc: 0.9621 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0956 | Train Acc: 0.9621 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0949 | Train Acc: 0.9697 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.0942 | Train Acc: 0.9621 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0934 | Train Acc: 0.9621 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.0928 | Train Acc: 0.9621 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0921 | Train Acc: 0.9621 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.0910 | Train Acc: 0.9621 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0899 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0888 | Train Acc: 0.9621 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0877 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0868 | Train Acc: 0.9621 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0860 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0854 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0848 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0842 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0835 | Train Acc: 0.9697 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0827 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0818 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0810 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0802 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0796 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0790 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0785 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0782 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0778 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0773 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.0765 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0756 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0747 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0740 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0733 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0730 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0725 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.0721 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0716 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.0708 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0701 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.0694 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.0690 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0685 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.0683 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0678 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.0675 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0669 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0662 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0655 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0650 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0645 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0641 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0637 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0634 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.0630 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0627 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.0622 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0617 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0612 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0606 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0602 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0598 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0595 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0592 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0590 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.0588 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0586 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.0581 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0577 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0572 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0568 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0565 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0564 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0566 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0567 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0562 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0554 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0548 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0546 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0547 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0544 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0537 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0532 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0530 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0530 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0529 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0526 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0522 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0520 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0519 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0518 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0515 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0511 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0507 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0504 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0502 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0501 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0500 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0497 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0495 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0492 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0490 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0488 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0487 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0486 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0485 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0484 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0484 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.0483 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0482 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.0480 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0478 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.0475 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0472 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0469 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0466 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0464 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0463 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0462 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0461 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0460 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0459 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0459 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0458 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0457 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0455 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0454 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0452 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0450 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0448 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0446 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0445 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0444 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0444 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0445 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0448 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0451 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0456 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0458 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0456 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0449 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0442 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.0434 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0430 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0429 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0431 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0434 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0436 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0437 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0436 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0433 | Train Acc: 0.9773 | Test Acc: 0.1944\n",
      "Fold 3 | Iteration 500 | Loss: 0.0429 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0424 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0421 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0420 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0419 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0419 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0419 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0420 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0421 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0421 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0421 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0420 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0419 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0417 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0415 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0412 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0411 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0409 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0408 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0408 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0407 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0408 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0407 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0407 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0406 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0405 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0404 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0402 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0401 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0401 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0401 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0402 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0402 | Train Acc: 0.9773 | Test Acc: 0.1389\n",
      "Fold 3 | Iteration 500 | Loss: 0.0403 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0404 | Train Acc: 0.9773 | Test Acc: 0.1389\n",
      "Fold 3 | Iteration 500 | Loss: 0.0404 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0403 | Train Acc: 0.9773 | Test Acc: 0.1389\n",
      "Fold 3 | Iteration 500 | Loss: 0.0401 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0399 | Train Acc: 0.9773 | Test Acc: 0.1389\n",
      "Fold 3 | Iteration 500 | Loss: 0.0397 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0394 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0393 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0392 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0391 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0391 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0391 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0392 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0393 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0394 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0395 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0396 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0397 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0396 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0394 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0392 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0389 | Train Acc: 0.9773 | Test Acc: 0.1389\n",
      "Fold 3 | Iteration 500 | Loss: 0.0387 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0385 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0384 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0383 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0383 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0383 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0384 | Train Acc: 0.9773 | Test Acc: 0.1389\n",
      "Fold 3 | Iteration 500 | Loss: 0.0385 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0386 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0387 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0388 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0389 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0389 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0388 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0386 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0384 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0382 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0379 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0378 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0377 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0376 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0375 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "Fold 3 | Iteration 500 | Loss: 0.0375 | Train Acc: 0.9773 | Test Acc: 0.1667\n",
      "\n",
      "--- Fold 4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\malth\\Miniconda3\\envs\\dtu02450\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 | Iteration 500 | Loss: 2.3967 | Train Acc: 0.2576 | Test Acc: 0.2222\n",
      "Fold 4 | Iteration 500 | Loss: 2.3754 | Train Acc: 0.2879 | Test Acc: 0.2500\n",
      "Fold 4 | Iteration 500 | Loss: 2.3526 | Train Acc: 0.2879 | Test Acc: 0.2500\n",
      "Fold 4 | Iteration 500 | Loss: 2.3271 | Train Acc: 0.2955 | Test Acc: 0.2500\n",
      "Fold 4 | Iteration 500 | Loss: 2.2974 | Train Acc: 0.2803 | Test Acc: 0.2222\n",
      "Fold 4 | Iteration 500 | Loss: 2.2624 | Train Acc: 0.2652 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 2.2214 | Train Acc: 0.2348 | Test Acc: 0.2500\n",
      "Fold 4 | Iteration 500 | Loss: 2.1743 | Train Acc: 0.2045 | Test Acc: 0.2500\n",
      "Fold 4 | Iteration 500 | Loss: 2.1221 | Train Acc: 0.1894 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 2.0668 | Train Acc: 0.1894 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 2.0125 | Train Acc: 0.1894 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.9643 | Train Acc: 0.1894 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.9268 | Train Acc: 0.2576 | Test Acc: 0.2222\n",
      "Fold 4 | Iteration 500 | Loss: 1.9013 | Train Acc: 0.2955 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.8867 | Train Acc: 0.2727 | Test Acc: 0.2500\n",
      "Fold 4 | Iteration 500 | Loss: 1.8815 | Train Acc: 0.2727 | Test Acc: 0.2500\n",
      "Fold 4 | Iteration 500 | Loss: 1.8827 | Train Acc: 0.2727 | Test Acc: 0.2500\n",
      "Fold 4 | Iteration 500 | Loss: 1.8822 | Train Acc: 0.2727 | Test Acc: 0.2500\n",
      "Fold 4 | Iteration 500 | Loss: 1.8743 | Train Acc: 0.2803 | Test Acc: 0.2500\n",
      "Fold 4 | Iteration 500 | Loss: 1.8605 | Train Acc: 0.2955 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.8453 | Train Acc: 0.2879 | Test Acc: 0.2222\n",
      "Fold 4 | Iteration 500 | Loss: 1.8317 | Train Acc: 0.3106 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.8199 | Train Acc: 0.3106 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.8084 | Train Acc: 0.3106 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.7970 | Train Acc: 0.3182 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.7864 | Train Acc: 0.3030 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.7775 | Train Acc: 0.3030 | Test Acc: 0.2222\n",
      "Fold 4 | Iteration 500 | Loss: 1.7707 | Train Acc: 0.2955 | Test Acc: 0.2222\n",
      "Fold 4 | Iteration 500 | Loss: 1.7655 | Train Acc: 0.2955 | Test Acc: 0.2222\n",
      "Fold 4 | Iteration 500 | Loss: 1.7607 | Train Acc: 0.2955 | Test Acc: 0.2222\n",
      "Fold 4 | Iteration 500 | Loss: 1.7551 | Train Acc: 0.2955 | Test Acc: 0.2222\n",
      "Fold 4 | Iteration 500 | Loss: 1.7488 | Train Acc: 0.3258 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.7418 | Train Acc: 0.3258 | Test Acc: 0.2222\n",
      "Fold 4 | Iteration 500 | Loss: 1.7344 | Train Acc: 0.3182 | Test Acc: 0.2222\n",
      "Fold 4 | Iteration 500 | Loss: 1.7268 | Train Acc: 0.3106 | Test Acc: 0.2222\n",
      "Fold 4 | Iteration 500 | Loss: 1.7188 | Train Acc: 0.3106 | Test Acc: 0.2222\n",
      "Fold 4 | Iteration 500 | Loss: 1.7102 | Train Acc: 0.3106 | Test Acc: 0.2222\n",
      "Fold 4 | Iteration 500 | Loss: 1.7013 | Train Acc: 0.3182 | Test Acc: 0.2222\n",
      "Fold 4 | Iteration 500 | Loss: 1.6921 | Train Acc: 0.3409 | Test Acc: 0.2222\n",
      "Fold 4 | Iteration 500 | Loss: 1.6834 | Train Acc: 0.3333 | Test Acc: 0.2500\n",
      "Fold 4 | Iteration 500 | Loss: 1.6750 | Train Acc: 0.3333 | Test Acc: 0.2500\n",
      "Fold 4 | Iteration 500 | Loss: 1.6664 | Train Acc: 0.3258 | Test Acc: 0.2500\n",
      "Fold 4 | Iteration 500 | Loss: 1.6577 | Train Acc: 0.3333 | Test Acc: 0.2222\n",
      "Fold 4 | Iteration 500 | Loss: 1.6490 | Train Acc: 0.3485 | Test Acc: 0.2222\n",
      "Fold 4 | Iteration 500 | Loss: 1.6403 | Train Acc: 0.3561 | Test Acc: 0.2500\n",
      "Fold 4 | Iteration 500 | Loss: 1.6316 | Train Acc: 0.3712 | Test Acc: 0.2222\n",
      "Fold 4 | Iteration 500 | Loss: 1.6225 | Train Acc: 0.3788 | Test Acc: 0.2222\n",
      "Fold 4 | Iteration 500 | Loss: 1.6133 | Train Acc: 0.3864 | Test Acc: 0.2222\n",
      "Fold 4 | Iteration 500 | Loss: 1.6039 | Train Acc: 0.3864 | Test Acc: 0.2222\n",
      "Fold 4 | Iteration 500 | Loss: 1.5944 | Train Acc: 0.4091 | Test Acc: 0.2222\n",
      "Fold 4 | Iteration 500 | Loss: 1.5848 | Train Acc: 0.4091 | Test Acc: 0.2222\n",
      "Fold 4 | Iteration 500 | Loss: 1.5754 | Train Acc: 0.3939 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.5659 | Train Acc: 0.3864 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.5566 | Train Acc: 0.3864 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.5474 | Train Acc: 0.4015 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.5378 | Train Acc: 0.4394 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.5283 | Train Acc: 0.4470 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.5188 | Train Acc: 0.4470 | Test Acc: 0.2222\n",
      "Fold 4 | Iteration 500 | Loss: 1.5092 | Train Acc: 0.4470 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.4998 | Train Acc: 0.4621 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.4904 | Train Acc: 0.4621 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.4810 | Train Acc: 0.4621 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.4713 | Train Acc: 0.4697 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.4613 | Train Acc: 0.4697 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.4515 | Train Acc: 0.4621 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.4414 | Train Acc: 0.4545 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.4312 | Train Acc: 0.4545 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.4206 | Train Acc: 0.4621 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.4099 | Train Acc: 0.4697 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.3989 | Train Acc: 0.4697 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.3878 | Train Acc: 0.4848 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.3765 | Train Acc: 0.4848 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.3652 | Train Acc: 0.4924 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.3539 | Train Acc: 0.5076 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.3426 | Train Acc: 0.5152 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.3311 | Train Acc: 0.5227 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.3194 | Train Acc: 0.5303 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.3078 | Train Acc: 0.5303 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.2960 | Train Acc: 0.5303 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.2841 | Train Acc: 0.5379 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.2721 | Train Acc: 0.5530 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.2598 | Train Acc: 0.5682 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.2475 | Train Acc: 0.5833 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.2353 | Train Acc: 0.5833 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.2229 | Train Acc: 0.5909 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.2107 | Train Acc: 0.5758 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.1984 | Train Acc: 0.5909 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.1859 | Train Acc: 0.5909 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.1731 | Train Acc: 0.5985 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.1605 | Train Acc: 0.6061 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.1475 | Train Acc: 0.5985 | Test Acc: 0.1944\n",
      "Fold 4 | Iteration 500 | Loss: 1.1344 | Train Acc: 0.5909 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.1209 | Train Acc: 0.5909 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.1075 | Train Acc: 0.5985 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.0939 | Train Acc: 0.5909 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 1.0802 | Train Acc: 0.6061 | Test Acc: 0.1667\n",
      "Fold 4 | Iteration 500 | Loss: 1.0669 | Train Acc: 0.6061 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 1.0551 | Train Acc: 0.6288 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 1.0455 | Train Acc: 0.6212 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 1.0331 | Train Acc: 0.6439 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 1.0156 | Train Acc: 0.6439 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 1.0050 | Train Acc: 0.6439 | Test Acc: 0.1389\n",
      "Fold 4 | Iteration 500 | Loss: 0.9952 | Train Acc: 0.6515 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.9797 | Train Acc: 0.6591 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.9691 | Train Acc: 0.6515 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.9591 | Train Acc: 0.6667 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.9447 | Train Acc: 0.6742 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.9342 | Train Acc: 0.6742 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.9242 | Train Acc: 0.6894 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.9106 | Train Acc: 0.7045 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.8996 | Train Acc: 0.6742 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.8899 | Train Acc: 0.7121 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.8775 | Train Acc: 0.7121 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.8661 | Train Acc: 0.6894 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.8561 | Train Acc: 0.7197 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.8447 | Train Acc: 0.7121 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.8336 | Train Acc: 0.7045 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.8231 | Train Acc: 0.7273 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.8126 | Train Acc: 0.7121 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.8028 | Train Acc: 0.7273 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.7920 | Train Acc: 0.7348 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.7803 | Train Acc: 0.7273 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.7706 | Train Acc: 0.7348 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.7607 | Train Acc: 0.7273 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.7506 | Train Acc: 0.7500 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.7406 | Train Acc: 0.7348 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.7300 | Train Acc: 0.7348 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.7193 | Train Acc: 0.7424 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.7098 | Train Acc: 0.7348 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.7003 | Train Acc: 0.7576 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.6909 | Train Acc: 0.7424 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.6825 | Train Acc: 0.7652 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.6733 | Train Acc: 0.7424 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.6631 | Train Acc: 0.7727 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.6522 | Train Acc: 0.7803 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.6414 | Train Acc: 0.7803 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.6325 | Train Acc: 0.7803 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.6244 | Train Acc: 0.7803 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.6169 | Train Acc: 0.7879 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.6080 | Train Acc: 0.7955 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.5985 | Train Acc: 0.8030 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.5887 | Train Acc: 0.7879 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.5786 | Train Acc: 0.7955 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.5698 | Train Acc: 0.7955 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.5604 | Train Acc: 0.7955 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.5532 | Train Acc: 0.8182 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.5452 | Train Acc: 0.8106 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.5366 | Train Acc: 0.8182 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.5273 | Train Acc: 0.8258 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.5179 | Train Acc: 0.8258 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.5102 | Train Acc: 0.8333 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.5020 | Train Acc: 0.8258 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.4940 | Train Acc: 0.8485 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.4866 | Train Acc: 0.8485 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.4780 | Train Acc: 0.8409 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.4706 | Train Acc: 0.8485 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.4646 | Train Acc: 0.8485 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.4623 | Train Acc: 0.8561 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.4583 | Train Acc: 0.8561 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.4478 | Train Acc: 0.8712 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.4331 | Train Acc: 0.8712 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.4277 | Train Acc: 0.8636 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.4253 | Train Acc: 0.8788 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.4147 | Train Acc: 0.8864 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.4077 | Train Acc: 0.8712 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.4035 | Train Acc: 0.8864 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.3944 | Train Acc: 0.8788 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.3889 | Train Acc: 0.8712 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.3860 | Train Acc: 0.8864 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.3747 | Train Acc: 0.8864 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.3700 | Train Acc: 0.8788 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.3662 | Train Acc: 0.8864 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.3569 | Train Acc: 0.8864 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.3526 | Train Acc: 0.8864 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.3484 | Train Acc: 0.8864 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.3393 | Train Acc: 0.8939 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.3370 | Train Acc: 0.8864 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.3305 | Train Acc: 0.9015 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.3238 | Train Acc: 0.9091 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.3196 | Train Acc: 0.8939 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.3147 | Train Acc: 0.9091 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.3081 | Train Acc: 0.9167 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.3045 | Train Acc: 0.9015 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.2989 | Train Acc: 0.9091 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.2935 | Train Acc: 0.9242 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.2893 | Train Acc: 0.9091 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.2846 | Train Acc: 0.9167 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.2792 | Train Acc: 0.9242 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.2751 | Train Acc: 0.9167 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.2708 | Train Acc: 0.9167 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.2656 | Train Acc: 0.9318 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.2628 | Train Acc: 0.9091 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.2593 | Train Acc: 0.9242 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.2551 | Train Acc: 0.9318 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.2528 | Train Acc: 0.9318 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.2478 | Train Acc: 0.9318 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.2419 | Train Acc: 0.9318 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.2372 | Train Acc: 0.9242 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.2322 | Train Acc: 0.9318 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.2284 | Train Acc: 0.9394 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.2263 | Train Acc: 0.9394 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.2239 | Train Acc: 0.9318 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.2202 | Train Acc: 0.9470 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.2165 | Train Acc: 0.9470 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.2099 | Train Acc: 0.9470 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.2056 | Train Acc: 0.9470 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.2036 | Train Acc: 0.9470 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.2003 | Train Acc: 0.9545 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1969 | Train Acc: 0.9545 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1929 | Train Acc: 0.9545 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1882 | Train Acc: 0.9545 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1857 | Train Acc: 0.9697 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1839 | Train Acc: 0.9545 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1813 | Train Acc: 0.9545 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.1776 | Train Acc: 0.9545 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1734 | Train Acc: 0.9545 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1696 | Train Acc: 0.9621 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.1673 | Train Acc: 0.9545 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1652 | Train Acc: 0.9773 | Test Acc: 0.0833\n",
      "Fold 4 | Iteration 500 | Loss: 0.1624 | Train Acc: 0.9545 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1586 | Train Acc: 0.9621 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1550 | Train Acc: 0.9773 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1529 | Train Acc: 0.9621 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1508 | Train Acc: 0.9773 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1481 | Train Acc: 0.9621 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1460 | Train Acc: 0.9848 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1428 | Train Acc: 0.9848 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1396 | Train Acc: 0.9848 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1373 | Train Acc: 0.9848 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1348 | Train Acc: 0.9848 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1325 | Train Acc: 0.9773 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1307 | Train Acc: 0.9848 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1280 | Train Acc: 0.9848 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1255 | Train Acc: 0.9848 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1236 | Train Acc: 0.9848 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1211 | Train Acc: 0.9848 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1187 | Train Acc: 0.9848 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1169 | Train Acc: 0.9848 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1147 | Train Acc: 0.9848 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1124 | Train Acc: 0.9848 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1106 | Train Acc: 0.9848 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1086 | Train Acc: 0.9848 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1066 | Train Acc: 0.9848 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1047 | Train Acc: 0.9848 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1027 | Train Acc: 0.9848 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.1009 | Train Acc: 0.9848 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0991 | Train Acc: 0.9848 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0976 | Train Acc: 0.9848 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0958 | Train Acc: 0.9848 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0941 | Train Acc: 0.9848 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0923 | Train Acc: 0.9848 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0904 | Train Acc: 0.9848 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0887 | Train Acc: 0.9848 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0871 | Train Acc: 0.9848 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0858 | Train Acc: 0.9848 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0842 | Train Acc: 0.9848 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0825 | Train Acc: 0.9924 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0809 | Train Acc: 0.9848 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0793 | Train Acc: 0.9848 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0778 | Train Acc: 0.9924 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0764 | Train Acc: 0.9924 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0750 | Train Acc: 0.9924 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0736 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0723 | Train Acc: 0.9924 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0711 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0697 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0685 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0672 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0659 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0646 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0635 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0625 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0614 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0602 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0590 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0581 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0571 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0560 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0549 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0539 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0530 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0520 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0511 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0502 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0493 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0484 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0475 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0466 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0458 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0450 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0441 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0434 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0427 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0419 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0411 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0404 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0397 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0390 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0383 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0376 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0369 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0363 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0356 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0350 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0344 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0337 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0331 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0326 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0320 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0315 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0309 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0304 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0299 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0294 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0289 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0284 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0279 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0274 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0270 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0265 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0261 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0256 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0252 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0248 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0244 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0240 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0236 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0232 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0229 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0225 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0221 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0218 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0215 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0211 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0208 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0205 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0202 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0198 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0195 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0192 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0189 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0186 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0184 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0181 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0178 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0176 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0173 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0171 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0168 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0166 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0163 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0161 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0159 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0157 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0154 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0152 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0150 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0148 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0146 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0145 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0143 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0141 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0139 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0137 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0136 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0134 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0132 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0130 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0129 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0127 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0125 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0124 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0122 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0121 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0119 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0118 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0116 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0115 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0114 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0112 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0111 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0110 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0108 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0107 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0106 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0105 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0104 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0103 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0101 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0100 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0099 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0098 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0097 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0096 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0095 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0094 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0093 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0092 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0091 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0090 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0089 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0088 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0087 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0086 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0086 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0085 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0084 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0083 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0082 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0081 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0081 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0080 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0079 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0078 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0077 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0077 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0076 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0075 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0075 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0074 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0073 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0072 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0072 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0071 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0071 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0070 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0069 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0069 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0068 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0067 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0067 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0066 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0066 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0065 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0065 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0064 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0063 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0063 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0062 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0062 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0061 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0061 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0060 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0060 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0059 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0059 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0058 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0058 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0057 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0057 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0056 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0056 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0055 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0055 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0055 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0054 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0054 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0053 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0053 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0052 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0052 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0052 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0051 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0051 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0050 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0050 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0050 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0049 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0049 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0049 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0048 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0048 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0048 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0047 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0047 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0047 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0046 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0046 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0045 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0045 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0045 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0045 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0044 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0044 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0044 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0043 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0043 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0043 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0042 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0042 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0042 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0041 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0041 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0041 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0040 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0040 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0040 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0039 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0039 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "Fold 4 | Iteration 500 | Loss: 0.0039 | Train Acc: 1.0000 | Test Acc: 0.1111\n",
      "\n",
      "--- Fold 5 ---\n",
      "Fold 5 | Iteration 500 | Loss: 2.4055 | Train Acc: 0.1389 | Test Acc: 0.3333\n",
      "Fold 5 | Iteration 500 | Loss: 2.3906 | Train Acc: 0.2014 | Test Acc: 0.3333\n",
      "Fold 5 | Iteration 500 | Loss: 2.3749 | Train Acc: 0.2500 | Test Acc: 0.4167\n",
      "Fold 5 | Iteration 500 | Loss: 2.3577 | Train Acc: 0.3056 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 2.3379 | Train Acc: 0.2778 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 2.3145 | Train Acc: 0.2847 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 2.2864 | Train Acc: 0.2778 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 2.2530 | Train Acc: 0.2778 | Test Acc: 0.2083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\malth\\Miniconda3\\envs\\dtu02450\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 | Iteration 500 | Loss: 2.2142 | Train Acc: 0.2778 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 2.1703 | Train Acc: 0.2778 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 2.1224 | Train Acc: 0.2778 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 2.0733 | Train Acc: 0.2778 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 2.0270 | Train Acc: 0.2778 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 1.9889 | Train Acc: 0.2778 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 1.9637 | Train Acc: 0.2778 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 1.9513 | Train Acc: 0.2778 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 1.9452 | Train Acc: 0.2778 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 1.9386 | Train Acc: 0.2986 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.9292 | Train Acc: 0.2847 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.9175 | Train Acc: 0.2778 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.9058 | Train Acc: 0.2778 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.8963 | Train Acc: 0.2639 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 1.8885 | Train Acc: 0.2639 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 1.8806 | Train Acc: 0.2708 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 1.8714 | Train Acc: 0.2639 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 1.8612 | Train Acc: 0.2847 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 1.8512 | Train Acc: 0.2986 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.8422 | Train Acc: 0.3194 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.8349 | Train Acc: 0.3125 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.8290 | Train Acc: 0.2847 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.8238 | Train Acc: 0.2847 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.8193 | Train Acc: 0.2847 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.8147 | Train Acc: 0.2847 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.8096 | Train Acc: 0.3056 | Test Acc: 0.1250\n",
      "Fold 5 | Iteration 500 | Loss: 1.8037 | Train Acc: 0.3056 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.7974 | Train Acc: 0.3125 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.7910 | Train Acc: 0.3056 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.7851 | Train Acc: 0.2986 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.7796 | Train Acc: 0.2847 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.7744 | Train Acc: 0.2917 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.7694 | Train Acc: 0.3264 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.7643 | Train Acc: 0.3264 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.7592 | Train Acc: 0.3333 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.7543 | Train Acc: 0.3333 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.7494 | Train Acc: 0.3333 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.7444 | Train Acc: 0.3403 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.7389 | Train Acc: 0.3333 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.7330 | Train Acc: 0.3403 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.7273 | Train Acc: 0.3403 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.7219 | Train Acc: 0.3333 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.7164 | Train Acc: 0.3333 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.7105 | Train Acc: 0.3333 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.7044 | Train Acc: 0.3472 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.6981 | Train Acc: 0.3542 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.6917 | Train Acc: 0.3542 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.6854 | Train Acc: 0.3611 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.6788 | Train Acc: 0.3611 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 1.6717 | Train Acc: 0.3681 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 1.6644 | Train Acc: 0.3681 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 1.6568 | Train Acc: 0.3750 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.6491 | Train Acc: 0.3750 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.6411 | Train Acc: 0.3819 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.6327 | Train Acc: 0.3889 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.6241 | Train Acc: 0.3819 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.6151 | Train Acc: 0.3958 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.6060 | Train Acc: 0.4028 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.5966 | Train Acc: 0.4028 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.5870 | Train Acc: 0.4028 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.5773 | Train Acc: 0.4167 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.5673 | Train Acc: 0.4236 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.5571 | Train Acc: 0.4236 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.5468 | Train Acc: 0.4306 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.5363 | Train Acc: 0.4375 | Test Acc: 0.1667\n",
      "Fold 5 | Iteration 500 | Loss: 1.5256 | Train Acc: 0.4306 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 1.5150 | Train Acc: 0.4306 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 1.5042 | Train Acc: 0.4306 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 1.4932 | Train Acc: 0.4236 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 1.4822 | Train Acc: 0.4306 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 1.4710 | Train Acc: 0.4236 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 1.4597 | Train Acc: 0.4167 | Test Acc: 0.3333\n",
      "Fold 5 | Iteration 500 | Loss: 1.4483 | Train Acc: 0.4167 | Test Acc: 0.3333\n",
      "Fold 5 | Iteration 500 | Loss: 1.4367 | Train Acc: 0.4306 | Test Acc: 0.3333\n",
      "Fold 5 | Iteration 500 | Loss: 1.4250 | Train Acc: 0.4375 | Test Acc: 0.3333\n",
      "Fold 5 | Iteration 500 | Loss: 1.4129 | Train Acc: 0.4444 | Test Acc: 0.3750\n",
      "Fold 5 | Iteration 500 | Loss: 1.4005 | Train Acc: 0.4583 | Test Acc: 0.3750\n",
      "Fold 5 | Iteration 500 | Loss: 1.3877 | Train Acc: 0.4583 | Test Acc: 0.3333\n",
      "Fold 5 | Iteration 500 | Loss: 1.3749 | Train Acc: 0.4514 | Test Acc: 0.3333\n",
      "Fold 5 | Iteration 500 | Loss: 1.3619 | Train Acc: 0.4583 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 1.3492 | Train Acc: 0.4583 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 1.3361 | Train Acc: 0.4792 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 1.3230 | Train Acc: 0.4931 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 1.3097 | Train Acc: 0.5000 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 1.2961 | Train Acc: 0.5000 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 1.2827 | Train Acc: 0.4931 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 1.2694 | Train Acc: 0.4931 | Test Acc: 0.3333\n",
      "Fold 5 | Iteration 500 | Loss: 1.2559 | Train Acc: 0.5069 | Test Acc: 0.3333\n",
      "Fold 5 | Iteration 500 | Loss: 1.2425 | Train Acc: 0.5208 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 1.2294 | Train Acc: 0.5069 | Test Acc: 0.3750\n",
      "Fold 5 | Iteration 500 | Loss: 1.2182 | Train Acc: 0.5069 | Test Acc: 0.3333\n",
      "Fold 5 | Iteration 500 | Loss: 1.2101 | Train Acc: 0.5208 | Test Acc: 0.3750\n",
      "Fold 5 | Iteration 500 | Loss: 1.1965 | Train Acc: 0.5278 | Test Acc: 0.3333\n",
      "Fold 5 | Iteration 500 | Loss: 1.1788 | Train Acc: 0.5347 | Test Acc: 0.3333\n",
      "Fold 5 | Iteration 500 | Loss: 1.1696 | Train Acc: 0.5278 | Test Acc: 0.3750\n",
      "Fold 5 | Iteration 500 | Loss: 1.1593 | Train Acc: 0.5347 | Test Acc: 0.3750\n",
      "Fold 5 | Iteration 500 | Loss: 1.1437 | Train Acc: 0.5486 | Test Acc: 0.3333\n",
      "Fold 5 | Iteration 500 | Loss: 1.1345 | Train Acc: 0.5556 | Test Acc: 0.3333\n",
      "Fold 5 | Iteration 500 | Loss: 1.1241 | Train Acc: 0.5556 | Test Acc: 0.3333\n",
      "Fold 5 | Iteration 500 | Loss: 1.1094 | Train Acc: 0.5625 | Test Acc: 0.3333\n",
      "Fold 5 | Iteration 500 | Loss: 1.0998 | Train Acc: 0.5625 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 1.0892 | Train Acc: 0.5625 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 1.0747 | Train Acc: 0.5764 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 1.0645 | Train Acc: 0.5833 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 1.0535 | Train Acc: 0.5694 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 1.0405 | Train Acc: 0.5833 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 1.0285 | Train Acc: 0.6042 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 1.0184 | Train Acc: 0.5903 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 1.0073 | Train Acc: 0.6111 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.9942 | Train Acc: 0.6042 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.9826 | Train Acc: 0.6111 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.9720 | Train Acc: 0.6250 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.9608 | Train Acc: 0.6181 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.9481 | Train Acc: 0.6250 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.9357 | Train Acc: 0.6250 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.9244 | Train Acc: 0.6319 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.9135 | Train Acc: 0.6597 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.9025 | Train Acc: 0.6528 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.8918 | Train Acc: 0.6458 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.8804 | Train Acc: 0.6736 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.8686 | Train Acc: 0.6597 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.8564 | Train Acc: 0.6736 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.8444 | Train Acc: 0.7014 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.8327 | Train Acc: 0.7014 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.8212 | Train Acc: 0.7083 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.8098 | Train Acc: 0.7014 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.7986 | Train Acc: 0.7222 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.7880 | Train Acc: 0.7014 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.7789 | Train Acc: 0.7431 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.7738 | Train Acc: 0.7153 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.7713 | Train Acc: 0.7222 | Test Acc: 0.3333\n",
      "Fold 5 | Iteration 500 | Loss: 0.7625 | Train Acc: 0.7500 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.7366 | Train Acc: 0.7431 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.7297 | Train Acc: 0.7292 | Test Acc: 0.3333\n",
      "Fold 5 | Iteration 500 | Loss: 0.7296 | Train Acc: 0.7639 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.7082 | Train Acc: 0.7708 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.7001 | Train Acc: 0.7569 | Test Acc: 0.3333\n",
      "Fold 5 | Iteration 500 | Loss: 0.6981 | Train Acc: 0.7639 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.6798 | Train Acc: 0.7847 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.6727 | Train Acc: 0.7708 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.6679 | Train Acc: 0.8056 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.6523 | Train Acc: 0.7847 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.6466 | Train Acc: 0.7986 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.6399 | Train Acc: 0.8194 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.6262 | Train Acc: 0.8125 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.6202 | Train Acc: 0.8056 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.6136 | Train Acc: 0.8125 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.6011 | Train Acc: 0.8125 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.5939 | Train Acc: 0.8333 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.5882 | Train Acc: 0.8194 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.5773 | Train Acc: 0.8333 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.5684 | Train Acc: 0.8264 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.5628 | Train Acc: 0.8333 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.5544 | Train Acc: 0.8333 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.5446 | Train Acc: 0.8194 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.5375 | Train Acc: 0.8403 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.5304 | Train Acc: 0.8403 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.5224 | Train Acc: 0.8333 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.5145 | Train Acc: 0.8403 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.5073 | Train Acc: 0.8403 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.4996 | Train Acc: 0.8403 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.4922 | Train Acc: 0.8472 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.4857 | Train Acc: 0.8333 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.4781 | Train Acc: 0.8333 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.4699 | Train Acc: 0.8472 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.4631 | Train Acc: 0.8542 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.4572 | Train Acc: 0.8472 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.4504 | Train Acc: 0.8611 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.4427 | Train Acc: 0.8611 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.4358 | Train Acc: 0.8542 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.4296 | Train Acc: 0.8681 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.4233 | Train Acc: 0.8681 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.4164 | Train Acc: 0.8750 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.4097 | Train Acc: 0.8681 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.4032 | Train Acc: 0.8819 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.3972 | Train Acc: 0.8681 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.3915 | Train Acc: 0.8819 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.3853 | Train Acc: 0.8750 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.3801 | Train Acc: 0.8681 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.3780 | Train Acc: 0.8681 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.3789 | Train Acc: 0.8681 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.3783 | Train Acc: 0.8681 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.3691 | Train Acc: 0.8889 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.3528 | Train Acc: 0.8819 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.3463 | Train Acc: 0.8819 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.3484 | Train Acc: 0.8819 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.3454 | Train Acc: 0.8819 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.3314 | Train Acc: 0.8889 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.3253 | Train Acc: 0.9028 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.3265 | Train Acc: 0.8889 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.3209 | Train Acc: 0.9097 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.3109 | Train Acc: 0.9097 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.3068 | Train Acc: 0.8958 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.3054 | Train Acc: 0.9097 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.2993 | Train Acc: 0.9097 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.2919 | Train Acc: 0.9167 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.2886 | Train Acc: 0.9097 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.2869 | Train Acc: 0.9167 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.2809 | Train Acc: 0.9375 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.2746 | Train Acc: 0.9375 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.2714 | Train Acc: 0.9306 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.2689 | Train Acc: 0.9375 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.2638 | Train Acc: 0.9444 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.2580 | Train Acc: 0.9375 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.2545 | Train Acc: 0.9444 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.2519 | Train Acc: 0.9444 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.2478 | Train Acc: 0.9444 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.2430 | Train Acc: 0.9514 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.2390 | Train Acc: 0.9444 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.2362 | Train Acc: 0.9514 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.2329 | Train Acc: 0.9514 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.2289 | Train Acc: 0.9514 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.2247 | Train Acc: 0.9514 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.2214 | Train Acc: 0.9444 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.2186 | Train Acc: 0.9514 | Test Acc: 0.2917\n",
      "Fold 5 | Iteration 500 | Loss: 0.2154 | Train Acc: 0.9514 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.2116 | Train Acc: 0.9514 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.2082 | Train Acc: 0.9514 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.2050 | Train Acc: 0.9514 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.2022 | Train Acc: 0.9514 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1992 | Train Acc: 0.9514 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1959 | Train Acc: 0.9514 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1926 | Train Acc: 0.9514 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1896 | Train Acc: 0.9514 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1870 | Train Acc: 0.9514 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1842 | Train Acc: 0.9514 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1814 | Train Acc: 0.9583 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1783 | Train Acc: 0.9514 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1757 | Train Acc: 0.9583 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1734 | Train Acc: 0.9514 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1712 | Train Acc: 0.9583 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1689 | Train Acc: 0.9583 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1664 | Train Acc: 0.9583 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1636 | Train Acc: 0.9583 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1610 | Train Acc: 0.9583 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1585 | Train Acc: 0.9583 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1564 | Train Acc: 0.9653 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1544 | Train Acc: 0.9583 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1523 | Train Acc: 0.9653 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1500 | Train Acc: 0.9583 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1481 | Train Acc: 0.9583 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1467 | Train Acc: 0.9583 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1455 | Train Acc: 0.9583 | Test Acc: 0.2083\n",
      "Fold 5 | Iteration 500 | Loss: 0.1436 | Train Acc: 0.9583 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1407 | Train Acc: 0.9653 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1381 | Train Acc: 0.9653 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1362 | Train Acc: 0.9653 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1351 | Train Acc: 0.9653 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1340 | Train Acc: 0.9583 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1324 | Train Acc: 0.9653 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1306 | Train Acc: 0.9653 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1282 | Train Acc: 0.9653 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1263 | Train Acc: 0.9653 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1250 | Train Acc: 0.9653 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1239 | Train Acc: 0.9653 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1222 | Train Acc: 0.9653 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1201 | Train Acc: 0.9653 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1185 | Train Acc: 0.9653 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1176 | Train Acc: 0.9653 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1169 | Train Acc: 0.9653 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1151 | Train Acc: 0.9653 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1132 | Train Acc: 0.9653 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1119 | Train Acc: 0.9653 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1107 | Train Acc: 0.9653 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1096 | Train Acc: 0.9653 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1084 | Train Acc: 0.9653 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1071 | Train Acc: 0.9653 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1059 | Train Acc: 0.9653 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1045 | Train Acc: 0.9653 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1032 | Train Acc: 0.9653 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1022 | Train Acc: 0.9722 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1012 | Train Acc: 0.9653 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.1001 | Train Acc: 0.9722 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0991 | Train Acc: 0.9653 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0981 | Train Acc: 0.9722 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0971 | Train Acc: 0.9722 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0959 | Train Acc: 0.9722 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0949 | Train Acc: 0.9722 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0941 | Train Acc: 0.9722 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0931 | Train Acc: 0.9722 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0922 | Train Acc: 0.9722 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0913 | Train Acc: 0.9722 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0903 | Train Acc: 0.9722 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0894 | Train Acc: 0.9722 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0885 | Train Acc: 0.9722 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0877 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0868 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0859 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0851 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0843 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0835 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0828 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0820 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0813 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0805 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0797 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0791 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0784 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0777 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0770 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0763 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0756 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0749 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0742 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0735 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0729 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0722 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0716 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0709 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0704 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0699 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0693 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0689 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0687 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0686 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0686 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0684 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0675 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0662 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0651 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0644 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0643 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0643 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0641 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0633 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0623 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0616 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0612 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0611 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0608 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0602 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0596 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0591 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0587 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0583 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0579 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0574 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0571 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0568 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0564 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0560 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0556 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0552 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0548 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0546 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0543 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0540 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0537 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0535 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0532 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0530 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0526 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0522 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0518 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0514 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0511 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0508 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0506 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0504 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0501 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0499 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0497 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0495 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0494 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0492 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0489 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0486 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0483 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0479 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0476 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0474 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0472 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0470 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0469 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0468 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0467 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0466 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0463 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0461 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0458 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0455 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0453 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0451 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0450 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0448 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0446 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0446 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0446 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0445 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0444 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0442 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0441 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0439 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0437 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0434 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0430 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0428 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0426 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0425 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0424 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0424 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0424 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0423 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0421 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0419 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0417 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0416 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0415 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0415 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0414 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0412 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0411 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0409 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0408 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0407 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0406 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0405 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0403 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0402 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0401 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0400 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0398 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0398 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0397 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0398 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0399 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0400 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0402 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0403 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0402 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0398 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0393 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0389 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0387 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0386 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0386 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0388 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0390 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0392 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0394 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0393 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0389 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0385 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0381 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0379 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0378 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0378 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0380 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0382 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0384 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0386 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0383 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0378 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0374 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0371 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0371 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0372 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0374 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0374 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0372 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0369 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0367 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0366 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0366 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0367 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0367 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0368 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0367 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0368 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0368 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0369 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0369 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0368 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0366 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0363 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0361 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0360 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0359 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0358 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0358 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0357 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0356 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0356 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0356 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0356 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0356 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0357 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0358 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0359 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0359 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0358 | Train Acc: 0.9792 | Test Acc: 0.2500\n",
      "Fold 5 | Iteration 500 | Loss: 0.0356 | Train Acc: 0.9792 | Test Acc: 0.2500\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Ann evaluation|\n",
      "\n",
      " ANN Average Test Accuracy: 0.19\n",
      "\n",
      " ANN Average Training Accuracy: 0.81\n",
      "\n",
      " ANN Average Generalization Error: 0.56\n",
      "\n",
      "|Log Reg evaluation|\n",
      "\n",
      " Log Average Training Accuracy: 0.31\n",
      "\n",
      " Log Average Test Accuarcies: 0.29\n",
      "\n",
      " Log Average Generalization Error: 0.02\n"
     ]
    }
   ],
   "source": [
    "class ANNClassifier(torch.nn.Module):\n",
    "    def __init__(self,input_dim, hidden_units, output_dim):\n",
    "        super(ANNClassifier,self).__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim,hidden_units),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_units, hidden_units),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_units, hidden_units),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_units, hidden_units),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_units,output_dim)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = x.shape[1]\n",
    "hidden_units = 132\n",
    "output_dim = 11\n",
    "iteration = 500\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Group K-Fold\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "# ANN lists for store metrics per fold\n",
    "all_train_losses = []\n",
    "all_train_accuracies = []\n",
    "all_test_accuracies = []\n",
    "all_generalization_errors = []\n",
    "ann_accuracy = []\n",
    "\n",
    "# Log list for store metrics per fold\n",
    "log_all_train_accuracies = []\n",
    "log_all_test_accuracies = []\n",
    "log_all_generalization_errors = []\n",
    "log_accuracy = []\n",
    "\n",
    "\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(gkf.split(x, y, groups=groups)):\n",
    "    print(f\"\\n--- Fold {fold+1} ---\")\n",
    "\n",
    "    model = ANNClassifier(input_dim, hidden_units, output_dim)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-10)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    X_train, X_test = x[train_idx], x[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    #ANN fold evaluation indx: \n",
    "    fold_train_losses = []\n",
    "    fold_train_accuracies = []\n",
    "    fold_test_accuracies = []\n",
    "    fold_generalization_errors = []\n",
    "    \n",
    "    #Log fold evaluation indx: \n",
    "    log_all_train_accuracies = []\n",
    "    log_all_test_accuracies = []\n",
    "    log_all_generalization_errors = []\n",
    "    log_accuracy = []\n",
    "    \n",
    "\n",
    "    # Logistic Regression model: \n",
    "    log_model = LogisticRegression(multi_class=\"multinomial\", max_iter=iteration)\n",
    "    log_model.fit(X_train, y_train)\n",
    "    y_pred_train_log = log_model.predict(X_train)\n",
    "    y_pred_test_log = log_model.predict(X_test)\n",
    "\n",
    "    #Log Reg model evaluatation:     \n",
    "    log_train_acc = accuracy_score(y_train, y_pred_train_log)\n",
    "    log_test_acc = accuracy_score(y_test, y_pred_test_log)\n",
    "    log_generalization_error = abs(log_train_acc - log_test_acc)\n",
    "\n",
    "    #Storeas the Log Reg model evaluations \n",
    "    log_all_train_accuracies.append(log_train_acc)\n",
    "    log_all_test_accuracies.append(log_test_acc)\n",
    "    log_all_generalization_errors.append(log_generalization_error)\n",
    "    log_accuracy.append(log_test_acc)\n",
    "\n",
    "    for iterations in range(iteration):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            train_preds = model(X_train)\n",
    "            train_labels = torch.argmax(train_preds, dim=1)\n",
    "            train_acc = accuracy_score(y_train.numpy(), train_labels.numpy())\n",
    "\n",
    "            test_preds = model(X_test)\n",
    "            test_labels = torch.argmax(test_preds, dim=1)\n",
    "            test_acc = accuracy_score(y_test.numpy(), test_labels.numpy())\n",
    "\n",
    "        # ANN evaluation for each fold \n",
    "        fold_train_losses.append(loss.item())\n",
    "        fold_train_accuracies.append(train_acc)\n",
    "        fold_test_accuracies.append(test_acc)\n",
    "        fold_generalization_errors.append(abs(train_acc - test_acc))\n",
    "\n",
    "        #Tjeks if the models overfits\n",
    "        if iteration % 100 == 0:\n",
    "            print(f\"Fold {fold+1} | Iteration {iteration} | Loss: {loss.item():.4f} | Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "    #Stores all the training information in the respectively indx for ANN\n",
    "    all_train_losses.append(fold_train_losses)\n",
    "    all_train_accuracies.append(fold_train_accuracies)\n",
    "    all_test_accuracies.append(fold_test_accuracies)\n",
    "    all_generalization_errors.append(fold_generalization_errors)\n",
    "    ann_accuracy.append(fold_test_accuracies[-1])\n",
    "\n",
    "\n",
    "# Average metrics over folds for ANN \n",
    "train_losses_avg = np.mean(all_train_losses, axis=0)\n",
    "train_accuracies_avg = np.mean(all_train_accuracies, axis=0)\n",
    "test_accuracies_avg = np.mean(all_test_accuracies, axis=0)\n",
    "generalization_errors_avg = np.mean(all_generalization_errors, axis=0)\n",
    "\n",
    "#Average metrics over folds for Log\n",
    "log_train_accuracies_avg = np.mean(log_all_train_accuracies)\n",
    "log_test_accuracies_avg = np.mean(log_all_test_accuracies)\n",
    "log_generalization_errors_avg = np.mean(log_all_generalization_errors)\n",
    "\n",
    "#ANN plot over results\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(train_losses_avg, label='Avg Train Loss')\n",
    "plt.plot(train_accuracies_avg, label='Avg Train Accuracy')\n",
    "plt.plot(test_accuracies_avg, label='Avg Test Accuracy')\n",
    "plt.plot(generalization_errors_avg, label='Avg Generalization Error')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.title('Avg Test-, Avg Train ACC, Avg Train Loss and Avg Generalization Error For the ANN')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#Averege evaluation of ANN \n",
    "print(f\"|Ann evaluation|\")\n",
    "print(f\"\\n ANN Average Test Accuracy: {np.mean(ann_accuracy):.2f}\")\n",
    "print(f\"\\n ANN Average Training Accuracy: {np.mean(fold_train_accuracies):.2f}\")\n",
    "print(f\"\\n ANN Average Generalization Error: {np.mean(fold_generalization_errors):.2f}\\n\")\n",
    "\n",
    "#Average evaluation of Log Reg\n",
    "print(f\"|Log Reg evaluation|\")\n",
    "print(f\"\\n Log Average Training Accuracy: {log_train_accuracies_avg:.2f}\")\n",
    "print(f\"\\n Log Average Test Accuarcies: {log_test_accuracies_avg:.2f}\")\n",
    "print(f\"\\n Log Average Generalization Error: {log_generalization_errors_avg:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8877d41a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dtu02450",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
